[
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Working memory (Miyake and Shah, 1999) is a short-term temporary store with limited capacity.\n\nSentence2: thus as a new (p i , v i , k i )-triplet is added to the MEM, there are trade-offs to be made regarding computational complexity versus performance of the agent.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We hypothesize that in general the agent should perform the best in training, somewhat worse on the holdout-interpolation level and the worst on the holdout-extrapolation level.\n\nSentence2: we expect to see a generalization gap.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To make it more computationally tractable, we place a stop-gradient in the memory write.\n\nSentence2: the write operation for the key in (1) becomes: where SG(â€¢) denote that the gradients are stopped.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models.\n\nSentence2: little progress has been made in understanding when specific memory systems help more than others and how well they generalize.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Using a similar analysis to [13], the preceding result can easily be strengthened to the strong converse, stating that P e is not only bounded away from zero when t is below the threshold given, but tends to one.\n\nSentence2: the proof based on Fano's inequality extends more easily to noisy settings.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Under adaptive testing (i.e., tests can be designed based on previous outcomes), this question is wellunderstood [30], as outlined below.\n\nSentence2: a standard implementation of COMP or DD yields decoding complexity O(n 2 t), which may be infeasible when n is large and decoding time is limited.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Taking the union bound over the B bundles, and noting that 2 Ã— 0.073 2 > 0.01, we find that we can classify all of the bundles correctly with probability approaching one when t mul = (100 log B)(1 + o(1)), so that the total number of tests used is t mul B = (100B log B)(1 + o(1)).\n\nSentence2: this leads to the SSS algorithm described in Algorithm 3.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Their idea is that -due to environmental circumstances-time-series often have heterogeneous lengths and their measurements are taken with different time-intervals in between.\n\nSentence2: we chose to build six polygonal curves with 2205 vertices each in the 6000-dimensional Euclidean space.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This problem, known as the discrete median, has a polynomial-time exhaustivesearch algorithm: calculate the cost of each possible curve by summing over all other input curves.\n\nSentence2: this only happens for line segments of length larger than 10 15 .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Their idea is that -due to environmental circumstances-time-series often have heterogeneous lengths and their measurements are taken with different time-intervals in between.\n\nSentence2: common approaches, where univariate time-series are represented by a point in a high-dimensional space, each dimension corresponding to one instant of time, become hard or even impossible to apply.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To the best of our knowledge, there is no tractable algorithm to compute an exact median polygonal curve.\n\nSentence2: we restrict the search space of feasible solutions to the input.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this section, we illustrate how RTPs can be used to model categorical data.\n\nSentence2: the OP is not self-consistent, and so the binary space partitioning-tree (BSP) process [9] was introduced to modify the cut distribution of the OP in order to recover self-consistency.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Their analysis, however, introduces several sources of looseness.\n\nSentence2: the use of Lipschitz constants, which lead to distributionindependent bounds, eradicates any hope that these bounds will be non-vacuous for modern models and datasets.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The utility of using data-dependent priors to control disintegrated mutual information depends on the balance of two effects: On the one hand, I(W;S) ≤ I(W;S|F), and so conditioning never improves a theoretical bound and may make it looser.\n\nSentence2: i(W ; S) depends on the unknown data distribution and so distribution-independent bounds will often be very loose.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the process of developing tighter distribution-dependent bounds, we also observe that, in some circumstances, one may obtain tighter estimates by working with conditional or disintegrated information-theoretic quantities.\n\nSentence2: doing so provides more opportunities to exchange expectation and concave functions than are available with previous mutual information bounds.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The use of data-dependent estimates leads to distribution-dependent bounds that naturally adapt to the model of interest and the data distribution.\n\nSentence2: using data-dependent estimates, we arrive at bounds in terms of the incoherence of gradients in the dataset.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We introduce the technique of data-dependent priors for bounding mutual information in datadependent estimates of expected generalization error.\n\nSentence2: we use data-dependent priors to forecast the dynamics of iterative algorithms using a randomly chosen subset of the data.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Deep neural networks have become an important tool in applied machine learning, achieving state-ofthe-art regression and classification accuracy in many domains.\n\nSentence2: quantifying and measuring uncertainty in these discriminative models, despite recent important advances, is still an open problem.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, we demonstrate an interesting property of our method that learns a broader prior over Ï† as the training set size decreases.\n\nSentence2: a relation between the predictive uncertainty and the number of unique structures (connectivity patterns) encoded in a BRAINet model (exemplified in Appendix C-Figure 1).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Bayesian neural networks are a powerful solution, where the prior over network weights is a design choice, often a normal distribution or other distribution encouraging sparsity.\n\nSentence2: this prior is agnostic to the generative process of the input data, which might lead to unwarranted generalization for outof-distribution tested data.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is a 10 frequency signal, where the frequencies at each sampling time-instant are generated uniformly at random in the audible frequency range 20 Hz to 20 kHz, and where the amplitudes Ar(t) are generated uniformly in the range [−180, 180];\n\nSentence2: in each trial among a total of N , at every sampling instant an independent realization of the sound-level signal is used.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When there is a large number of trials, the error in the estimation of empirical CDFs reduces further.\n\nSentence2: the simulation results validate our distribution-learning method with location-unaware samples.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this case, an integration model will be selected so that multiple cues can be weighed and combined together to form a unified estimate of head direction of self-motion [7,8].\n\nSentence2: when we wear a goggle to navigate in a virtual reality world while sitting on a spinning chair, the visual and the vestibular signals would be quite discordant and it would be wrong to integrate them during inference [9].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A number of psychological studies have suggested that our brains indeed perform causal inference as an ideal observer (e.g., [10,[12][13][14]).\n\nSentence2: it has been challenging to come up with a simple and biologically plausible neural implementation for causal inference.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As such, we construct an error predictor (EP) and an iterative error correction machine (EC machine) as shown in Figure 2.\n\nSentence2: we freeze the autoencoder-decoder from the previous stage and reuse them to generate the states of the input nodes h k , k = 0, ..., K and output nodes h t , t = 0, ..., T .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages.\n\nSentence2: binary decompilation can be leveraged for code vulnerability analysis and malware detection.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Lastly, it should not require any modification to existing model architectures so that we can achieve better flexibility and minimal engineering effort.\n\nSentence2: most of the methods that currently exist do not meet some of the above criteria.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, knowing uncertainty is important for fundamental machine learning research [19]; for example, many reinforcement learning algorithms (such as Thompson sampling [40]) requires estimating uncertainty of the distribution [39].\n\nSentence2: there has not been any well-established, effective and efficient method to assess prediction uncertainty of deep learning models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In fact, trying to minimize the natural log loss for a classification task is the same as trying to maximize the doubling rate in a gambling problem.\n\nSentence2: in practice, we are often in a horse race where some information about the horse is known.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The rest four methods (SCML, DRML, SGDINC, and our M-FLRML) claim that there is no need for SVD preprocessing, which are compared using the original data matrices as input.\n\nSentence2: since the SVD calculation for datasets M10-100 and M40-100 has exceeded the memory limit of common PCs, only these four methods are tested on these two datasets.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Smaller matrix size theoretically means the ability to process larger datasets on the same size of memory.\n\nSentence2: in practice, we find that the bottleneck is not the optimization process of FLRML.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition to these generalizations, there is good reason to suspect the results to extend to much broader class of models and optimization procedures.\n\nSentence2: a wealth of recent literature suggests that the mean field theory governing the wide network limit of fully-connected models [32,33] extends naturally to residual networks [35], CNNs [34], RNNs [39], batch normalization [40], and to broad architectures [37].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: [7], it is shown that the comparison of performance between finite-and infinite-width networks is highly architecture-dependent.\n\nSentence2: it was found that infinite-width networks perform as well as or better than their finite-width counterparts for many fully-connected or locally-connected architectures.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: VAEs with non Euclidean latent space have been recently introduced, such as Davidson et al. (2018) making use of hyperspherical geometry and Falorsi et al. (2018) endowing the latent space with a SO(3) group structure.\n\nSentence2: gradients rµz can straightforwardly be computed thanks to the exponential map reparametrisation (Eq 3), and gradients w.r.t. the dispersion rz are readily available for the wrapped normal.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: See a sample of the perturbed images produced by our method in Section B.\n\nSentence2: this is closely related to the standard approximation variant of polynomial maximization problem where the goal is to obtain, in polynomial time, an objective value as close to the optimal one, without violating the B n OE ball constraint.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle the uncertainty.\n\nSentence2: the inference in MLNs is usually very difficult due to the complicated graph structures.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose such an approach called the probabilistic Logic Neural Networks (pLogicNet).\n\nSentence2: logic rules can be imperfect or even contradictory.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Some recent work also studies using reinforcement learning for reasoning on knowledge graphs [6,23,38,47], where an agent is trained to search for reasoning paths.\n\nSentence2: the performance of these methods is not so competitive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Such methods have been proven effective for reasoning on knowledge graphs.\n\nSentence2: lastly, there are also some recent studies trying to combine statistical relational learning and graph neural networks for semi-supervised node classification [33], or using Markov networks for visual dialog reasoning [32,51].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These graphs have been proven useful in many tasks, such as question answering [49], relation extraction [34] and recommender systems [4].\n\nSentence2: one big challenge of knowledge graphs is that their coverage is limited.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The level-2 model had a lower fitting accuracy compared to the level-1 model in almost all subjects (Figure 1b).\n\nSentence2: we used only the first two levels of ToM for further analysis.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As the level of ToM increases, the player free-rides in earlier rounds because others (modeled as optimal agents) would be expected to free ride in later rounds.\n\nSentence2: using higher levels of ToM shifts free-riding towards the first round, decreasing the contribution rate over all rounds to 0 (Figure 2d).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The accuracy of a model was determined by the similarity between the model's predicted action and the actual action of the subject in each round on average.\n\nSentence2: the idea of selecting actions that maximize the total expected reward transforms the model from one based on HMMs (previous section) to a Partially Observable Markov Decision Process (POMDP) [19].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We fit models based on conformity with different levels of ToM to the behavior of each subject and compared the accuracy of the different level models in explaining human behavior.\n\nSentence2: in our model fitting, we used a set of free parameters that explain all games of each subject across all the different conditions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Specifically, in our model fitting, we used a set of free parameters that explain all games of each subject across all the different conditions.\n\nSentence2: different conditions are modelled naturally in our framework without any parameter tuning for each condition.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In terms of training objective, they use lower bound (5) with beam search over T * (Y ), which is different from our lower bound (6).\n\nSentence2: we found our lower bound to be beneficial in terms of quality and less prone to getting stuck in local optima.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This has already been used in previous works [10,11,12].\n\nSentence2: this leads to poor performance and unstable results.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also we noticed that tokens which are generated first are often uninformative (e.g., punctuation, determiners, etc.).\n\nSentence2: wu et al. [7] suggest that left-to-right NMT models fit better for right-branching languages (e.g., English) and right-to-left NMT models fit better for left-branching languages (e.g., Japanese).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This generation procedure is inherently sequential.\n\nSentence2: it is challenging to effectively parallelize it on GPU accelerators.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As expected, the right-to-left generation order is better than the left-to-right for translation into Japanese.\n\nSentence2: our model significantly outperforms both baselines.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While in our model positional encodings of most of the tokens change after each insertion operation and, therefore, decoder self-attention is re-applied at each generation step, the model by Gu et al. [22] does not need this and has better theoretical time complexity of O(len(Y )2) in contrast to our O(len(Y )3).\n\nSentence2: in practice our decoding is on average only 50% times slower than the baseline; for the details, see Section 5.2.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Secondly, the decoder predicts the joint probability of a token and a position corresponding to a single insertion (rather than the probability of a token, as usually done in the standard setting).\n\nSentence2: the predicted probabilities should add up to 1 over all positions and tokens at each step.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, if the entire probability mass is concentrated on a single trajectory, both lower bounds are tight.\n\nSentence2: when maximizing (6), we also expect most of the probability mass to be concentrated on one or a few \"best\" trajectories.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Instead of minimizing the exact loss function L∞, we can optimize its upper bound L∞- in a much more efficient way\n\nSentence2: the proposed ISDA boils down to a novel robust loss function, which can be easily adopted by most deep models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To further improve the efficiency, we derive a closed-form upper bound of the expected cross-entropy (CE) loss with the proposed data augmentation scheme.\n\nSentence2: instead of performing the augmentation procedure explicitly, we can directly minimize the upper bound, which is, in fact, a novel robust loss function.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds.\n\nSentence2: translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we propose an implicit semantic data augmentation (ISDA) algorithm for training deep image recognition networks.\n\nSentence2: these models generally suffer a performance reduction on CIFAR-100, which do not contain enough samples to learn a valid generator for each class.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This motivates us to augment the training set by applying such semantic transformations on deep features.\n\nSentence2: manually searching for semantic directions is infeasible for large scale problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When the feature of a person, who does not wear glasses, is translated along this direction, the new feature may correspond to the same person but with glasses (The new image can be explicitly reconstructed using proper algorithms as shown in [9]).\n\nSentence2: by searching for many such semantic directions, we can effectively augment the training set in a way complementary to traditional data augmenting techniques.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In case (a), ABQ will not allocate any points to this region A at all, after a finite number of iterations.\n\nSentence2: the information about the integrand f on this region A will not be obtained after a finite number of evaluations, which makes it difficult to guarantee the consistency of quadrature, unless f has a finite degree of freedom on A.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In fact, it is only recently that researchers have proposed stochastic gradient descent (SGD) algorithms for DRO with f -divergence-based ambiguity sets [17].\n\nSentence2: f -divergence measures can only compare distributions with the same support, while the Wasserstein distances do not have such a restriction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Furthermore, we conduct three different experiments to show its superb performance on both synthetic and real-world datasets.\n\nSentence2: our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem.\n\nSentence2: we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As standard ADMM-type algorithms are very sensitive to the choice of penalty parameters, it is hard for us to tune the optimal penalty parameter for each subproblem with different .\n\nSentence2: we use a constant penalty parameter for the non-adaptive case.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The latter makes our algorithmic framework numerically more robust in general.\n\nSentence2: we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On the numerical side, we demonstrate via extensive experiments that our proposed method can be sped up substantially by adopting a geometrically increasing step size strategy.\n\nSentence2: our method can achieve a hundred-fold speedup over the standard solver (which is the only other method that has been used so far to solve (1.1)) on both synthetic and real-world datasets without the need to tune an optimal penalty parameter in every iteration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We observe our model consistently attend the head or tail on two attention maps respectively.\n\nSentence2: we compare the detected parts with head or tail ground truth annotations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: [25,5,26] train a multiple-part detector with groundtruth annotations to produce the bounding boxes of parts and learn the part features with conventional recognition tasks.\n\nSentence2: the heavy involvement of human labor for part annotations makes tasks costly in the real large-scale problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We find that the complexity of our model can be reduced by using the same CNN with shared weights for both image and part patches, but the zero-shot learning performance is slightly degraded, e.g., the score for CUB-PS and AWA-PS decreases by 3.6%, 2.7%, respectively.\n\nSentence2: the heavy involvement of human labor for part annotations makes tasks costly in the real large-scale problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, the heavy involvement of human labor for part annotations makes tasks costly in the real large-scale problems.\n\nSentence2: learning part attentions in a weakly supervised way is desirable in the zero-shot recognition scenario.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, recovering Fourier coefficients from the measurements of f i can be treated as recovering a sparse vector in R 2 i .\n\nSentence2: in this paper, we provide a method to learn the directed structure of a Bayesian network using data.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our setting is similar to an interventional setting where a selection can be compared to an intervention and an assignment can be compared to an experiment, although our method never queries the 2 k distinct values, but a single random assignment instead.\n\nSentence2: we compare our results to the state-of-the-art interventional methods in Table 1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, when neural network function approximation is used, the effect of optimistic initialisation can disappear quickly with optimisation (Osband et al., 2018).\n\nSentence2: with non-orthogonal state-action embeddings, Q value estimates may decrease for yet unseen state-action pairs, and estimates for different state-action states can move in tandem.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, when neural network function approximation is used, the effect of optimistic initialisation can disappear quickly with optimisation (Osband et al., 2018).\n\nSentence2: to gain a better insight into the value prediction step, we examine its idealised implementation: Suppose we have access to a belief over MDPs, P T (as in PSRL), and want to compute the implied distribution P QÏ€ for a single policy Ï€.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL.\n\nSentence2: we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 3 shows the scaling for Successor Uncertainties and Bootstrap+Prior for this problem.\n\nSentence2: the raw scores are reported in table 2 (appendix), and the difference in human normalised score between SU and the competing algorithms for individual games is charted in figure 4.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Propagation of uncertainty is a desirable property when using Upper Confidence Bounds (UCB; Auer, 2002) for exploration, since UCB methods rely only on the first two moments of PQˆπ .\n\nSentence2: propagation of uncertainty is not sufficient for effective exploration under posterior sampling.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we study this in the classic setting where the unknown environment is modelled as a Markov Decision Process (MDP).\n\nSentence2: we focus on developing an algorithm that combines effective exploration with neural network function approximation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For UBE, this is a consequence of proposition 1, and the similarly poor behaviour of BDQN suggests it may suffer from an analogous issue.\n\nSentence2: a family of methods called Randomised Value Functions (RVF; Osband et al., 2016b) attempt to overcome these issues by directly modelling a distribution over Q functions, P Q, instead of over MDPs, P T .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Examples of correlations are even found in stateless decision-making problems, such as multi-armed bandits, where prominent patterns in the reward mechanisms of different arms can translate into correlated action choices of the operating agent [7,9].\n\nSentence2: these statistical relationships become more pronounced in the case of contextual bandits, where effective decision-making strategies not only exhibit temporal correlation but also take into account the state context at each time point, introducing a second source of correlation [12].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: First, we note that a difficulty of Problem (RC fair ) is the discontinuity of its objective function.\n\nSentence2: we show that (RC fair ) can be formulated equivalently as a two-stage robust optimization problem by introducing a fictitious counting phase after e  is revealed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here, imposing fairness is more challenging as we do not know a-priori which nodes may fail.\n\nSentence2: we must ensure that fairness constraints are satisfied under all failure scenarios.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Next, we show that between-community coverage is negligible compared to within-community coverage.\n\nSentence2: we determine the distribution of the monitors, in the presence and absence of fairness constraints.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These heuristics, although computationally efficient, are coverage-centered and do not take fairness into consideration.\n\nSentence2: they may lead to discriminatory outcomes, see Table 1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similarly, using Bender's decomposition is needed: even for moderate values of K (2 to 3), the K-adaptability MILP (5) could not be loaded in memory.\n\nSentence2: to the best of our knowledge, ours is the first paper enforcing fairness constraints in the context of graph covering subject to node failure.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To motivate our approach, consider deploying in the open world a state-of-the art algorithm for robust graph covering (which does not incorporate fairness considerations).\n\nSentence2: we apply the solutions provided by the algorithm from [45] on five real-world social networks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We thus propose to augment the robust covering problem with fairness constraints.\n\nSentence2: we propose to achieve max-min fairness by imposing fairness constraints on each group's coverage: we require that at least a fraction W of nodes from each group be covered.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, when considering uncertainty in node performance/availability, the objective function loses the submodularity property while exact techniques fail to scale to even moderate problem sizes.\n\nSentence2: existing (exact or approximate) approaches do not apply.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The symmetry results in significant slow down of the Brand-and-Bound procedure [18].\n\nSentence2: we introduce symmetry breaking constraints in the formulation (5) that stipulate the candidate covering schemes be lexicographically decreasing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The steps of the proof are similar to those in the proof of Proposition 1 with the difference that, under uncertainty, monitors should be distributed such that the fairness constraints are satisfied even after J nodes fail.\n\nSentence2: we quantify a minimum number of monitors that should be allocated to each community.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Indeed, when deploying interventions in the open world (especially in sensitive domains impacting life and death like the ones that motivate this work), care must be taken to ensure that algorithms do not discriminate among people with respect to protected characteristics such as race, ethnicity, disability, etc.\n\nSentence2: we need to ensure that independently of their group, individuals have a high chance of being covered, a notion we refer to as group fairness.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Meanwhile, local minima are characterized by the condition that the gradient with respect to the parameters r âœ“ L(âœ“) is zero.\n\nSentence2: one way to avoid finding local minima that aren't global is to ensure that the parameter gradient is zero if and only if the output space gradient (for each case) is zero.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We show that natural gradient enables us to use a much larger step size, resulting in an even faster convergence rate.\n\nSentence2: the maximal step size of natural gradient descent is O (1) for (polynomially) wide networks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In general, finding a global minimum of a general non-convex function is an NP-complete problem, and neural network training in particular is NP-complete [Blum and Rivest, 1992].\n\nSentence2: in the past two years, researchers have finally gained substantial traction in understanding the dynamics of gradient-based optimization of neural networks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Intuitively, we would expect the smallest eigenvalue to be very small if all x i are similar to each other.\n\nSentence2: some notion of diversity of the training inputs is needed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that strongly convexity is a very mild assumption since we can always add L 2 regularization to make the convex loss strongly convex.\n\nSentence2: this function class includes regularized cross-entropy loss (which is typically used in classification) and squared error (for regression).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The combination of dilated convolution and dense connection enables Pyramid-like multi-scale feature fusion instead of parallel convolution [15,35] while keep the depth of network.\n\nSentence2: one is CDNwithDC, which is implemented without dilated convolution and uses traditional one-step data consistency layer instead.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An example of sub-Nyquist sampled MR Image is given in Figure 1.\n\nSentence2: it stands for how large portion of image can be seen by a neuron.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: [36] show that with careful initialization, residual networks can be trained as stable as those with normalization.\n\nSentence2: we also show the possibility of estimating RMS on a subset of the summed inputs, maintaining this invariance property.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For a fixed number of neurons, the network may follow either of these dynamics, depending on the initialization.\n\nSentence2: we show that kernel dynamics correspond to interpolation with cubic splines, whereas active dynamics yields adaptive linear splines, where neurons accumulate at the discontinuities and yield piecewise linear approximations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For appropriate initial distributions of neurons, the solution to kernel learning is a smooth cubic spline (Theorem 5).\n\nSentence2: arbitrary initializations will locally interpolate between these two regimes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: According to [34], the mixing matrix in OICA can be estimated up to the permutation and scaling indeterminacies (including the sign indeterminacy) of the columns.\n\nSentence2: these indeterminacies stop us from comparing the estimated mixing matrices by different OICA algorithms.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here we adopt MMD as the distributional distance as it does not require an explicit discriminator network, which simplifies the whole optimization procedure.\n\nSentence2: because conducting randomized controlled trials is usually expensive or infeasible, discovering causal relations from observational data, i.e.,causal discovery [1,2]) has received much attention in the past decades.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Causal discovery witnessed significant progress over the past decades.\n\nSentence2: many recent causal discovery methods make use of independent, non-Gaussian noise to achieve identifiability of the causal models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here we visualize the causal diagram estimated by LFOICA and the ground-truth in Figure 3(a) and 3(d).\n\nSentence2: we can readily extend our LFOICA algorithm to estimate the causal adjacency matrix B. Granger causal analysis has been shown to be sensitive to temporal frequency/resolution of time series.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, if assuming each IC follows a Mixture of Gaussian (MoG) distribution, we can simply derive the likelihood for the observed data.\n\nSentence2: the number of Gaussian mixtures increases exponentially in the number of ICs, which poses significant computational challenges.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In order to speed up optimization, we gradually increase the number of frames used.\n\nSentence2: we first optimize the parameters using only 1 simulated frame.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Regular simulators need to run one simulation for each input variable to compute the gradient, while our method only needs to run once for all gradients to be computed.\n\nSentence2: the more input variables there are during learning, the greater the performance gain that can be achieved by our method over finite-difference methods.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Collisions that actually happen in each step are very sparse compared to the complete set.\n\nSentence2: we use a dynamic approach that incorporates collision detection and response.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Each collision involves a vertex-face or edge-edge pair, which both have 4 vertices and 12 variables.\n\nSentence2: the original matrix size (n + m = 13m) should be about 13 times bigger than in our method (m).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use a bounding volume hierarchy for collision detection [27], and non-rigid impact zones [12] to compute the collision response.\n\nSentence2: we solve a cubic equation to detect the collision time t of each vertex-face or edge-edge pair that is sufficiently close to contact: where x k and v k (k = 1, 2, 3) are the relative position and velocity to the first vertex.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Somewhat analogously, in the sequential machine learning setting, modern machine learning methods, such as artificial neural networks, can be drastically disrupted when presented with new information from different domains, which leads to catastrophic interference and forgetting [19,14].\n\nSentence2: to retain long-term memory (for both human and machine learners), it is crucial to devise teaching strategies that adapt to the underlying forgetting mechanisms of the learner.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our theoretical framework is inspired by recent results on sequence submodular function maximization [43,36] and adaptive submodular optimization [10].\n\nSentence2: [43] introduced the notion of string submodular functions, which, analogous to the classical notion of submodular set functions [15], enjoy similar performance guarantees for maximization of deterministic sequence functions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Overall, GR achieved higher gains compared to the baselines.\n\nSentence2: dataset We simulated concepts of two different types: \"easy\" and \"difficult\".\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here, we highlight the differences with some recent work in the machine learning literature involving forgetful learners.\n\nSentence2: [44] aimed to teach the learner a binary classifier by sequentially providing training examples, where the learner has an exponential decaying memory of the training examples.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From the ablation study, we observe that each component plays its own role in a complementary way.\n\nSentence2: recently, gradient-based meta-learning methods [10,36] have been successfully applied to few-shot learning, with a procedure purely leveraging gradient descent.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In line with our goal of domain generalization, the model is trained on a sequence of simulated episodes with domain shift.\n\nSentence2: at each iteration, the available domains D are randomly split into sets of meta-train D tr and meta-test D te domains.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because of this delay, we want a model which predicts brain activity to have access to the words that precede the timepoint at which the fMRI image is captured.\n\nSentence2: we use the 20 words (which cover the 10 seconds of time) leading up to each fMRI image as input to our model, irrespective of sentence boundaries.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our observations that the fine-tuning reduces [CLS] attention to the [SEP] token can be interpreted in these terms.\n\nSentence2: further analysis is needed to understand whether this reduction in attention is specifically due to the task of predicting fMRI recordings or generally arises during fine-tuning on any task.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Fu et al. (2017) proposes a way to estimate a latent space that is high-dimensional both in time and space from simulated fMRI and MEG activity.\n\nSentence2: effectively combining fMRI and MEG/EEG remains an open research problem.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The summation and elimination of inner indices is called contraction.\n\nSentence2: the basic idea is that tensors are distinguished only by the indices they own and we consider them as vertices.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, molecular systems biology studies molecules (e.g., gene products, proteins) in a living cell that interact according to biochemical laws.\n\nSentence2: structural causal models support counterfactual inference, but do not identify the mechanisms.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Predicting the outcome of an intervention requires us to model the system.\n\nSentence2: discrete-state continuous-time Markov process models unambiguously describe the changes of system components across all the system states (i.e., not only at equilibrium) in term of hazard functions [11,28].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We model the system as a continuous-time discrete-state Markov process M with hazard rates functions in Table 1.\n\nSentence2: create \"observation\" and \"intervention\" models (Algorithm 2 lines 3-4) In a probabilistic programming language, the deterministic functions in Eq. (19) are specified with a Dirac Delta distribution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The evaluation stems from the insight that noise at the equilibrium captures the stochasticity in the Markov process trajectories.\n\nSentence2: we repeatedly simulate pairs of the trajectories with and without the counterfactual intervention, with a same random seed in a pair, such that each pair has an identical stochastic component.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose a novel deep segmentation method that learns to segment with correct topology.\n\nSentence2: we propose a topological loss that enforces the segmentation results to have the same topology as the ground truth, i.e., having the same Betti number (number of connected components and handles).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is understandable; while cross entropy loss gradient is applied to all pixels, topological gradient is only applied to a sparse set of critical pixels.\n\nSentence2: the weight needs to be much smaller to avoid overfitting with these critical pixels.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To have meaningful topological measure on these small patches, we apply relative persistent homology as a more localized approach for the computation of topological structures.\n\nSentence2: for each patch, we consider the topological structures relative to the boundary.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note we adopt a cubical complex discretization, which is more suitable for images.\n\nSentence2: as shown in the figure on the right, with the additional frame, a Y -shaped branching structure cropped within the patch will create two handles and be captured by persistent homology.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose a novel method that learns to segment with correct topology.\n\nSentence2: we design a continuous-valued loss function that enforces a segmentation to have the same topology as the ground truth, i.e., having the same Betti number.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We assume a binary segmentation task.\n\nSentence2: there is one single likelihood function f , whose value ranges between 0 and 1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It then forces the network to learn patterns near these locations, at the expense of overfitting and consequently slightly compromised per-pixel accuracy.\n\nSentence2: on this version of the Rand index we exclude the zero component of the original labels (background pixels of the ground truth).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: So does the optimal mapping γ∗.\n\nSentence2: they are both moved closer to the matched persistent dot in Dgm(g).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In Fig. 7, for most membrane pixels, the network learns to make correct prediction quickly.\n\nSentence2: for a small amount of difficult locations (blurred regions), it is much harder to learn to predict correctly.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These overlaps between patches propagate correct topology everywhere.\n\nSentence2: correct topology within a patch means the segmentation can be a deformation of the ground truth.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Unlike the classical MABs, the arms have non-stationary reward distributions.\n\nSentence2: we will focus on the class of restless bandits whose arms change their states based on Markov chains.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Below we discuss two popular methods for large scale score matching.\n\nSentence2: there are two main challenges with this approach.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We then produce samples using Langevin dynamics, which approximately works by gradually moving a random initial sample to high density regions along the (estimated) vector field of scores.\n\nSentence2: our sampling strategy is inspired by simulated annealing [30,37] which heuristically improves optimization for multimodal landscapes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: First, we do not need to sample from a Markov chain during training.\n\nSentence2: the walkback procedure of GSNs needs multiple runs of the chain to generate \"negative samples\".\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Nonequilibrium Thermodynamics (NET [51]) used a prescribed diffusion process to slowly transform data into random noise, and then learned to reverse this procedure by training an inverse diffusion.\n\nSentence2: nET is not very scalable because it requires the diffusion process to have very small steps, and needs to simulate chains with thousands of steps at training time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We then produce samples using Langevin dynamics, which approximately works by gradually moving a random initial sample to high density regions along the (estimated) vector field of scores.\n\nSentence2: there are two main challenges with this approach.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: With a lower memory footprint, our method allows training with larger batches in each iteration and better utilization of available GPU resources, making it more practical and economical to deploy and train very deep architectures in production environments.\n\nSentence2: training with fewer features per layer leads to much lower accuracy, while the naive approximation baseline simply fails, highlighting the importance of preventing accumulation of errors across layers using our approach.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To examine the reason behind the robustness of our method, Fig. 4 visualizes the error in the final parameter gradients used to update the model.\n\nSentence2: we take two models for CIFAR-100-at the start and end of training-and then compute gradients for a 100 batches with respect to the convolution kernels of all layers exactly, and using our approximate strategy.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our experiments, we use quantization to K-bit fixed point representations as a simple and computationally-inexpensive approximation strategy to validate our method.\n\nSentence2: experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method yields performance close to exact training, while storing activations compactly with as low as 4-bit precision.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, we need to store all intermediate activations during training because they are needed to compute gradients during back-propagation: ( 5)-( 8) involve not just the values of the incoming gradient, but also the values of the activations themselves.\n\nSentence2: training requires enough available memory to hold the activations of all layers in the network.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A number of works focus on reducing the memory footprint of a model during inference, e.g., by compression [7] and quantization [11], to ensure that it can be deployed on resource-limited mobile devices, while still delivering reasonable accuracy.\n\nSentence2: these methods do not address the significant amount of memory required to train the networks themselves (note [7,11] require storing full-precision activations during training), which is significantly larger than what is needed for inference.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A major obstacle to using approximate activations for training is that, with the standard implementation of back-propagation (backprop), approximation errors compound across the forward and then backward pass through all layers.\n\nSentence2: we need to store all intermediate activations during training because they are needed to compute gradients during back-propagation: ( 5)-( 8) involve not just the values of the incoming gradient, but also the values of the activations themselves.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For max-pooling, exact back-propagation through the pooling operation would require storing the arg-max indices (the number of bits required to store these would depend on the max-pool receptive field size).\n\nSentence2: since max-pool layers are used less often in recent architectures in favor of learned downsampling (ResNet architectures for image classification use max-pooling only in one layer), we instead choose not to approximate layers with max-pooling for simplicity.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We visualize the evolution of training losses in Fig.  3, and report test set errors of the final model in Table 1.\n\nSentence2: since this is typically at the final layer for most convolutional networks, and computing these activations is immediately followed by back-propagating through that layer, approximating these activations offers no savings in memory.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When the goal is to just compute the final output of the network, the activations of an intermediate layer can be discarded during the forward pass as soon as we finish processing the subsequent layer or layers that use it as input.\n\nSentence2: we need to store all intermediate activations during training because they are needed to compute gradients during back-propagation: ( 5)-( 8) involve not just the values of the incoming gradient, but also the values of the activations themselves.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We plot the average squared error between these gradients.\n\nSentence2: these methods likely represent the best possible solutions if the goal is restricted to computing exact gradients.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This in-turn can lead to the computation being inefficient and \"memory-bound\": it forces the use of smaller training batches for each GPU leading to under-utilization of available GPU cores (smaller batches also complicate the use of batch-normalization [12] with batch statistics computed over fewer samples).\n\nSentence2: practitioners are forced to either use a larger number of GPUs for parallelism, or contend with slower training.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While self-training has shown some gains in standard accuracy [25], the consistency-based approaches perform significantly better on popular semisupervised learning benchmarks [33].\n\nSentence2: our paper considers the very different regime of adversarial robustness, and we observe that robust self-training offers significant gains in robustness over fully-supervised methods.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures.\n\nSentence2: most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Domain adaptation (DA) solves this problem by building a model utilizing the knowledge of label-rich dataset, i.e., source domain, which generalizes well on the label-scarce dataset, i.e., target domain.\n\nSentence2: it means that inference of pseudo labels is easily influenced by imbalance distribution of samples in different classes where certain classes would dominate the process of self-training and cause errors accumulation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: First, feedback is only binary: either the customer buys the item, or she does not.\n\nSentence2: the platform must learn from censored feedback.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The subset of arms for which the reward is revealed at time t depends on the covariate x t , and the exact price p t from the above subset.\n\nSentence2: the assumption of a pre-defined graph structure is not satisfied.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: DEEP-C with Rounds: Theoretical analysis of regret for arm elimination algorithms typically involves tracking the number of times each sub-optimal arm is pulled before being eliminated.\n\nSentence2: this is challenging in our setting, since the set of arms which get \"pulled\" at an offered price depends on the covariate vector at that time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For our setup with d = 100, while both Decoupled DEEP-C and Sparse DEEP-C perform similar in average regret, we find that Sparse DEEP-C significantly outperforms Decoupled DEEP-C in standard deviation and in 95th-percentile.\n\nSentence2: 95th-percentile of Sparse DEEP-C is 33% lower than that under Decoupled DEEP-C.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, there are some crucial differences.\n\nSentence2: these works assume (a) a discrete set of arms, (b) the existence of a sequence of graphs indexed by time (possibly fixed) with the arms Contextual Non-parametric residuals Binary feedback Kleinberg and Leighton (2003) X X as its nodes, (c) the action involves pulling an arm, and at each time the reward at each neighbor of the pulled arm is revealed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information describing the customer's history and the product's type to predict her valuation of the product.\n\nSentence2: her true valuation is unobservable to the seller, only binary outcome in the form of success-failure of a transaction is observed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our work, we assume a parametric model for this relationship.\n\nSentence2: we presume that the expected value of the logarithm of the valuation is linear in the covariates.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The MNL contextual bandit is a multinomial generalization of generalized linear contextual bandits [23,30], particularly logistic bandits, that reduces to generalized linear bandits when the assortment contains a single item.\n\nSentence2: this extension is non-trivial since the MNL model cannot be expressed in the form of a generalized linear model [15]; hence, the results of generalized linear bandits do not directly apply.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also, in contrast to the standard contextual bandit problems, in the MNL contextual bandit, the item choice (feedback) is a function of the entire offered assortment.\n\nSentence2: regret analysis is more complicated.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The proof of Theorem 2 utilizes the anti-concentration property of the maximum of Gaussian random variables for ensuring frequent optimism.\n\nSentence2: we show in the following lemma that the proposed optimistic sampling can ensure a constant probability of optimism.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: They suggest that further supervised pretraining of models can provide significant benefits.\n\nSentence2: they propose supervised pretraining on contact prediction and remote homology detection, and show it increases performance on secondary structure prediction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From a theoretical point of view, among all these works, some algorithms are backed up with convergence results [4] or have quality of approximation guarantees materialized by a recovery bound [1].\n\nSentence2: there is still a lack of convergence rate analysis for the scalable Tucker problem.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As shown in Figure 6(a), our initial experiments showed that it is hard to capture shape details such as holes and thin structures when only global image features are used.\n\nSentence2: we introduce a local feature extraction method to focus on reconstructing fine-grained details, such as the back poles of a chair (Figure 6).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To represent 3D shapes, many of these methods utilize either voxels [2][3][4][5][6][7][8][9] or point clouds [10] due to ease of encoding them in a neural network.\n\nSentence2: such representations are often limited in terms of resolution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An SDF simply encodes the signed distance of each point sample in 3D from the boundary of the underlying shape.\n\nSentence2: given a set of signed distance values, the shape can be extracted by identifying the iso-surface using methods such as Marching Cubes [17].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To overcome this issue, Insafutdinov and Dosovitskiy [28] introduce a distilled ensemble approach to regress camera pose by combining several pose candidates.\n\nSentence2: dISN first estimates the camera parameters that map an object in world coordinates to the image plane.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To address this problem, we introduce a local feature extraction module.\n\nSentence2: we estimate the viewpoint parameters of the input image.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We define precision as the percentage of the generated points whose distance to the closest ground truth point is less than a threshold.\n\nSentence2: we also report the results of DISN using estimated camera poses and ground truth poses, denoted by 'Ours cam ' and 'Ours' respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The classical analyses for non-private SCO depend crucially on making only one pass over the dataset.\n\nSentence2: a single pass noisy SGD is not sufficiently accurate as we need a non-trivial amount of noise in each step to carry out the privacy analysis.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We argue that the approximate proximal operation will essentially have no impact on the guarantees of A ProxGD as compared to those of A NSGD .\n\nSentence2: in terms of privacy, the sensitivity of the approximate gradients (evaluated via the approximate prox operator) will basically remain the same as that of the exact gradients.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Aside from extending the analysis of this approach to population loss, we show that it can lead to algorithms for private SCO that use only near-linear number of gradient evaluations (whenever these assumptions hold).\n\nSentence2: we give a variant of objective perturbation in conjunction with the stochastic variance reduced gradient descent (SVRG) with only O(n log n) gradient evaluations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, for convex Lipschitz losses in d-dimensional Euclidean space, the best bound on the population loss achievable via uniform convergence is â„¦( d/n) [Fel16].\n\nSentence2: sGD is known to achieve excess loss of O( 1/n) which is independent of the dimension.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In general, Longer input sequences yield better recognition results.\n\nSentence2: one problem arising for a model requesting more input frames is that the GPU resources required for training and inference also significantly increase in both memory and time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The proposed architecture is based on a combination of a deep subnet operating on low-resolution frames with a compact subnet operating on high-resolution frames, allowing for high efficiency and accuracy at the same time.\n\nSentence2: we shall demonstrate the benefit of leveraging more frames for temporal modeling in Section 4.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Several of these methods are based on the observation that if all the candidate subsequences in the Viterbi algorithm converge at some point, then all subsequent states will share a common subsequence up to that point [32,33].\n\nSentence2: these methods do not guarantee reduction in latency since, in the worst case, they still need to process all the rewards before producing any output.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We evaluate the performance of any online algorithm in terms of its competitive ratio Ï, which is defined as the ratio OP T /ON .\n\nSentence2: the algorithm then leverages the subsequence w [i,i+L] to decide its next state Å·i .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, similar results were obtained when we used 3 beams to approximate the optimal Î³-discounted reward within each (L + 1)-long peek window.\n\nSentence2: we can potentially design fast yet accurate heuristics for some low latency settings.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Unlike Peek Search, this method does not discount the rewards on paths.\n\nSentence2: the algorithm first selects a reset point uniformly at random from {1, 2, . . . , L + 1}.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our algorithms are efficient dynamic programs that may not only be deployed in settings where Viterbi algorithm is typically used but also, as we mentioned earlier, several others where it is impractical.\n\nSentence2: our work would potentially widen the scope of and expedite scientific discovery in several fields that rely critically on efficient online Markov decoding.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The corpus is not divided into separate train and test sets.\n\nSentence2: we formed 5 random partitions each having 80% train and 20% test sentences.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The proofs revolve around our novel âˆ†-dimensional prismatic polytope constructions, where each vertex corresponds to a state.\n\nSentence2: (Sketch) The algorithm gives up reward on at most âˆ† steps every L + 1 steps, however these steps are cleverly selected.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In particular, OSA agreed with the optimal algorithm (Viterbi) on only 58.8% of predictions under both entropy and expected classification error measures suggested in [24].\n\nSentence2: just with L = 1, Peek Search matched with Viterbi on 77.4% predictions thereby outperforming OSA by an overwhelming amount (over 30%).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We leverage insights from Randomized Peek Search to translate its almost optimal expected performance to the deterministic setting.\n\nSentence2: we introduce the Peek Reset algorithm that may be loosely viewed as a derandomization of Randomized Peek Search.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that we do not make any distributional assumptions on the rewards for any (L + 1)-long peek window.\n\nSentence2: our algorithms accommodate various settings, including those where the rewards may be revealed in an adaptive (e.g.  non-stochastic, possibly adversarial) manne\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: (Right) If the graph is not fully connected, some of the transitions may not be available (e.g. state 4 to state 1 in our case).\n\nSentence2: the online algorithm might not be able to join the optimal path in one step, and thus may have to forgo additional rewards.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We also provide the first results on the limits of online Markov decoding under latency constraints.\n\nSentence2: we craft lower bounds for the online approximation of Viterbi algorithm in both deterministic and randomized ergodic chain settings.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We resolve the fundamental problem of online decoding with general n th order ergodic Markov chain models.\n\nSentence2: we provide deterministic and randomized algorithms whose performance is close to that of the optimal offline algorithm even when latency is small.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On the other hand, we found OSA to be significantly suboptimal in terms of both log-probability and label agreement.\n\nSentence2: oSA agreed with the optimal algorithm (Viterbi) on only 58.8% of predictions under both entropy and expected classification error measures suggested in [24].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, we establish that no online algorithm can perform significantly better than our algorithms.\n\nSentence2: our algorithms provide strong guarantees even for low latency, and nearly match the lower bounds for sufficiently large latency.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The facility costs for a and b are respectively f and 0.\n\nSentence2: we can assume the facility b is always open.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The expected connection cost of the solution S 1 is O(1) times that of S 0 .\n\nSentence2: combining the two lemmas, we have that the expected cost of the solution S 1 is at most O(1/ )opt, finishing the proof of Theorem 6.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Intuitively, we can imagine that the central curator should treat all copies independently: the input data for one copy should not affect the decisions we make for the other copies.\n\nSentence2: it is tricky to prove this.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Almost all existing methods assume that the labelled nodes are randomly selected.\n\nSentence2: the probability of missingness may depend on the unobserved data after conditioning on the observed data in the real world.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On the other hand, the normality assumption of the error term can be restrictive for GNM consisting of (1) and (3).\n\nSentence2: in the real data part, GNM is also compared with the model with a misspecified ignorable missing mechanism, and some other state-of-art 'de-biasing' methods.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is the first model selection result for contextual bandits with non-vacuous regret for all values of d m , and to the best of our knowledge is the first positive result of this type for any online learning setting with partial information.\n\nSentence2: our problem of estimating E m,i clearly falls into this general setup if we uniformly explore the actions for n rounds, then set {x s } n s=1 to be the features obtained through the feature map i and {y s } n s=1 to be the observed losses.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In any case this function is run only once on each node (line 33): all pending messages are canceled (lines 36-37) and the decision is published (line 40) with a unique message identifier that is produced using grpId as a seed (line 39).\n\nSentence2: if several nodes make a decision in the same session, all messages carrying this decision will have the same identifier and will thus be considered as du­plicates of the same message by the communication layer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Once an interim topology is formed, the process with the lowest id becomes the leader, and only that process takes up the responsibility of rebuilding the power-law topology.\n\nSentence2: in the domain of man-made systems, if the topology of a power-law netÂ­work gets altered due to failures or adversarial attacks, then remedial actions to restore the power-law property are very important.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: All such slots are summed and are assigned as the credit of link ej denoted as creditj .\n\nSentence2: some of these slots may have been already allotted to the links preceding ej in this group Gk.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In [5], the authors propose a heuristic for determining the transmission order by ranking links along a path.\n\nSentence2: their ranking does not allow slot reuse along a path and is for minimizing the round-trip delay -not the end-to-end delay.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The scheme results in end-to-end delay spanning multiple frames.\n\nSentence2: because the TDMA frame length is minimized, the end-to-end delay is minimized while increasing the throughput.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Reuse energy as much as possible.\n\nSentence2: we recycle the discharging current of the LCD to reduce energy consumption.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We assume that at any time interval, a node is in contact with at most (N - 1) nodes.\n\nSentence2: snw and EBR routing defines specific number for message copies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These protocols replicate a fixed number of messages to the encountered nodes.\n\nSentence2: in Epidemic [20] and Prophet [13], unlimited copies of mes­sage is replicated in the network for increasing the chances of delivery.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is important to note that our system is designed to be transparent to already-existing streaming infrastructure in order to ensure both vendor neutrality as well as simplified deploy­ment.\n\nSentence2: we do not operate our own stream source, but instead we assume that there is an already-­published stream, for instance from a Content Distribution Network (CDN), that can be played directly from its source and our customers would transparently use our system to minimize the load on that source.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This can be direct interactions between peers, such as the communication over a chat, but also less immediate elements, such as the higher probability of porting to a friend’s location who can then act as initial seeder for that region.\n\nSentence2: physical proximity (e.g., when inducing low latencies between peers) can also influence the overlay topology as well as the function of specific peers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: According to our measurement, the content popularity decays quite fast with time since content consumers attention mainly focuses on the newly created content.\n\nSentence2: the out-of-date content will lose user interests very soon.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: They show that EFT (Earliest Finish Time first) scheduler outperforms in most cases.\n\nSentence2: it also requires a performance model and parameters for given tasks before scheduling.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition,the appearance of the light (e.g., repeated lamp pattern) might also in­terfere the stripe pattern in the captured image.\n\nSentence2: we first apply a noise subtraction technique to reduce the noise.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It however has a limit because the allowed strip width is eventually constrained by the resolution of an image.\n\nSentence2: as a result, it is very hard for a light to dynamically adapt the level of redundancy required by a specific receiver.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To understand the causes of these problems, we classify the unsynchronized scenarios into two cases.\n\nSentence2: in practice, a receiver can only measure the number of rows occupied by a strip W as an integer estimate of Wreal ,and demodulate the symbol by 1f′ = 1 2W Tr.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, the exposure duration of the tested smartphones cannot be configured manually.\n\nSentence2: we reduce the distance of the light-to-camera link so that the camera can automatically pick a low exposure duration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Existing systems that either have low data rate or support only repetitive transmission can ignore this problem since small error in strip width estimation would not affect the bit error rate.\n\nSentence2: to boost the throughput, a high order frequency modulation is needed, in which case small error in strip width estimation would result in a high bit error rate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Signal losses in a NLOS link hence only occur during the aforementioned idle time gap.\n\nSentence2: aNLOS link experiences relatively constant losses, while a LOS link might suffer from dynamic losses, depending on not only the idle time gap but also the size and the location of the captured light in the image.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, when the original SNR was low (12 dB with minimum Data PHY rate of 385 Mbps), no link connectivity is possible irrespective of the blockage position.\n\nSentence2: under human blockage, beam dilation can help maintain link connectivity, but only if the original link SNR is high, and when blockage is sufficiently far away from the transmitter/receiver, such that only part of the beam is blocked.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The measurement reveals new challenges pertinent to MAC protocols involving ultra-directional, flexible 60 GHz beams.\n\nSentence2: 60 GHz links become highly sensitive to human blockage and device motion, posing non-trivial tradeoffs between link quality and responsiveness, especially w.r.t. the beam searching and interference-aware scheduling.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: beam to quasi-omni leads to 3Ã— higher throughput, despite its lower bit-rate (when steering overhead is not considered).\n\nSentence2: beam steering overhead is much smaller compared with the device motion case, and SNR-maximizing beam-steering can still be effective\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The theory of optimal experiment design [2] has established a set of such objective functions.\n\nSentence2: maximizing the determi­nant of FIM (aka D-optimality )leads to a design that min­imizes (a bound on) the volume of the error ellipsoid, and minimizing the trace of the inverse FIM (aka A-optimality) leads to a design that minimizes (a bound on) the average mean squared error (MSE).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similarly, node 5 takes the place of node 1, while node 1 becomes part of the corresponding degree 1 chain.\n\nSentence2: whenever a peer departs, the child of this peer in any Gi can connect to his parent in that Gi.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this scheme, BSs are also deployed in the sensing field and are supplied with limited available energy.\n\nSentence2: keeping all deployed BSs always active is not energy effcient and we have to optimize both the energy effciency of BSs and that of sensor nodes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We can bound the number of consecutive iterations such that there is no improvement in the max-flow.\n\nSentence2: every such iteration will add at least one node to the source set.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: depends on the topology of G, our proposed policy LFBP can adapt to the topology changes and efficiently track the optimal solution.Additionally, the loop free structure of a DAG is preserved under link removals.\n\nSentence2: can be chosen such that it is greater than the largest difference between the rescaled states.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An edge E i. bidders i and j conflict with each other when they use a same channel simultaneously.\n\nSentence2: a channel can be allocated to multiple bidders as long as no edge exists between any pair of these bidders, which is referred to as spatial reusability.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, list B is obtained by walking through the tree layer by layer from the root node and from left to right on each layer.\n\nSentence2: whenever we visit a new node, we append it at the tail of a FIFO list.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Existing auction mech­anisms are mainly proposed to be strategy-proof to stim­ulate bidders to reveal their valuations of spectrum truth­fully.\n\nSentence2: (a) It can effec­tively improve bidder utility, giving bidders incen­tive to cheat.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: With network diversity, the cooperation could lead to higher robustness to fading and mobility due to a diversity gain.\n\nSentence2: there could be static clients that experience low coherence times due to the dynamics of the environment they are in (e.g., human motion, closing/opening of doors).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The main objective here is to observe the influence of performance factors on the systems and test the workability of our method.\n\nSentence2: in section III, we present our service quality-based reimbursement model.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In [8], we showed that GameOn can network players with reliable p2p connections.\n\nSentence2: a question we did not answer is how to improve the second dimension of reliability, i.e., forming a group with sufficient long collocation time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: GameOn uses p2p network techniques such as Wi-Fi Di­rect and Bluetooth to avoid long latencies of the cellular network.\n\nSentence2: gameOn has to take the peer connectivity into consid­eration so that peers can reach out to each other directly once a game session gets started.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although physical systems can often be modeled and described using first principles from physics and conservation laws [1], it is fundamental to ensure that the values of the parameters in the model describing the plant be inferred from input/output data.\n\nSentence2: it is required to ensure identifiability of the corresponding dynamical systems [2]; in particular, due to the unknown nature of the parameters that we aim to determine (i.e., to identify) it is common to consider structural identifiability – two models are the same if and only if the parameters describing this model are the same [3].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In any case, the application of a hard decision mechanism on an analog signal (i.e., single thresholding 0 or 1 , all-or-none) requires careful control of its features to operate without additional calibration mechanisms.\n\nSentence2: moreover, a two bit additional selection input is considered to enable a user to choose the possible 4 frame_size.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By statistics analysis, we find that the traffic density of the aggregate traffic load of different days do not change obviously.\n\nSentence2: the traffic density of different hours varies significantly.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Researchers found out that the inhomogeneity in the spatio-temporal distribution of the data traffic leads to extremely insufficient utilization of network resources.\n\nSentence2: it is important to fundamentally understand this distribution to help us make better resource planning or introduce new management tools such as time-dependent pricing to reduce the congestion.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The Root-MST algorithm is suitable for real-time application.\n\nSentence2: the MUSIC-MST is suitalbe for the high accuracy DOA estimation when the real-time is not necessary.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The conclusions are drawn in Section 6.\n\nSentence2: the computational complexity is very high.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Then the teleoperator can have a understanding of the detected environment and operate the telerobot pre­cisely and efficiently.\n\nSentence2: in the link, the idea of cognitive radio is utilized and an adaptive chan­nel switching mechanism is employed to avoid unpredictable interferences.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The internal jammer plays a twofold role: we assume ADV has previously compromised the secret keys of a set c = {0, 8, 16, 24} of nodes of the network (with N = 32) , while retaining further jamming capabili­ties.\n\nSentence2: aDV jams the transmissions with proba­bility  pA ∈ [0.1, 0.9]: the adversary firstly jams the frequencies related to the keys it is in possession of, than if it has enough jamming power, it random chooses other frequencies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Pairs in TDBS are pre-computed during the off-line phase.\n\nSentence2: the protocol is “static” —with respect to the network pairs— and cannot deal with the join of a new node: a new node joining the network involves a new computation of all the 1-factorizations for all the network pairs, and therefore, all the pseudo-random frequency sequences must be invalidated and re-computed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that a hardness result for execution time follows also from the hardness of the communication cost when using a sin­gle object for all transactions, since the optimal TSP tour for an object provides the best time bound for scheduling the object in its transactions.\n\nSentence2: this reduction does not prove that the execution time is hard to approximate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Therefore, an execution schedule in G has duration χ time steps if and only if H has a valid coloring with χ colors.\n\nSentence2: fig. 1 highlights the sets of transactions to use q1,2 in block B1\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It also presented mutated binary search which takes advantage of the prefix length distribution for each specific prefix.\n\nSentence2: the algorithm makes branching decisions based on the characteristics of the sufixes of the visited prefix entry, reducing the average number of memory references significantly.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These URL datasets may not fully characterize the NDN forwarding rules in the future.\n\nSentence2: as namespace design and name assignment principles are still being studied, the characteristics of the NDN FIB tables have not been determined, and it is even possible that multiple namespaces with distinct characteristics may co-exist.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The trans­lation lookaside buffer (TLB) is employed to accelerate the address translation procedure.\n\nSentence2: for applications re­quire large memory space and expose scarce memory access locality, the page-based virtual memory system consumes a considerable amount of CPU cycles due to TLB misses [28].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The prefetching performance with CityA was better than SipE, but the benefits of prefetching all buckets still diminished due to excessive prefetching.\n\nSentence2: more efficient name parsing and hash implementation as well as prefetching strategies are needed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is the first Interest-based forwarding strategy shown to be correct in the presence of Interest loops, Interest aggregation, faults, and the forwarding of Interests over multiple paths.\n\nSentence2: unlike ndnSIM in which the next hop selection for requested prefixes is based on hop count, in SIFAH next hops are sorted based on rank of each FIB entry.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To satisfy the requirement described in Subsection 2.1, we consider content name elements as follows.\n\nSentence2: these results show that the system can access the newest content and can change the retrieval stream dynamically by changing only the requested content name.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Experimentation in real testbed using GeoNetworking implementation can provide very precise evaluation result in the deployment phase of the implementation, however it requires heavy cost (i.e. vehicles, equipment, drivers, time, etc.).\n\nSentence2: the simulation can provide the evaluation result in flexible networks with various scenarios in a low cost, however the result is based on the preconfigured model in the simulation and often diverse from the experimental evaluation in the real testbed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Then, we can know that when an accident occurs on road (1), vehicles on roads (2), (3) and (4) are expected to go to road (1) with probabilities 3/8, 3/8 and 1/3 respectively.\n\nSentence2: with those probabilities, we can represent the roads using the spanning tree in figure 3.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The STRIDE Threat Model does not rate the extent of a threat, it only reveals if the system is vulnerable to it.\n\nSentence2: the STRIDE method is not applied.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Again, as the wireless links are direct point to point and are not shared with any wired paths, they do not contribute to any cyclic dependencies involving either wired or wireless links.\n\nSentence2: the dynamic MAC operates as a token passing MAC when the wireless utilization is higher than a pre-set threshold.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In line 23, a generic monitoring socket (msock) is created.\n\nSentence2: a new event can be defined by a built-in event and a custom function that further determines the condition for the event.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When SRVCC is integrated into the network, existing threshold values should be adjusted to guarantee that inter-RAT handover never occurs before SRVCC.\n\nSentence2: even after verifying the proper threshold, call drops due to SRVCC failures are still captured, which indicates that incorrectly ordered delivery of the signaling messages still exists.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To address these issues, recently several breakthrough systems [10 15] are developed for seeing through the walls with WiFi.\n\nSentence2: these past systems still need either customized devices\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These virtual antennas are wirelessly connected to the reader and carry the reflections through the backscatter signals.\n\nSentence2: an additional step is required to separate the reflections off the moving object from the backscatter signals reflected by the tags.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Specifically, the tag right below the phone has the maximum value.\n\nSentence2: our problem is to recovery the trajectory of the moving object behind walls and closed doors given a sequence of read frames.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the case of absence (or presence) of moving object, the tags will suggest a signal difference lower (or higher) than a threshold.\n\nSentence2: if the tags suggest the signal difference is lower (or higher) than a threshold, it indicates the absence (or presence) of moving object.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Deploying ECNs together with cameras can inevitably increase the infrastructure cost.\n\nSentence2: when wireless capacity is limited, the saved bandwidth by Vigil can be used to forward users' traffic, thereby recouping the cost of ECNs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: There is no one fixed choice of tperiod; it depends on the application scenario.\n\nSentence2: we achieve VLC in the dark by reducing the duty cycle of the light source to an extremely low level such that it becomes imperceptible (Figure 1) to human eyes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This new primitive enables VLC links to maintain always-on communication, greatly broadening its applicable scenarios and lowering the energy consumption of LED lights.\n\nSentence2: we achieve VLC in the dark by reducing the duty cycle of the light source to an extremely low level such that it becomes imperceptible (Figure 1) to human eyes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition, we used base station user current­ly connect as user's current location.\n\nSentence2: if a user switches frequently among very close base stations due to cell oscillation, methods of removing data noise is expected.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Handover incentives for self-interested wlans with overlapping coverage.\n\nSentence2: a low rate transmitter influences the throughput of all Wi-Fi transmitters and all transmitters achieve the same throughput.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When the Wi-Fi direct transmission rate is higher than the Wi-Fi hotspot rate, then the throughput to mobile A in Figure 1(a) is maximized if only the Wi-Fi direct link is used.\n\nSentence2: if the Wi-Fi hotspot rate is higher, then the selection that maximizes the throughput depends on whether the hotspot backhaul is a bottleneck.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When performing an assigned task, nodes will constantly report their findings while they might only communicate occasionally when returning from or moving to the task location, or while they rest.\n\nSentence2: the report messages sent during the actual task performance contain rather novel information with respect to overall situational awareness at the central command.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Currently, rescue organizations mainly use interactive voice communication during their missions [4].\n\nSentence2: the communication is very sensitive to delays.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The random traffic model creates rather static traffic due to constant time in­tervals and message sizes.\n\nSentence2: changes of the traffic intensity are not covered.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Classic forwarding schemes do not take into consideration the peculiarities of the THz communications which may limit the network performance.\n\nSentence2: a novel channel-aware forwarding scheme is proposed to overcome the frequency selective feature of the THz band via judicious selection of the next-hop node.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The longest forwarding scheme only experiences two hops to reach the sink hence the end-toend delay is low.\n\nSentence2: the long transmission distance of the first hop penalizes the capacity.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As a result, we find that the uniform throughput of a nanonetwork powered by energy harvesting is upper bounded by the power constrained bound [12].\n\nSentence2: the opera­tion of an energy harvesting enabled nanonetwork operates as a power constrained network if the energy conversion is perfectly performed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Sections 3 and 4 provide an upper and a lower bound for the throughput capacity.\n\nSentence2: implementing nanosensors will set very strict constrains in the area and maximum capacity of the energy bu↵ers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Several types of raindrop-size distributions are known and a specific attenuation model recommended by the ITU-R is usually being used for attenuation calculation.\n\nSentence2: they are not fully suitable for describing our artificial rain conditions which obey a different raindrop size distribution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A bigger bag is the more accurate because it models more interactions between nodes and octree areas.\n\nSentence2: the figure 3 indicates that increasing the bag volume over a liter consume more and more CPU.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Context discovery can also be achieved without perform­ing ambience fingerprinting, where crowdsourcing is used for the extraction of semantic location from sensor data (e.g., audio, im­age), without any need for fingerprinting [8].\n\nSentence2: this type of approach relies on heavy computer vision techniques (e.g., scene classification, optical-character-recognition, object recognition), as well as heavy natural language processing (e.g., speech recognition, sound classification).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This can also be achieved in the context of IoT, by associating access to beacon-advertised websites (e.g., by UriBeacons [14]) to the user's MAC address.\n\nSentence2: whenever a user clicks on a link that was advertised by a specific beacon, it creates an opportunity for the server to associate the user's MAC address to the IP address from which the website was accessed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the grocery store scenario, besides having the familiar functionality to search, see product recommendations and browse for products, IoT apps supported by Incognito will be able to provide richer, interactive shopping experiences.\n\nSentence2: ioT apps will enable more natural text query scenarios: What is popular with women?\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The specific requirements will depend on the design of the ecosystem.\n\nSentence2: additionally, we assume that the user has access to a service that maps the location to an authoritative server for that location/environment.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By exposing a little more information about their identity and shopping history, the user may be given new suggestions for what to buy or even access to special sales.\n\nSentence2: a user may not want to expose all of their personal information in a given context.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The stricter insertion rule of OPC, which only inserts chunks in sequence, reduces the DRAM accesses for insertions/evictions by roughly 40%, leading to better memory performance.\n\nSentence2: nevertheless, ICN caching has not yet met these expectations, receiving criticism for its effciency [8, 9], based on the debatable performance superiority of distributed in-network caching over independent caches at the network edge, as well as on the questionable support for packet-level caching by today's hardware.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By individually naming each packet, ICN allows routers to turn their queueing buffers into packet caches, thus exploiting the network s existing storage resources.\n\nSentence2: the perfor­mance of packet caching at commodity routers is restricted by the small capacity of their SRAM, which holds the in­dex for the packets stored at the, slower, DRAM.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: (b) The spatial distribution of the waypoints visited by humans can be modeled by fractal points.\n\nSentence2: people are more attracted to certain areas than to others, thus creating a clustering phenomenon around popular places that can be observed across several spatial scales [10].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We did not use the speed model suggested by the authors of SLAW because it is used to represent both pedestrians and people moving by other means.\n\nSentence2: we are only considering walkers in a campus.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Results corresponding to this metric are plotted in Figure 3.\n\nSentence2: we found that the MANET topology under SLAW shows a high level of connectivity, whereas ran­dom waypoint based models exhibit a high isolation ratio.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is important to take into account that different mobility models lead to differences in the spatial node distribution.\n\nSentence2: the first step in our evaluation methodology is to determine the connectivity properties resulting from the use of different mobility models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, for a route to be useful, it is needed that it remains valid at least until the reply for a query returns to the sender node; this is not always the case.\n\nSentence2: a flexible routing mechanism able to find alternate routes is necessary in order to maximize the benefits of the services provided by the MANET.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Besides, Figure 2(a) shows that the probability of having 2-hop neigh­bors with either RWP or HRWP is nearly zero.\n\nSentence2: this probability is about 0.3 considering the SLAW mobility pattern.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is directly affected by node mobility.\n\nSentence2: it is useful to determine the effects that node mobility causes on a routing protocol.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: All name mappings overheard by any (intermediate) node are stored for future use.\n\nSentence2: we realize a (partial) knowledge distribution in order to answer future requests more quickly.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If a node needs the corresponding IP address to a given name, it sends a request to the multicast group and the corresponding node answers the request again with a multicast group message.\n\nSentence2: the highest delay is observed in the DNS mode, because of the combination of the three time delays to find a route to the server, communicate with the server, and finally find a route to the node.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In mDNS, the request and the response are both sent multicast to the whole group – or in our system broadcast to the whole network.\n\nSentence2: the necessary information, e.g., finding an IP and a route to its host, can be gained in parallel rather than sequential.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Due to the appearance of the licensed users, the unlicensed users cannot access all of n channels, and we denote the channels not occupied by the licensed users as available.\n\nSentence2: both users can find out the set of available channels after taking a short sensing stage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the long term, addressing the problem of increasingly poor quality information obtained through channel sensing will require new approaches to medium access control in wireless networks.\n\nSentence2: given the large existing userbase of devices operating on CSMA-based protocols, in particular the 802.11 protocol family, it is worth considering what we can do to mitigate the problem without drastically changing the current protocols, as an interim solution while new protocols are developed and standardised.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This type of collision can be a large source of overhead if many nodes are waiting to transmit and we are using a small p value (or small contention window size in 802.11).\n\nSentence2: the rate of these collisions does not depend on a; it is instead a function of p (or contention window size).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Conversely, at low G, we see very high optimal p values, as there is so little traffic that the probability of collision is very low, and the best strategy is for nodes to simply transmit as soon as they have data available.\n\nSentence2: if we wish to replicate this behaviour using the contention window in the 802.11 DCF, both very high and very low values of p will be problematic.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This makes certain simplifications possible.\n\nSentence2: there is no notion of packet transmission time, since the packet arrival rate does not need to be consid­ered.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The goal of CSMA is to use channel sensing to gather more information about the channel state before transmitting.\n\nSentence2: the information gathered is not perfect — it does not exactly match the true channel state at any given time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, considering only the service rates of the cells to ac­count for their heterogeneity, as in JSQ-µ, is not sufficient and could even turn out to be counterproductive.\n\nSentence2: jSQ-I, that uses knowledge of femto channel state is system­atically performing better than pure JSQ.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We study various load balancing strategies that aid the machines to select a proper cell in such a multi-RAT heterogeneous network deployment scenario so that the access delay for the M2M traffic is minimized.\n\nSentence2: we derive the optimal static policy of choosing between the WLAN femtocell and the LTE macrocell.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The reason becomes obvious if we look at the optimal static assignment probabilities at the femto (Figure 4, lower panel), which indicates that for these values of arrival rates, it is always optimal to stay in the femto as macro already has enough users.\n\nSentence2: at higher val­ues of the arrival rate, FPI performs much better than the optimal static policy, and even the JSQ-based policies out­perform the optimal static policy by a good margin.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the proposed scheme, nodes periodically exchange constant-size hello messages with their one-hop neighbors.\n\nSentence2: the protocol's net­work complexity is constant in terms of the network size.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Consumer mobility is handled as if there is a failure in the network where unsat­isfied Interests will be retransmitted.\n\nSentence2: during this process the Consumer will experience a long delay to retrieve the data which may affect the QoS of real-time ap­plications [3].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: 3.1 Mobility Anchor Approach Schemes in this class are based on the Mobile IP proto­col [6], which is a proposed standard in the Internet Engi­neering Task Force (IETF) to support host s mobility in IP networks.\n\nSentence2: mobile IP uses fixed nodes called Home Agents that keep track of the location of all mobile hosts originally registered in the home network.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The Hybrid scheme takes advantage of both techniques.\n\nSentence2: regular Data will always be sent using the shortest path, while Data during handover will be tunneled and will not be dropped.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This process needs three Data structures: Forwarding Informa­tion Base (FIB), Pending Interest Table (PIT) and Content Store (CS) [23].\n\nSentence2: fIB is used to forward In­terest s to the nearest content location.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The existing ndnSIM (and ns-3) does not provide Wi-Fi handover between access points, nor a mobility management scheme.\n\nSentence2: two main updates are done to the cur­rent ndnSIM: AP handover handler and a base for mobility management schemes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The main functionalities of NDN are assumed to have the default values in ndnSIM.\n\nSentence2: ndnSIM uses an opportunistic scheme named as Cache Everything Everywhere for object caching [7].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Name-based information dissemination for fragmented networks in disasters.\n\nSentence2: rP chooses data mule b as the next hop because data mule b is likely to reach Shelter#4 faster compared with data mule c. Similarly, Shelter#4 selects data mule c as the next hop for a message to RP.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The sums of weights over the routes are 67 (57 + 10) and 74 (42 + 32), respectively.\n\nSentence2: a rendezvous point-based publish/subscribe mechanism to support information dissemination in fragmented networks has been proposed in [6].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The events can take place at dedicated venues, for example, arenas, or take place ad-hoc on the streets of a city or in the countryside.\n\nSentence2: the system functionality is implemented on a set of fixed NetInf routers together with a mobile streaming application developed for video recording and viewing on Android phones and tablets.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The manifest is an object (say, in xml) describing the properties of the content object that are relevant at the network layer.\n\nSentence2: the manifest includes the location of the object, some security properties and some information regarding the transport of the object, including for instance its size.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: There is a tension in making the manifest expressive, but at the same time, in keeping it sim­ple.\n\nSentence2: the DNS++ request also includes the source address, as the network needs to know which client is making the request for path optimization.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We collect the link delays to show the network performance is comparable to the NDN Testbed.\n\nSentence2: developers could evaluate end-to-end application performance before evaluating it in the NDN Testbed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: With respect to the first strategy, if a rational gateway manages to convince clients of a smaller level of deduplication, the gateway can charge higher prices.\n\nSentence2: with respect to the second strategy, the gateway may try to derive information about the contents of the uploaded files3; notice that acquiring information about users data can be economically beneficial for the gateway since the stored data can be misused, e.g.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is mainly due to the fact that BLS sig­natures are considerably faster to compute by the gateway when compared to RSA signatures.\n\nSentence2: as shown in Figure 4(b), BLS signatures are more expensive to verify by the clients than the RSA-variant employed in DupLESS.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Namely, Bitcoin relies on blocks, a hash-based Proof of Work con­cept, to ensure the security of transactions.\n\nSentence2: bitcoin peers need to find a nonce, which when hashed with the last block hash and the root of the Merkle tree accumulating recent transac­tions, the overall hash is smaller than a 256-bit threshold.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, we assume that all parties are synchronized.\n\nSentence2: at each point in time, all parties share the same view on the current epoch (e.g., its index number if epochs are represented by an increasing counter).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similar to [10], we instantiate GetRandomness by leveraging functionality from Bitcoin, since the latter offers a convenient means (e.g., by means of API) to acquire time-dependent randomness.\n\nSentence2: bitcoin peers need to find a nonce, which when hashed with the last block hash and the root of the Merkle tree accumulating recent transactions, the overall hash is smaller than a 256-bit threshold.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This condition minimizes the number of open nodes in the tree, and hence the effort in proving/verifying membership and cardinality.\n\nSentence2: by doing so, ClearBox enables cloud users to verify the effective storage space that their data is occupying in the cloud, and consequently to check whether they qualify for benefits such as price reductions, etc.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: According to [21], from context information, the services may be adapted, adjusting to characteristics of the current environment and from the user profile.\n\nSentence2: it is possible to provide more optimized and customized services, in order to offer a greater quality of experience to users.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the third and fourth experiments, it was observed that the time to restore the QoE after the degradation detection is 5 seconds.\n\nSentence2: the time of restoration of the QoE can range between 9 seconds, once the semantic engine is programmed to perform the inference process every 5 seconds.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The first group requires from 20% up to 40% more time for both elapsed and CPU time.\n\nSentence2: note that CPU time represents both the kernel-land and user-land CPU time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that CBC has 128 bit block size which adds overhead over CTR.\n\nSentence2: we recommend CTR for small size data.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Using IPsec, the encrypted data are independent of the MTU as IPsec fragmentation occurs over the encrypted payload.\n\nSentence2: reducing the MTU size increases the transmitted IP headers as well as networking packet processing, whereas encrypted data remain the same.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Intelligent Transportation Systems (ITS) include techniques and communication technologies between vehicles (V2V) and between vehicles and infrastructure (V2I).\n\nSentence2: iTS are not limited to road transport but include the use of information and communication technologies (ICT) for rail, water and air transports.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This can be explained by the fact that in the case of a low number of nodes, multiple forwarders retransmit the received messages since they do not hear other retransmissions (low number of nodes).\n\nSentence2: as long as the number of nodes increases, several nodes close to forwarders mute themselves since the sched­ules of other nodes expire first.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While this momentum is encouraging, a closer look un­der the hood reveals a less rosy picture: NFV products and prototypes tend to be merely virtualized software implemen­tations of products that were previously offered as dedicated hardware appliances.\n\nSentence2: this native API provides support for: zero-copy packet transfer over vports for high throughput communication between E2D and NFs, and rich message ab­stractions which allow NFs to go beyond traditional packetbased communication.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While this momentum is encouraging, a closer look un­der the hood reveals a less rosy picture: NFV products and prototypes tend to be merely virtualized software implemen­tations of products that were previously offered as dedicated hardware appliances.\n\nSentence2: nFV is currently replacing, on a one-to-one basis, monolithic hardware with monolithic software.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We selected it for consideration because it contains multiple 2@/112-dense prefixes, identified in the forthcoming results in Section 6.2.2.\n\nSentence2: the structure shown is markedly different from other plots.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: First, we monitor the landscape of devices that listen to DNS requests and reply with valid DNS responses.\n\nSentence2: we perform weekly Internet-wide scans in IPv4 for more than one year and enumerate the responsive DNS resolvers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We begin with a general overview of DNS resolvers in the IPv4 address space.\n\nSentence2: concurrent to our work, Scott et al. [32] probed the IPv4 address space for open resolvers to analyze DNS resolutions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our clustering mechanism helped to group similar HTTP responses and served as data exploration step to further categorize the results.\n\nSentence2: we assigned labels to each individual cluster and mapped these labels to website categories that classify the type of served HTTP content.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the second phase of our study, we take the viewpoint of a client system to analyze the integrity of the DNS reso­lutions provided by open recursive DNS resolvers.\n\nSentence2: we aim to understand whether the resolvers actually return legitimate answers or perform bogus resolutions to provide clients with false IP address information.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To verify that our results are not biased due to Internet-wide scans performed by other organizations or research projects, we monitored incoming DNS requests to our network and identified a small set of hosts that performed DNS scanning activities.\n\nSentence2: we observe a single daily DNS request initiated by the Shadowserver Foundation and single DNS requests from Team Cymru and the Open Resolver Project within our monitoring period of 10 days.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To aggregate HTTP data, we use the IPs returned by the resolvers upon resolving the domains and request HTTP information as if they belong to the original website that we tried to contact.\n\nSentence2: measurements on the DNS Protocol.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We found these executables to be linked to malicious software.\n\nSentence2: we analyzed the samples dynamically and observed them to at­tempt downloading further (potentially malicious) executa­bles.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Prior to learning, we bin data as described in Section 0 5.1.1.\n\nSentence2: we use only 5 bins for each management practice (instead of 10), because the amount of data we have is insuf.cient to accurately learn .ne-grained models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By comparing cases that have the same propensity scores i.e., an equally likely probability of being treated based on the observed confounding practices we can be confident that the actual presence or absence of treatment is not determined by the confounding practices.\n\nSentence2: a treated case and an untreated case with the same propensity score have the same probability of having a given value for a confounding practice (e.g., number of roles); thus propensity score matching mimics a randomized experiment.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The main issue is that the components output by PCA are linear combinations of a subset of management practices.\n\nSentence2: this approach also makes the im­plicit assumption that linear combinations of practice metrics can explain network health.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We find that basic learning algorithms (e.g., C4.5 [27]) produce mediocre models because of the skewed nature of management practices and health outcomes.\n\nSentence2: they over-fit for the majority healthy network case.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Given that ticket logs capture a wide-range of network issues, operators view tickets as a valuable measure of network health.\n\nSentence2: operators from the OSP indicated that number of tickets is a useful metric.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To model control plane structure, we leverage prior work on configuration models [5].\n\nSentence2: we extract routing instances from device configurations, where each instance is a collection of routing processes of the same type (e.g., OSPF processes) on different devices that are in the transitive closure of the adjacent-to relationship.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Furthermore, not every device is changed every month in 77% of networks less than half of a network's devices are changed in a given month but most devices are changed at least once per year in 80% of networks more than three-quarters of the devices are changed in a year (Figure 12(b)).\n\nSentence2: changes occur frequently, and to different sets of devices in different months.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Many of the certificates that we find in the scans are invalid (e.g., self-signed certificates on WiFi routers).\n\nSentence2: we pre-process the dataset by verifying all observed certificates.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition to being the first approach to reliably de­tect differentiation from mobile devices without special privileges, our study also identifies several instances of middleboxes violating end-to-end principles for a variety of popular apps.\n\nSentence2: we find evidence of video transcoding, HTTP header manipulation in requests and responses, and proxies that modify TCP behavior.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, using our testbed containing commercial traffic shapers (Section 4.2), we found that some shapers will by default label random traffic with high port numbers as peer-to-peer traffic, a common target of differen­tiation.\n\nSentence2: this approach is unreliable to generate control trials because one can reasonably expect it to be shaped.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In particular, we can attribute the increased number of sources in recent years to bugs and misconfigurations in P2P networks (Qihoo 360 Safe and BitTorrent).\n\nSentence2: sources sending P2P traffic generally produce few connection attempts at irregular intervals.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The Bro Scanner category is based on Bro's definition of a scanner: contacting at least 25 unique des­tinations on the same port within 5 minutes [46].\n\nSentence2: the mix of popular applications changes regularly [41], which reduces the predictability of IBR.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We cannot validate all path changes from the hosts sending IBR, as we do not know when these hosts start sharing links to the darknet.\n\nSentence2: aS-level path changes should be observable in both Ark data and IBR.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Like PlanetSeer and Hubble, passive traffic measurements such as IBR can help inform when and where active measurements would be most useful [31, 51].\n\nSentence2: for some IP addresses the inter-observation time is still short (27% of IP addresses have a median inter-observation time of less then 1 hour).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although IBR originates from many sources, we lack control over who sends it and when.\n\nSentence2: the mix of popular applications changes regularly [41], which reduces the predictability of IBR.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Kumar et al. examined IBR from the Witty worm to extract host uptimes.\n\nSentence2: since Witty targeted a buffer overflow in network security products, the number of networks they could analyze was limited (inferring uptime for only about 800 machines) and not diverse (about a quarter of the machines were from only two institutions) [32].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For these devices, the paradigm for user-interaction has shifted away from on-device interfaces to richer, downloadable soft user interfaces (UIs) on remote devices like smartphones.\n\nSentence2: requiring the involved process of app installation for every new device, as is currently prescribed in industry, is unrealistic and does not scale well.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For these devices, the paradigm for user-interaction has shifted away from on-device interfaces to richer, downloadable soft user interfaces (UIs) on remote devices like smartphones.\n\nSentence2: when a user selects an item from the list, the best possible interface for a given device is opened within the app, presenting a user interface that may interact directly with the device.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The emerg­ing user interface paradigm for many such things eschews physical buttons, knobs, and displays in favor of virtual interfaces that are downloaded from the web and rendered on remote platforms like smartphones.\n\nSentence2: such smartphone app-based interfaces often require tedious discovery and installation, as well as device dis­covery, pairing, and configuration before a user can interact with a nearby device.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As the flying foxes dataset is relatively sparse in recorded contacts between animals, we model behavior and motion patterns of the animals based on empirical traces.\n\nSentence2: we classify the animal behavior into one of three states: resting, foraging, and travelling between foraging sites.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: PROPHET [14] attempts to improve delivery probability for a limited number of copied messages by sending copies to the best nodes it meets.\n\nSentence2: conceptually, there is need for balancing the energy spent on sensing, data mulling, and delivery of direct packets to destination.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To save energy, the static nodes work in a (q, p) duty-cycle schedule, which impacts the packet receiving for the static node.\n\nSentence2: the probability for the static nodes to be successfully synchronized and localized is ana­lyzed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the proposed scheme, if the static node successfully receives 4 or more packets, it could be synchronized and localized.\n\nSentence2: due to the mobility of the mobile node, the limited communication range of the nodes, and the (q, p) duty-cycle scheme, the probability of successfully receiving at least 4 packets is not always 1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To detect or enforce neutrality through technical means, we need a concrete technical definition.\n\nSentence2: the way our community has defined neutrality, non-neutral networks may be preferable to neutral ones.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Presto argues that the main culprit of inef.ciencies in schemes like ECMP is the coarse granularity: each flow, even a large one, hashes all its packets onto one path.\n\nSentence2: presto par­titions flows into equal size chunks of 64KB, called flow­cells, and sprays them in a round-robin fashion among available paths.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The well-connected nature of PlanetLab nodes means that our results are more representative of enterprise users rather than wireless or home users.\n\nSentence2: some of our insights (e.g., loss correlation on wide area paths) have broader applicability.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, OVS3 had already finished FlowMod processing when it received the packet.\n\nSentence2: oVS3 received it with the new routing information, i.e., clockwise type.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Network routing information can be dynamically modified with ease using software-defined networking (SDN).\n\nSentence2: it is not possible to simultaneously change the routing information in all the switches in a network.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As described in Sect. 3.3, geometric characteristic of eNBs is a major positioning impairment which can affect the performance of trilateration based positioning; however, the conventional methods do not utilize the characteristics of hyperbola equations, when they perform weighted averaging in multiple OTDOA based method.\n\nSentence2: in this paper, we propose a novel weighting method which can reduce the weighting value of several OTDOA from unsuitable deployment of eNB in terms of geometric characteristic.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Three different algorithms are proposed for performing with the proposed switching circuit in order to achieve both energy harvesting and data communication from Wi-Fi signals.\n\nSentence2: the attenuation sharply increase when the distance increase, on the other hand the received signal strength decrease.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The important point is that the increase in the number of coordination sets imposes an overhead on the processing resources (BSs) and on the backhaul (X2 links).\n\nSentence2: there should be a trade-off between the number of coordination sets, and the overhead on the BSs and the backhaul.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This way, we are able to study the effects of upload algorithms on the UEs performance while the UE's distance from their serving BSs increases.\n\nSentence2: we used the CD++ toolkit as the platform to implement the DEVS models.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Regarding availability, quick fail-over in case of emergencies is challenging [82,88].\n\nSentence2: higher redundancy is needed a priori to achieve acceptable availability.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Subsequently, we present a general algorithmic framework to solve the QMRP in an online manner.\n\nSentence2: given the computational complexity of the problem, we employ a sampleselect approach, where in the first stage, a set of feasible paths is sampled, and subsequently one of them is selected for the actual embedding (cf. Section 4.2).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To evaluate the potential of CXPs, we map the global Internet substrate for pathlet stitching over IXPs.\n\nSentence2: we outline a novel abstraction of the Internet topology, in which vertices are IXPs and edges are virtual links connecting two IXPs over an ISP.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, many servers will synchronously transmit data to the same server in data center networks.\n\nSentence2: it is desirable to design a transport protocol with zero packets dropping even there are massive concurrent flows.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, the flow needs another round to take back the new congestion window W [n + 1].\n\nSentence2: in TFC, a flow will send a marked packet without any payload after the flow establishment phase to take back proper congestion window.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, fast convergence can be satisfied.\n\nSentence2: 3) Switches do not need to maintain states for each flow and the transport control function at end hosts become more simple.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Fastpass [31] uses a centralized arbiter to determine the time at which each packet should be transmitted as well as the path to use for that packet.\n\nSentence2: the scalability of Fastpass is limited by the centralized arbiter that needs to deal with all the packets.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is because each flow has the same congestion window and flows n3 pass fewer hops than flows n2.\n\nSentence2: the goodput of flows n3 is a little larger than flows n2.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The source then transmits 1,500-byte packets that are received and forwarded by DF s relaying design.\n\nSentence2: the destination applies DF s ML de­coding to decode the packets and measure the error rate to map the real decoding SNR.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, note that a relay system is especially helpful when the source-destination link is too poor to ensure reliable transmissions.\n\nSentence2: the reasonable operation range of a relay system is when SNRsr > SNRsd , which means that PDF is a better relaying choice in practice.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Section 4 introduces our DF design, and Section 5 discusses some practical issues.\n\nSentence2: amplify-and-forward [1][2] is the simplest relay approach that amplifies the received signals and forwards them to the destination.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The key idea is to use the short Beam Sounding frames to probe alternate fail-over sector pairs before falling back to BFT.\n\nSentence2: mOCA retransmits the Beam Sounding frames first by traversing one level higher in the beamforming codebook, yielding a widened sector.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We implement all algorithmic aspects of MOCA in software and couple it with the FPGA software-defined-radio WARP combined with a 60 GHz frontend.\n\nSentence2: we implement a 60 GHz programmable node and testbed using VubIQ [19] transmitter and receiver waveguide system, operating in 57-64 GHz unlicensed frequency band with 1.8 GHz bandwidth (compliant with 802.11ad).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Wider beamwidth selection, on the other hand, results in sector pairs with more resilience towards mobility, at the cost of lower data rates.\n\nSentence2: in MOCA, we devise an algorithm to dynamically adapt beamwidth in response to mobility such that the average link throughput is maximized.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A successful transmission results in a bit rate of r(B) for time tslot, whereas breakage leads to training penalty tBF T .\n\nSentence2: in MIMO systems below 6 GHz, mul­tiple streams and diversity are achieved exploiting the rich scat­tering propagation that is not present in the 60 GHz environment nor with switched narrow-beam antenna arrays.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast to IEEE 802.11n/ac techniques to support multiple spatial streams with multiple RF chains [2], 802.11ad requires only a single RF chain for the antenna array.\n\nSentence2: 802.11ad realizes Tx and Rx beamforming via analog phase shifters that are configured according to a predefined codebook of beamforming co­efficients, with each codebook entry referred to as a sector.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As mentioned in Section 6.1, when there is no variation around the tags, the HL-interval of each tag is stable and distinguishable.\n\nSentence2: we collect signals from 5 Alien tags in a stable environment and distinguish them using their HL-intervals.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We show the ESD trends of two tags in Fig. 11 (a) and (b).\n\nSentence2: according to Equation 4, we can infer that when H is getting closer to T , the amplitude of the oscillation will become larger.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The failure of the above attempt implies that the COTS reader cannot a.ord the distinction on the approach behavior to­wards a specific target.\n\nSentence2: we design a monitor, which sits near the tagged object and passively listens to, records, Table 1: Reader Commands and Parameters and analyzes the RF communication between the COTS reader and tags.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As the ID or identifier of each tag, the EPC memory contains PC, EPC and CRC-16, which are stored in the order of the most significant bit first (MSB).\n\nSentence2: pC is 16-bit long, and its first 5 bits (denoted as PC5 ) define the EPC length.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our emulator creates and manages virtual nodes accord­ing to user requirements.\n\nSentence2: the resulting emulated opportunistic network is composed of several opportunis­tic nodes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In an opportunistic network context, end-to-end delays, delivery ratio and drop ratio are very important factors that developers want to study before deploying their applications.\n\nSentence2: this set of network-related metrics is not sufficient to describe the impact of those networks: developers also need to test their applications from a user viewpoint, and answer the question: Is my application working as expected?\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To elimi­nate the errors in phase measurements, however, those exist­ing systems either require high-cost dedicated hardware or labor-intensive offline training effort, which rendering them impractical.\n\nSentence2: they either use a dedicated hard­ware which is well designed without hardware diversity [2], or they use a learning based approach which regards the hardware phase errors as a constant that adds into the true phase measurement [4].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the case of Macha Works, the Community Network LinkNet allowed for more community members to ac­cess the internet than just the leadership of institutions .\n\nSentence2: the network lowers the barrier for access for all community members.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Community Networks (CNs) are increasingly attracting attention from academics, donors [2], and policy makers [3].\n\nSentence2: they are being proposed as a potential solution to provide access to information and connection technologies in places where it is not profitable for the market forces to do [1].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The feasibility of creating dedicated bearers in LTE offers the potential to create such a bearer between the mobile device and an MEC server in the edge cloud.\n\nSentence2: the creation of every such bearer incurs additional overhead in the form of control messages.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The results indicate that until the network capacity is saturated (around 90 Mbps), the location of the MEC server plays the dominant role in the end-end application latency.\n\nSentence2: once the network capacity is saturated, the background traffic has a large im­pact to increase latency significantly in the conventional ar­chitecture.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, an underwater acoustic sensor node to perform the underwater sensing is designed and implemented.\n\nSentence2: we describe the design criteria, architecture and functional modules of underwater acoustic sensor node.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In practical, the signal approaches Gaussian distribution when large values of dispersion have been already accumulated.\n\nSentence2: a modeling error is expected for the first spans of an uncompensated system or at any number of spans for the dispersion compensated case where the residual dispersion is low.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similar comparisons have been performed in [10,11] but they are limited in uncompensated systems only investigating just a moderate number of system parameters.\n\nSentence2: in [10] the impact of different span lengths and fiber types is not included while in [11] very restricted combinations of system parameters have been examined.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Mining these relationship will be useful to our work as well as other internet measurement researchers.\n\nSentence2: russian and Brazilian detoured paths fall mostly in the I st , IIIrd and IV th quadrant.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Typically the delay of the clock path is difficult to reduce from the initial routing result since clocks are nor­mally hardwired with global routing resources; hence, our strategy is to add delay to the data path.\n\nSentence2: to cor­rect the hold timing violation in our example, we must add 0.3 ns of delay across the two connections spanning between data in and register A.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The problem of solving long-path timing violations has received the vast majority of attention in past research [2].\n\nSentence2: in industry we must also solve the problem of short-path timing violations to guarantee functionality of routed de­signs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Algorithms have been developed to synthesize application specific network topologies from a high-level specification consisting of connectivity and bandwidth/latency/energy requirements[10, 11, 14].\n\nSentence2: the approach[6] used by Cong et al. removes complexity from the generated network if it is known a priori that certain communication traces will never occur simultaneously.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Meanwhile, the changes to the GENIE variant yielded a small savings, requiring no major architectural changes.\n\nSentence2: gENIE's interconnect employs static, table-based routing, which is generated based on the logical Links defined by the designer in the input specification.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In a regular, homogeneous IaaS system, customizing an OpenFlow network would invariably involve software packet-processing using virtual machines.\n\nSentence2: in our infrastructure, we can use VFRs to accomplish this packet-processing at line rate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To test the feasibility of this approach we used our source-to-source translator to instrument expressions whose results were likely to survive HLS optimization.\n\nSentence2: we instrumented several benchmarks from the assignment operations whose final result was an array read or write, add/subtract, multiply, or function call.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The results are normalized to baseline.\n\nSentence2: eCP was used for pro­tecting hard errors.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When freeing a block, the corresponding no-use blocks are also reclaimed.\n\nSentence2: freeing a 16-page block in (1:2)-Alloc automatically forms a 32-page block after reclaiming its no-use buddy.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Write cancellation WC improves VnC but not significantly.\n\nSentence2: assume we use ECP-N where N represents the maximal number of errors that ECP can protect, and an adjacent line has accumulated X errors (WD errors and/or hard errors) before the current line write.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we propose SD-PCM for achieving reliable write operations in super dense PCM.\n\nSentence2: we focus on mitigating WD along bit-lines such that we can construct super dense PCM chips with 4F2 cell size, i.e., the minimal for diode-switch based PCM.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Scaling PCM in deep sub-micron regime faces nonnegligible inter-cell thermal interference during programming, referred to as write disturbance (WD) phenomenon.\n\nSentence2: the heat generated for writing one cell may disseminate beyond this cell and disturb the resistance states of its neighboring cells.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: At the device level, one bank stores 4096 SLC cells in one row, which spreads across eight PCM data chips.\n\nSentence2: one device row holds the content of one logical page in the OS.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we propose to record error locations and values in the ECP region, and rectify the adjacent lines only if there are more errors than what ECP can handle.\n\nSentence2: writing adjacent lines requires additional VnC processing, resulting in cascading verification and large performance loss.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The straightforward way to encrypt data at runtime is to employ a one-way function such as AES, and use a global key, as shown in Figure 2(a).\n\nSentence2: if the same key is used for all lines, one can simply compare encrypted lines to figure out which lines store the same content, and employ a dictionary-based attack.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Disabling encryption and using FNW would improve the system performance by 40%, on average.\n\nSentence2: dEUCE bridges two-thirds of the performance gap between encrypted memory and unencrypted memory.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our studies show that a typical writeback modifies, on average, only 12% of the bits in the cacheline.\n\nSentence2: dEUCE reduces the number of bit flips for an encrypted memory system from 50% to 24%, while incurring a storage overhead of 32 bits per line.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The reason why we are only concerned with the conflicts for the polygons creating a cutting line is because a polygon could be cut by multiple cutting lines but it creates exactly one cutting line.\n\nSentence2: the conflict cost of a path in the graph will not be over-counted and remain correct.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because there is no coloring conflict between the polygons in different rows [8, 11], the TPL layout decomposition problem for each row can be individually solved.\n\nSentence2: the TPL layout decomposition problem can be considered as a set of single-row decomposition problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As we redesign the solution graph to be the simple solution graph for a cell, the solution graph for a BCP cannot be directly used to combine with a simple solution graph.\n\nSentence2: we also devise a reduced simple solution graph for the BCP of two adjacent cells ci and cj .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Besides, a polygon can be split into two sub-polygons by inserting a stitch to further resolve conflicts.\n\nSentence2: [1, 12] pointed out that even stitch insertion can not avoid coloring conflict in DPL.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The effects of passive elements (i.e. metal extensions with zero current) such as those added at cathode (reservoir), or anode (sink), or dummy vias have been investigated in many works [17].\n\nSentence2: a compact model suitable for CAD tools that can explain and model the underlying physics has not been proposed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The total number of atoms must be equal in the initial state and in the steady state.\n\nSentence2: the number of atoms accumulated and depleted in different segments of the net is zero: ∫∫∫net Δc dV = 0\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although this model is very powerful, it does not capture the effect of segments adjacent to the path with the highest sum of the jL products, yet not belonging to it.\n\nSentence2: while the max jL product remains the same, the entire stress distribution might be affected by the segments not on the path with the maximum sum of\nthe jL products.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use a practical approach based on the Bellman-Ford algorithm [5] to solve the NCD problem with a worst case run-time complexity of O(V E).\n\nSentence2: our algorithm performs much better than O(V E) on the average because it contains two early termination features.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is well known that the variations-induced skew between sinks that are close in the topology is less than for sinks that are distant in the tree because of a lower point of divergence.\n\nSentence2: we propose to place the sinks corresponding to tight edges close in the topology, as shown in Figure 1(b).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As the cycle is of length 2, it is not possible to skew the margin in the cycle.\n\nSentence2: the maximum safety margin between the pair of sinks will be M no matter how the other safety margins are inserted in the tree.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose to insert a safety margin in each edge, or skew constraint, equal to the maximum margin of the edge (as defined in Section 2.1), with an upper limit of Muser.\n\nSentence2: only edges on the cycle with an average margin of M will have a safety margin of M. All other edges have safety margins strictly greater than M. All skew constraints corresponding to non-tight edges have a safety margin of Muser and do not require further consideration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The SNES is measured with respect to a slack target of 5 ps.\n\nSentence2: sNES may still be negative although the WSL is positive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This clustering score dc merely depends on netlist connectivity and circuit area.\n\nSentence2: many pairs of cells usually share the same score, which defers a great number of clustering choices to the actual implementation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similar as in FastPlace 3.0, we adapt the BestChoice clustering algorithm from [3], which iteratively clusters cells according to their netlist connectivity and size.\n\nSentence2: we compare our­selves to NTUPlace4 [13], the best participant of the con­test, and to the tool of Cong et al.  [10] which achieved the best results on these benchmarks so far.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Besides netlength optimization, our framework is able to handle different objectives like timing and routability behav­ior.\n\nSentence2: for the congestion-driven mode of our tool, we present a heuristic which dynamically adapts the threshold used to estimate the criticality of a routing edge.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The quality of the placement has far-reaching effects on the whole design process.\n\nSentence2: the total overlap is low because only a small ratio of cells is moved.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The non-timing critical blocks can in theory operate faster, but they are synthesized at the frequency of the critical block to save area and power.\n\nSentence2: even with degraded transistors, these blocks can be synthesized to operate at the frequency of the critical block, albeit with a larger area.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Fabrication in sub-20nm technology is limited by the standard photo lithography process.\n\nSentence2: this helps in printing non-uniform de­vice structures and in overcoming errors due to mask mis­alignment.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This architecture features two wide metal 1 (M1) power tracks and eleven M2 tracks for internal signal routing.\n\nSentence2: synopsys' initiatives also do not represent state-of-the-art technologies anymore.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since the pattern generation in this method is performed by modeling the patterns in all free spaces at the same time, the dense wire patterns are pushed as far as possible from each other, so that the resulting routing is more evenly dis­tributed.\n\nSentence2: this method cannot produce a glob­ally even distribution, because its iterative characteristic might lead to a local optimal result.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This suggests that the QueueVadis client is pretty effective in estimating the real queuing delay experienced by a user.\n\nSentence2: given that the total wait time at the F&B venue was approx. 4 minutes (240 secs) and 10 minutes at Theaters, a 10% estimation error would translate to errors of less than minute at F&B and 2 minutes at Theater.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In Figure 5b, we compare each of the conditions with the nominal condition.\n\nSentence2: we see how many bits remain the same across multiple measurements at the nominal condition as well as the extreme condition under test.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our observation of DRAM refresh and remanence properties, however, we noticed that certain DRAMs actually exhibit behavior similar to SRAMs, i.e. they have seemingly random startup values.\n\nSentence2: the cells do not initialize to 0 as would be ex­pected.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the nominal condition, DRAMs have 60% to 80% stable bits.\n\nSentence2: at high temperature the stability is actually higher at over 90% for three DRAMs).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In other words, we have a grid for memory rows and columns that can give us a very good picture of the cell distribution in the memory array.\n\nSentence2: spatial correlations (neighborhood stable cells) can be made in both x and y directions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The LP based solution utilizes the simplex algorithm and in practice, runs faster.\n\nSentence2: note that with certain inputs, simplex algorithm may require exponential time to reach a solution [17].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The delay of the clock propagation path (the delay between the clock arrival time to the ICG cell and the clock arrival time to the register gated by this ICG cell) is at least the ICG cell delay and is bounded by the longest path within the local tree.\n\nSentence2: each ICG cell is associated with a lower and upper bound of clock propagation path delay.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that in existing SC-based LDPC decoders, the SB length is fixed and decided at design time.\n\nSentence2: we utilize the communication channel conditions quantified by the current Signal-to-Noise Ratio (SNR) to dynamically adjust the SB length on-the-fly to the minimum value for which the QoS requirements are still satisfied.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For such platforms, the additional data transfers cancels the performance benefits of the accelerated decision making.\n\nSentence2: existing fused architectures where the CPU and GPU share the same die, or even system-onchips integrating a CPU, an accelerator cards, and a NIC [6] have common memory spaces, and require no additional data copies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Intuitively, the number of packets to be analyzed and forwarded in parallel should be limited by the number of cores that perform the analysis; for regular GPUs, one can have as many as hundreds such cores.\n\nSentence2: in practice, the number of parallel decisions is limited by the available memory bandwidth of the system: if every unit accesses memory in a short enough time interval, the memory bandwidth is quickly saturated, and the amount of concurrency in the sys­tem decreases.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Due to the dominance of MAC protocols over energy, a number of contributions have been made in their improvement.\n\nSentence2: contention based protocols can afford longer slots with large sleeping time and usually needs RTS/CTS exchange to enable collision free media access.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While stages 2 and 3 of our reference ConvNet can be perfectly split into 8—8 blocks and thus no performance is lost, Stage 1 has only 3 input channels and can load the core only with ..............=3/8.\n\nSentence2: stages with a small number of input or output channels also perform much less operations and efficiency in these cases is thus not that important.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The implementation of [18] suffers from the same issues, their superior energy efficiency in the processing core is gained with FDSOI technology which could also be applied to our architecture.\n\nSentence2: the effective throughput thus depends on the size of the image: ..............=(h....-h..+1)(......-....+1)h........... Table 2: Throughput and efficiency for the individual stages of our reference ConvNet.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As Cyclone always schedules instructions speculatively, a replay mechanism is implemented by using scoreboarding logic at the functional units.\n\nSentence2: when an instruction exits the queue, it checks if its operands are ready.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The counter is decremented by two on cycles where an L1 miss takes place, and incremented by one otherwise.\n\nSentence2: to get relevant numbers, we identify a region of interest in each benchmark using Simpoint 3.2 [21].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although this lengthens the dependency chain of the second load, scheduling misspeculation associated with cache bank conflicts nearly vanish.\n\nSentence2: if bank conflicts are not predicted in some fashion, they always cost issue-to-execute cycles whether dependents were scheduled assuming an L1 hit or not.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here, the processor tracks all the instructions that are in the shadow of a load.\n\nSentence2: all the instructions that have been issued at least load-to-use cycles after the load and that are inflight between the Issue stage and the Execute stage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our framework, two accesses conflict if they access a different set in the cache but map to the same bank.\n\nSentence2: two accesses to the same set do not conflict, as we consider a cache organization with a Single Line Buffer having two read ports, as described by Rivers et al. [22].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The choice of whether to put an instruction in the replay loop or in the replay queue is made by the checker.\n\nSentence2: not to be mistaken for the speculative scheduling of Stark et al. which relates to the atomicity of Wakeup & Select in the scheduler [26].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The former because µ-ops would have been replayed, and the latter because dependents would, again, have paid issue-to-execute cycles instead of bank-conflict delay cycles before re-executing.\n\nSentence2: if bank conflicts are not predicted in some fashion, they always cost issue-to-execute cycles whether dependents were scheduled assuming an L1 hit or not.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In non-linked data structures, for example, such as simple arrays and matrices, a sequential access may depend on a running index and not on the outcome of recent accesses (and can also be prefetched by simple stride prefetchers).\n\nSentence2: we still preserve weak data dependency through the index variable that is being incremented, and consider such cases as manifestations of semantic locality.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, identifying program semantics is particularly difficult when relying only on the executable binary, which lacks type and symbol information, and whose code is mangled by compiler optimizations.\n\nSentence2: we can only approximate semantic locality.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, PBFS-biased’s false-positiveinduced rollbacks slow down the main threads’ critical paths resulting in high performance loss.\n\nSentence2: sRT-iso fares much better than PBFS-biased.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Like conventional replay, our replay re-executes all the preceding instructions irrespective of whether the triggering instruction is (transitively) dependent on a preceding instruction.\n\nSentence2: this process assumes, similar to [19], that a fault is masked if it does not affect the processor state after the run-window.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A full rollback is expensive because our false-positive rates are low but non-negligible, and avoidable as we show below.\n\nSentence2: rename faults reach the filters upon instruction completion by which time many later instructions have been fetched all of which have to be squashed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A previous scheme employs bit-mask filters to capture value neighborhoods.\n\nSentence2: the scheme achieves low coverage improving which results in high false-positive rates, and performance and energy overheads.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because there is only one squash state machine for all the bit positions of a first-level filter, the cost is minimal.\n\nSentence2: we implement the delay via a small delay buffer that holds a few recently-completed instructions (e.g., 6-8) for potential replay in the near future, as shown in Figure 4.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because front-end stages are reexecuted in current pipelines only by a full pipeline rollback, we propose to squash for rename faults.\n\nSentence2: to avoid the performance and energy penalties of full rollbacks for every trigger, we need to distinguish between rename faults and false positives.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, while replay re-executes instructions following a load, a soft-fault trigger implies that a nearby preceding instruction was likely corrupted.\n\nSentence2: we propose a mechanism to replay nearby preceding instructions upon a trigger.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Overall, these results show that FaultHound achieves both good coverage and good false-positive rates, meeting the challenge stated in Section 2.2.\n\nSentence2: the remaining faults are binned as SDC, only for which we report coverage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the case of the commercial workloads, SRT-iso can hide the extra copy under the cache misses of the main thread, incurring low performance degradation.\n\nSentence2: pBFS-biased s false-positive­induced rollbacks slow down the main threads critical paths resulting in high performance loss.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because our proposal is a best-effort scheme, the preceding instructions may pre-maturely exit the issue queue and miss the opportunity to be replayed, resulting in loss of coverage.\n\nSentence2: a full rollback squashes all previous instructions in the pipeline so that the first instruction that was corrupted by a fault, as well as all later instructions to which the fault has propagated, are likely squashed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: ESP can exploit the above characteristics of events present in a large number of asynchronous applications -mobile and web 2.0 applications, desktop GUI applications, IoT, sensor networks.\n\nSentence2: events in web servers and node.js applications are either independent or do not hold up processing of future events, making it easier to expose event-level parallelism, without needing ESP.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also, this training happens so prematurely that other branches would have overridden it by the time it is needed.\n\nSentence2: if we are able to keep the branch predictor trained on branch outcomes of just enough future branches, it will have trained on the most relevant branches to perform well.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Fortunately, asynchronous programs also provide us with new opportunities for optimizations that are absent in synchronous programs.\n\nSentence2: if we want to reuse the computation from the pre-execution of an event, we would have to invest in some fairly complex hardware to check for any dependencies between events and recover from mis-speculations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The entire cluster is pipelined and is implemented with a data-driven flow control mechanism using a ready state bit in the reconfigurable datapath.\n\nSentence2: mAD's versatility frees future accelerator designers from having to redesign and rethink interfacing to the core and provides a general and efficient mechanism, freeing them to focus on computation mechanisms alone.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: FP benchmarks tend to be highly predictable, have low I$/ D$ miss-rates, and good MLP.\n\nSentence2: these benchmarks also feature quite biased forward branches and large basic blocks, which often permit the branch resolution to be scheduled in such a way as to avoid stalling the in-order.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While some machines will issue post-branch instructions in parallel with branch instructions (e.g., the DEC Alpha 21164 [11]), the benefit of doing so is limited, as the primary impact of control dependence is on compiler generated code schedules [20, 31].\n\nSentence2: even with perfect branch prediction, control dependence impacts performance on in-order machines.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is because, by the time a long-latency load event has been seen, it is already too late to try and prefetch its result.\n\nSentence2: prior works have focused on executing forward slices after a miss event [4, 8, 15, 18, 26], as it is relatively easy to propagate poison bits to squash future instructions or store them for later processing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For our silicon-photonic NoCs, we consider monolithically-integrated links [13, 25, 26].\n\nSentence2: conservative and aggressive designs employ different parameters for the photodetector sensitivity and laser efficiency (-17 dBm vs. -20 dBm in the former, and 20% vs. 30% in the latter, respectively).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, with the decreasing size of the semiconductor technology process, the static dissipation is increasing due to rising leakage currents and is becoming the major component of the power consumption.\n\nSentence2: when evolving digital circuits with respect to the power consumption, the area of the circuit can be used to estimate the power consumption [13].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We now demonstrate how the single-core processor of Sec. 4.1 may be converted into a dual-core processor with secure shared state.\n\nSentence2: we will extend the design of Sec. 4.1 essentially by instantiating two copies of the processor core, and wrapping them with a secure harness whose design is akin to a softwarebased monadic separation kernel [15].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Caisson [20] is based on a classic security type system [32] for a Verilog-like hardware definition language.\n\nSentence2: security in Caisson inherits the strengths and weaknesses of the type-­based approach; i.e., security is statically checkable, but some secure programs are excluded.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Generally speaking, the energy efficiency of a power converter is high when its input and output voltages are close to each other.\n\nSentence2: due to the high voltage of the vehicle battery pack [9], the string charger architecture has the potential for achieving high overall system efficiency.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition, high-voltage or highcurrent gate control is required for switches in the reconfigurable vehicular PV array.\n\nSentence2: we implement an IGBT (insulated-gate bipolar transistor)-based reconfiguration switch network to meet the above mentioned requirements.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the sensorless PV array reconfiguration framework, the timing and energy overheads are larger due to the solar irradiance estimation procedure.\n\nSentence2: it is beneficial to make the sensorless framework also event-driven in order to limit the total number of reconfigurations and thereby total timing and energy overheads.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If additional latency is then added to sub-circuit B, it will have only a minor impact on the overall circuit's total exe­cution cycles.\n\nSentence2: extending the latency of computations (i.e.the schedule) in sub-circuit B ensures that the critical path of the overall circuit is not within B.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: They develop an efficient algorithm to discover and constrain all multi-cycle paths in a circuit, while considering every reachable circuit state.\n\nSentence2: [3] does not report the speedup offered by multi-cycling in isolation; rather, it presents results for multi-cycling combined with bitlevel optimizations and control-flow graph restructuring.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A growing portion of chip area and energy consumption is expended in caches.\n\nSentence2: emerging memory technologies such as spintronic memories that promise high density and low leakage power are of great interest in cache design.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In order to access a bit stored in the tape, we need to shift the head to the desired location on the tape via the shift controller and then perform the required operation.\n\nSentence2: the access latency for a given bit stored in such a structure is variable and depends on the number of shift operations performed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Benchmarks (all benchmarks used in this paper are circuit matrices) are obtained from University of Florida Sparse Matrix Collection [19].\n\nSentence2: if matrices change dramatically, re-pivoting will always happen.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If so, we still use the same pivot choice as the previous factorization (line 13) so the symbolic structure of column i will not be changed.\n\nSentence2: speedup of the proposed solver compared with NICSLU.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, searching for all dependent columns from the subsequent columns will traverse all the subsequent columns so it is timeconsuming.\n\nSentence2: we use a simple method that once a re-pivoting occurs, all the subsequent columns are computed by the normal factorization algorithm.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When a channel break happens on a transistor of a DP TIG-SiNWFET gate, the redundant structure of the pass tran­sistors masks the e.ect of faulty transistor.\n\nSentence2: the black curves belong to the fault free device.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From the experimental results we conclude that the single bit faults (if injected) were biased at a particular bit position irrespective of the initial register states.\n\nSentence2: the frequency of occurrences of single bit faults varied for different key-IV pair initializations of the cipher.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To face with this problem, several emerging communication architectures such as 3D-stacked NoC, optical, and wireless NoC (WiNoC) architectures have been proposed [4].\n\nSentence2: the latter use a wireless backbone upon the traditional wire-based NoC [5], with the introduction of new elements such as antenna and transceiver.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Woo and J. Kim, Diversifying wear index for MLC NAND .ash memory to extend the lifetime of SSDs, in Proceedings of the 11th ACM International Conference on Embedded Software (EMSOFT 13), 2013.\n\nSentence2: this result implies that the proposed subpage pro­gramming can significantly improve the lifetime of flash memory in handling small writes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is demonstrated that neural network accelerators significantly reduce energy consumption [2] and at the same time exhibit a broad application scope [3].\n\nSentence2: neural networks can well accommodate the emerging high-performance machine learning applications such as recognition, mining, and synthesis (RMS) [4].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we leverage the error resiliency of neural network to mitigate timing errors in NN accelerators.\n\nSentence2: when timing errors significantly affect the output results, we propose to retrain the accelerators to update their weights, thus circumventing critical timing errors.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Such phenomenon, however, limits the effectiveness of timing speculation due to the performance/energy penalties associated with timing error correction [9].\n\nSentence2: when errors occur in a timing-speculative circuit, the system needs to be rolled back to a pre-error state for re-computation (usually with slower frequency), which incurs both performance penalty and extra energy consumption.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: They proved that conflict free memory mapping always exists to tackle collision problem for any code.\n\nSentence2: it is based on a simulated-annealing algorithm in which one cannot predict when the algorithm will end and the final architecture is not optimized.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Parameters and variables From the [O] model, we recall that a tile represents the minimal area unit considered within the FPGA grid and we denote by tileW and tileH its width and height respectively.\n\nSentence2: following one of the few approaches existing in literature, we use a Node-Arc model (better described in Section II) similar to the one used in [17] where each node is a Reconfigurable Region (RR), thus allowing high level analysis.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: (1) Each multi-pin interconnect is viewed as multiple segments, each representing the signal path from the driver to a receiver.\n\nSentence2: for an interconnect with one drive (X) and two receivers (R1 and R2), we will try to characterize the delays of two segments, namely, X-to-R1 and X-to-R2, respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this work, we have presented a more flexible architecture on which the VOT-based delay characterization can be more easily applied.\n\nSentence2: for an interconnect with one drive (X) and two receivers (R1 and R2), we will try to characterize the delays of two segments, namely, X-to-R1 and X-to-R2, respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As a result, the ECC is redefined to protect the other half of the content.\n\nSentence2: the ECC capability is implicitly doubled.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Besides, excessive use of bypass switches increases the cost of cooling system.\n\nSentence2: there is a trade-off between the number of switches and the power saving provided by this approach.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, the interconnect parasitic capacitances that are major contributors to performance degradation and on-die signal integrity problems, must be accurately accounted for.\n\nSentence2: an empirical-based parasitic extraction is performed on an early-stage layout obtained from the floorplan, computing the optimal electromigration-aware wiring topology and shortest rectilinear paths in-loop, without the need for detailed routing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Additionally, high-performance and highly-­scaled CNFETs have also been demonstrated [Shulaker14b-c, Franklin12a].\n\nSentence2: cNFETs are promising candidates for building the next-generation of high-performance and energy-­efficient digital systems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose to write back the record to the two SSDs in the mirrored architecture, with one copy for each drive, and upon the system waking up, a correct record is loaded into the host.\n\nSentence2: in order to distinguish the correct record upon an alteration of the records by a maliciously implemented FTL, the records stored in both drives must be authenticated.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our vector IR is similar to the original TCG IR, but the parameters buffer we put is the offset of the guest CPUState and guest SIMD registers, rather than the register's number.\n\nSentence2: we can simply and efficiently perform address calculations when generating guest SIMD instructions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because multiple reads are generated from a similar location in the DNA sequence, it is highly probable that multiple reads contain the same k-mer.\n\nSentence2: if the occurrence of a k-mer is very low we can assume that the kmer has erroneous bases.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although the amount of speedup achieved is signficant, it must be noted that our implementation has not utilized the full capabilities of the FPGA device.\n\nSentence2: it is designed to scale to a larger number of threads without hassles by tweaking parameter values in the source code.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The VCO is commonly used as a voltage-to-frequency converter in ADCs.\n\nSentence2: the performance of the VCO based ADC is limited by the nonlinear behavior of the VCO, frequency drifts over time due to 1/f noise, and process, voltage, and temperature variations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The technique proposed in the previous Section IV avoids entirely the usage of delay tables, but requires a large number of multipliers instead.\n\nSentence2: note that at the already at today s 20nm node, 3D-stacked Virtex UltraScale chips [20] feature twice the LUT count of the Virtex 7 family.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This concludes the proof of the theorem.\n\nSentence2: this measure captures the ongoing communication complexity of the scheme: In order to verify that the predicate holds, it is assumed that periodically, nodes transmit their labels to their neighbors, and each node runs the local veri.cation procedure on the complete neighborhood s label set.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In a configuration Gs, we are given a graph G = (V , E ), a state space S, and a state assignment function s : V → S. The state of a node v, denoted s(v), includes all local input to v\n\nSentence2: in a configuration Gs, we are given a graph G = (V , E ), a state space S, and a state assignment function s : V → S. The state of a node v, denoted s(v), includes all local input to v\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Second, the exact value of the probability parameter E is typically not very important.\n\nSentence2: the correctness of all the schemes in this paper is oblivious to E, in the sense that E can be chosen as close to 0 as desired by straightforward adjustments of our schemes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The goal and the prover in an RPLS remain exactly as defined for PLS.\n\nSentence2: in an RPLS the verifier v has access to a source of independent random bits at each node.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We design packet processing algorithms to extrapolate RSS values from legacy WiFi packets emitted by uncontrolled wire­less devices.\n\nSentence2: we can reduce the RSS noise by orders of magnitude, thereby substantially amplifying the tiny radio RSS disturbance caused by the target's audio vibration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, we show that several pragmatic counter-measures may make the eavesdropping significantly hard and thwart adversaries.\n\nSentence2: we analyt­ically provide a guideline of safety distance for a reflective victim and propose PHY-layer power randomization mechanisms for an emissive victim, which can reduce the eavesdropper's recovered audio quality by orders of magnitude.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When targeted at vanilla PC or smartphone loudspeakers, the attacker can success­fully recover high-quality audio even when blocked by sound-proof walls.\n\nSentence2: we propose several pragmatic counter­measures that can effectively reduce the attacker's audio recovery quality by orders of magnitude.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Cyclostationary is complex and requires priori knowledge even though it is good at low SNR and requires high sensing time.\n\nSentence2: in a smart home environment, where the sensing time and priori knowledge comes into play, en­ergy detection method is preferred to other.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We conduct the analysis through a stochastic approach, since the synaptic transmission is inherently stochastic.\n\nSentence2: we first analytically char­acterize the stochastic filtering of the spike train performed by each presynaptic terminal.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The aim is to develop radically new medical diagnosis and treatment techniques.\n\nSentence2: several questions and challenges arise to design a fully functional intrabody nanonetwork deployed inside the nervous system.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In both cases, the second capture clock pulse must be running at the nominal (normal mode) operating speed of the circuit under diagnosis.\n\nSentence2: in this case as well, scan output compaction schemes drastically reduce the diagnostic resolution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As a result the frame-level RMS energy of the filtered baseband signal mostly corresponds to the presence or absence of body movements (figure 3).\n\nSentence2: there may be some frames where relatively high RMS energy may be caused due to aperiodic changes in the machine (e.g when the machine switches) (figure 3).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also we can observe that orientation A yields relatively smaller error rate.\n\nSentence2: it still requires the user to either place a smartphone in their vicinity or wear a wristband while sleeping.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The runtime overhead breakdown for 100 fault-free iterations is shown in Figure 6a.\n\nSentence2: we immediately make a copy of the user-specified host memory contents to an internal host buffer and convert the blocking call to a nonblocking call.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The relative recovery time3 increases as we apply more optimizations because these decrease the VOCL-FT overhead and make executions more efficient.\n\nSentence2: when referring to (or plotting) a new optimization, those previously discussed for that particular experiment are also leveraged.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition to the improved QoS for endusers, CDN hosting makes a content publisher's service more scalable and reliable, saves the publisher money by reducing the load on their network backbone, and boosts pagerank­ing on search engines [11].\n\nSentence2: an increasing number of content publishers, ranging from small personal webpages and blogs [10] to large-scale social networking websites, are hosting their content on CDN systems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The GFW avoids IP address filtering in blocking CDN content because of its significant collateral damage, i.e., blocking (popular) non-forbidden content.\n\nSentence2: iP address blocking of forbidden websites hosted on a CDN network will disable all the legitimate websites hosted by the same CDN provider as well.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Fortunately, the Internet has been gradually diverging from the old end-to-end communication paradigm by embracing modern forms of communication.\n\nSentence2: caching Internet content, practiced to improve performance, security, and reliability, has revolutionized the way content is being communicated on the Internet.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The censors can alternatively block all IP addresses of the target CDN domain.\n\nSentence2: we have implemented a SWEET server on a Linux server in Amherst, which listens for incoming emails from Cache-Browser clients targeted to a specific email address.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Proxy-based circumvention systems, including domain fronting services like meek, FireFly proxy, and Flashlight proxy, are online ser­vices that are solely used for censorship circumvention.\n\nSentence2: the censors can pressure their hosting web ser­vices to entirely remove them.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The use of a private CDN can provide better control on content distribution and better privacy protection, however, its maintenance (e.g., ensuring security, reliability, and availability) may be complicated, costly, and prone to failures.\n\nSentence2: except for giant content producers, typical content publishers tend to use shared CDN services powered by commercial CDN providers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We design a system called CacheBrowser that leverages the censors challenges in blocking CDN content.\n\nSentence2: except for giant content producers, typical content publishers tend to use shared CDN services powered by commercial CDN providers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thanks to the business competition between numerous commercial CDN providers, offering low-cost or even free [13] CDN service, CDN hosting has become affordable even to host personal websites [10] and blogs [12].\n\nSentence2: some providers offer free CDN hosting services [13], including CloudFlare, Incapsula, CoralCDN, and SwarmCDN.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Many of these behaviors are physical such as punching or kicking.\n\nSentence2: there is no such a system for the vocal agitation metrics of Cohen-Mansfield Inventory.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From equation (4), we can observe that the leakage current is exponentially proportional to (Vgs-Vth).\n\nSentence2: layout of different approaches is also shown for fair comparison of different approaches.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These large LLC sizes, however, are beginning to challenge commonly held assumptions in operating system (OS) timeslice scheduling behavior.\n\nSentence2: fill times for large LLCs have grown to the point that they are a significant portion of the application’s scheduled timeslice, resulting in lost performance and efficiency, particularly in highly loaded, multi-process, multi-core environments.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: With the large 16MB LLC, it takes a consid­erable amount of time for applications to load their work­ing sets in the cache, thus the performance improvement increases from 15ms to 30ms timeslice length due to the in­creased amount of time to amortize LLC cold start cache misses.\n\nSentence2: beyond 30-45ms timeslices, there is only marginal performance improvement since the applications working set data is then loaded in the LLC and the cold start penalty amortized.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On the contrary, the graph workloads for dynamic graphs perform heavy graph structure/property updates and involve complex code structure and access patterns.\n\nSentence2: it is non-trivial to convert these workloads.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Recently, the advances in 3D stacking technology re-initiated the interest in PIM architectures.\n\nSentence2: we can have the bandwidth in FLITs as following.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Hence, in this section, we choose the breadth-first-search (BFS) as the object of our analysis.\n\nSentence2: the irregular accesses patterns of graph properties don't exist.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we design and implement a cost-effective RFID indoor localization system for locating objects attached with passive tags.\n\nSentence2: we deploy two guide rails, x-axis and y-axis guide rails, on the ceiling of a warehouse or workshop.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By using mobile readers mounted on guide rails, we design and implement a costeffective RFID indoor localization system, which requires neither reference tags nor RSSI functions, for stock-taking and searching in warehouse operations.\n\nSentence2: we install two guide rails, which can allow a reader to move horizontally or vertically, on the ceiling of a warehouse or workshop.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Timing was possible only for the two latter numbers because the starting time of the first key press was unclear.\n\nSentence2: all errors, including those made when pressing the first key, were recorded.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To avoid generating audible 200 Hz noise as a side-effect, every other channel was turned off by setting it to 0 and every other to 1.\n\nSentence2: the gross DC signal level of the entire array is kept at 50% at all times.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Whatever part of the hand or fingers was on the location received the feedback.\n\nSentence2: the feedback was given based on the key locations, not based on the hand location and shape.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is also worth noting that the individual elements of the set of labels we are concerned with (⇤)can be thought of as composite labels.\n\nSentence2: one can consider la­bels such as Return Air Temperature to be a composition of many individual tags (in this case return , air and temperature ).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The emerging Multi-Level Cells (MLC) STT-RAM memory technology improves the capacity and energy efficiency issues of large-sized memory banks.\n\nSentence2: mLC STT-RAM incurs non-negligible protection overhead to ensure reliable operations when compared to the Single-Level Cells (SLC) STT-RAM.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To achieve high compression rates HEVC tries to match the block structure according to the object shape through a recursive partitioning process which exponentially increases the motion search complexity as it has to be performed for every block partitioning.\n\nSentence2: 3) An Approximation-Aware Cache Management Unit (Section 4.3) that classifies a cache set as reliable (i.e. this set needs to be stored with reliability) or ‘unreliable’ (i.e. this set does no necessarily require reliability and can tolerate approximation errors).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When a set is classified as a USet, we improve the average associativity of the cache.\n\nSentence2: data stored in a USet is not protected against read and write errors.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To achieve high compression rates HEVC tries to match the block structure according to the object shape through a recursive partitioning process which exponentially increases the motion search complexity as it has to be performed for every block partitioning.\n\nSentence2: to achieve high performance, HEVC supports a light-weight data-parallelism support by dividing the video frame into so-called Video Tiles\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similarly, disintegration into a larger number of smaller chips requires a corresponding increase in assembly steps, for which we also do not have relevant cost information available.\n\nSentence2: in recent years, fundamental physical limitations have slowed down the rate of transition from one technology node to the next, and the costs of new fabs are sky-rocketing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While it makes sense to run each chip's portion of the NoC at the same clock speed (otherwise the slowest portion will likely become a bottle­neck), from a physical-design perspective it is easier to clock chips independently.\n\nSentence2: each chip's NoC por­tion operates in its own local timing domain, thereby requiring domain-crossing synchronizing FIFOs when going from CPU chip to interposer, and vice versa.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 9 compares the performance (execution time) of GS-DRAM, Row Store, and Column Store on the transaction workload for var­ious values of i, j, and k (x-axis).\n\nSentence2: we extend each cache line tag in the cache tag store with p additional bits to store the pattern ID of the corresponding cache line.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Al­though each bank is further divided into smaller structures [12, 26, 28, 41, 47], for the purposes of understand­ing our mechanism, the following abstraction of a bank is sufficient.\n\nSentence2: pseudo-random mapping schemes [38] potentially incur a small number of conficts for almost any access pattern.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Unfortunately, this approach is very costly as it sig­nificantly increases the pin count of the memory channel.\n\nSentence2: we need a simple and low cost mechanism to allow the memory controller to effciently communicate different access patterns to the DRAM module.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The administrator can decide how many times she wants to check the tag by listing the proposed positions.\n\nSentence2: there is no need for going back to the server to set the new position.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The visibility of the tags was approximately the same with dif­ferent seeing positions.\n\nSentence2: sensors cover all the lab area and the invisibility of tag happened for another reason, such as high interference in the lab.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In line with this, they would like to know if exercises caused any problems or complaints and whether the exercises were understood by the patient.\n\nSentence2: they emphasized that asking these questions will not be useful for every patient.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In order to deal with the size of the database, one could use one of a number of lossless compression techniques.\n\nSentence2: the database must be uncompressed eventually, meaning, while nonvolatile memory is reduced, the volatile memory must still be large enough to store the CRPs and ECC code words.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Prominent examples include two workshop series sponsored by the National Science Foundation (NSF) and Computing Community Consortium (CCC).\n\nSentence2: these workshops assess the inadequacies of current EDA funding and education, technological challenges facing the CMOS industry, and new markets to which EDA methods can be exported.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Prior approaches that attempt to overcome the computational bottleneck of error estimation can be classified into two categories: 1) Methods that estimate the range of error: These methods capture the range of the error in approximate computation in terms of its minimum and maximum value, and are primarily based on interval and af.ne arithmetic [11], with modi.cations [3], [10], [12] suitable for asymmetric distributions of errors in approximate circuits.\n\nSentence2: these approaches are computationally intensive, may lead to storage explosion [10], and often overestimate errors [13].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: OpenCL [12] is finding some success, also in commercial tools [13].\n\nSentence2: having been designed mainly for vector-based processors, it does not adapt well to irregular applications.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, the back-end generates the final circuit description in Verilog, together with the simulation and synthesis scripts that enables Bambu to directly interface with 3rd-party tools.\n\nSentence2: the Convey WX (Wolverine) accelerator is a PCI-Express drop-in solution that provides similar features.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To address the limitations of HPC systems, GEMS employs a runtime that provides: a global address space across the cluster, so that data do not need to be partitioned, lightweight software multithreading, to tolerate data access latencies, and message aggregation, to improve network utilization with fine-grained transactions.\n\nSentence2: designing accelerators by employing Hardware Description Languages (HDL) is hard and time-consuming.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Here the biased multiplier is assumed to truncate the lower bits without changing the value of the highest bit to 1 .\n\nSentence2: the lower bits are approximated by 0.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: At the same time, these applications also afford new opportunities for novel low-power implementations.\n\nSentence2: the common feature of these application domains is an inherent tolerance for limited and insignificant inaccuracies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use Verilog to implement the applications in hardware and use DC compiler for synthesis.\n\nSentence2: we also use MentorGraphics ModelSim to evaluate the numerical output of the designs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Abstract Short-reach nanophotonic interconnects are promising to solve the communication bottleneck in data centers and chip-level scenarios.\n\nSentence2: the nanophotonic interconnects are sensitive to process and thermal variations, especially for the microring structures, resulting in signi.cant variation of an optical link s bit error rate (BER).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our simulation and analysis demonstrate that the proposed adaptive tuning approach scales well with respect to different process variations, thermal variations, numbers of clusters, and laser types.\n\nSentence2: the adaptive tuning could save more power for link structures with non-uniform communication path lengths and/or using comb lasers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As illustrated in Fig. 8, the BER decreases when the received power decreases or when the sampling time deviates from the ideal sampling point.\n\nSentence2: the voltage offsetting method or the sampling time offsetting method could be leveraged to accelerate the BER test.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Threshold logic has been revisited as a promising logic style when considering emerging technologies, such as memristors, spintronics devices and tunneling diodes.\n\nSentence2: state-of-theart threshold logic synthesis tools are not based on specific technologies when defining cost functions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since the k-cuts are pre-computed in the input design, it is possible to mark a matchable/unmatchable label for each cut and indicate these marks to the technology mapper.\n\nSentence2: we filter those pre­computed functions and indicate only TLFs to be matched.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As shown in Table 2, the overall modeling cost is dominated by the PN simulation in this example.\n\nSentence2: once the PoMs are chosen, we need to further build a performance model to approximate the NF as a polynomial function of the PoMs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The key idea of CL-BMF is to pass the knowledge from the low-complexity model to the high-complexity model (i.e. co-learning) to reduce its training cost.\n\nSentence2: once the low-complexity model is accurately built based on a small number of training samples, the high-complexity model can be trained by borrowing the information from the lowcomplexity model, instead of relying on the expensive training data only.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because an attacker uses JTAG differently from a legitimate user, it is possible to detect an unauthorized access using customized machine-learning algorithms.\n\nSentence2: a JTAG protection scheme, termed SLIC-J, is proposed to monitor JTAG activity, detect malicious accesses, and ultimately protect the JTAG from being misused [31].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We assign a data to be written to each cluster.\n\nSentence2: we generate a one-to-many mapping code from each data to the codewords in the corresponding cluster.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We cluster its codeswords and generate a cluster graph satisfying the S-bit flip conditions.\n\nSentence2: we generate one-to-many mapping from each data to the codewords in the cluster.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A pipeline module is called a full buffer, e.g., PCFB, if its input and output data channels can hold distinct data tokens at the same time.\n\nSentence2: a half buffer, e.g., PCHB, WCHB, and NCL, can hold at most one data token on both of its input and output channels.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Compared with linear programming based methods, TSE cannot handle cycle times in real numbers naturally.\n\nSentence2: it can only solve the problem that has an integer (or a finite decimal rational number) solution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is obvious that the two operations introduce no frontier pins as the gate being inserted or removed is not connected to the current circuit.\n\nSentence2: for gate-level design modifiers we only deal with the operation repower gate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Both operations alter the structure of the design and directly affect the timing.\n\nSentence2: we need to identify the frontier pins that capture the incremental timing update for such changes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This is the case when there is no time remaining to solve hardware problems.\n\nSentence2: correcting hardware problems in the software seems relatively easy (Alvarez Cabrera et al. 2010, Bradley 2010).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Over the past 40 years, many definitions were presented (Colorado State University 2012).\n\nSentence2: defining a mechatronic system precisely becomes even more difficult.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The duration of each talk was set to be eight minutes (exactly as in the .rst WACI session from 1998) plus two minutes for questions.\n\nSentence2: amusing elements in the presentations are tolerated ;-) but are in fact optional.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Instead of soliciting talks via an open call and hoping for the best, we proactively invited speakers whom we believe are capable of delivering excellent WACI presentations.\n\nSentence2: this year's WACI session consists exclusively of invited speakers\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Six participants stated they feel more connected to people receiving shared information and their relationship is strengthened.\n\nSentence2: sharing may have adverse effects on sharers in case of not receiving adequate feedback [5]: P12: Depends on the information or feedback, sometimes feeling better, sometimes no change at all.\"\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To the best of our knowledge, there is lack of studies focused on cache prefetching for CGRAs platform.\n\nSentence2: the CDPM mechanism is aimed at cache prefetching in CGRAs platform and operates at the context level.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, inaccurate prefetching can waste limited memory bandwidth and induce pollution of scarce cache resources, limiting or even crippling CGRA performance.\n\nSentence2: existing prefetching techniques are aimed at instruction driven processors, where the decision to issue prefetch requests is based on historical information that relates to independent memory accesses.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 6 shows the contents of the pattern cache.\n\nSentence2: 13 dwarfs [23], which were defined to design and evaluate parallel programming architectures, were chosen as benchmarks for the study.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: improve performance by predicting which memory addresses will be accessed in the near future and then speculatively issuing prefetch requests.\n\nSentence2: inaccurate prefetching can waste limited memory bandwidth and induce pollution of scarce cache resources, limiting or even crippling CGRA performance.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As we will discuss in Sec­tion 3.2, source-level transformations also have drawbacks in both complexity of evaluating coding styles as well as analysis to determine how code will be affected by optimization.\n\nSentence2: we use LLVM-IR and LLVM's debug metadata to track source correspondence without these limitations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: (f ) The violating cell c2 is swapped with c6 to form a larger implant area with c5, without any area overhead.\n\nSentence2: as feature sizes decrease, design rules have become more restricted.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this case, only c2 is moved to be clustered with c1 and the initial clusters are maintained.\n\nSentence2: if the initial clusters are not good enough, it may significantly change the clusters to find smaller wirelength as shown in Figures 4(e) (f ).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: By repeating the above procedures for all large eigen­values, a good spectral graph sparsifier can be obtained.\n\nSentence2: computing the largest eigenvalue and its eigenvector can be too costly for large-scale graph Laplacian matrices, even when state-of-the-art eigenvalue decomposition methods are considered [18].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that the additional bandwidth may also be used to transfer up to cache blocks per read (instead of a single cache block as is done typically) if narrow-width values are present.\n\nSentence2: in this paper, we instead experiment with transferring 64-bit sets of data stored in the cache lines.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, one child said that his sense of the self and others was confused during the game and he felt like he became a different child.\n\nSentence2: it is believed that simulating different sensory experiences between ASD and TD and creating a place to realize such people with ASD has demonstrated pluralistic possibilities in the way people perceive the world according to different sensory experiences.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This type of sensory experience that blurs the boundary between the self and others is a phenomenon that has been commonly reported in the studies done by people with ASD.\n\nSentence2: it is believed that the device developed for this study is able to give TD people the ASD sensory experience.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The interviewee reported that a high percentage of the visually impaired students he tutors give up on mathematical learning goals, particularly in the area of geometry and primarily due to challenges in visualization of the concepts.\n\nSentence2: in line with formal statistics [4], he noted that visually impaired students do have the potential to excel in this area.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As a result, while combining multiple energy sources such as MFCs improves overall reliability and functionality, it also triggers other issues; for example, the highest generated output voltage constrains or excludes the contributions from other generated voltages.\n\nSentence2: multiple MFC outputs might not be combined to the load directly through multiple power converters.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It can be seen that some output voltages are higher than the load output voltage (e.g., Voutc2, Voutc4).\n\nSentence2: these converters output energy is directed to the load.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: EC simple Power Attacks (SPA) that aim at power signal observation during PM computations so as to discriminate point operations, can be thwarted by employing PM algorithm variants (e.g. Montgomery Power Ladder (MPL) or double and always add algorithm [14]).\n\nSentence2: those variants are still vulnerable to refined PA and zero PA [12] or comparative SPAs (doubling and relatively doubling attacks [10] [19]) as well as differential power analysis attacks (DPA) [16] that can only be thwarted through PM randomization [8].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: investigated various visual displays for communicating privacy risks and aiding users in avoiding privacy-invasive apps.\n\nSentence2: they studied the impact of framing e.ects (e.g., displaying how privacy-preserving vs. privacy-invasive an app is) on users perceptions of an app s trustworthiness.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Because risk is a multi-dimensional concept [15, 16], multi-dimensional risk information also needs to be dis­played for users that want a more detailed view of the risks posed by an app.\n\nSentence2: the main focus of the present work was (1) to identify the most meaningful dimensions of risk and understand users perceptions of those risks, and (2) to better understand other sources of risk information (in addition to permissions) and how such information is used by experts and average Android users to assess apps.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although this seems to indicate that the risk types identified by the expert users and used in this study are appropriately comprehensive, there may be better names or descriptions that could be used to convey the scope of the risk types.\n\nSentence2: a third of the re­sponses described things that we consider to fall under the device instability risk type, which may indicate the need for a better descriptor for that risk type.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Different apps may expose users to different types of risk, such as ones involving data privacy, monetary loss, device instability, and data integrity.\n\nSentence2: any single approach to risk communication will be deficient for some users.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We coded each of the user-supplied group labels into one of 16 different summary labels based on the risk concept that it seemed to describe.\n\nSentence2: par­ticipants were given the following instructions: (1) sort the permissions into groups based on the type of risk that you feel is associated with each permission (a single permission may be assigned to more than one group); and (2) give each group of permissions a name describing the risk type.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, if the phisher is attempting to emulate PayPal, there are so many previous attempts made to phish PayPal that anything that is extremely close to PayPal social website URL is either already a registered domain, or is blacklisted.\n\nSentence2: in order for a phisher to emulate a Paypal URL, they have to start introducing misspellings, such as www.paypal.com, or they must construct another domain name to look like PayPal, such as www.paypal.com.anotherurl.com.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: One possible argument for involving the server in password strength estimation is that the server can check passwords against a dictionary of common words/pass­words or a known database of leaked passwords.\n\nSentence2: this only makes sense if the size of such a dictionary/database is significant (in which case the secure way to implement the service is to send salted and hashed passwords over HTTPS).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Currently, most CSPs offer performance metrics to measure the physical performance and system usage of a cloud such as CPU utilization, latency, and network throughput.\n\nSentence2: there has been little effort made on information assurance and security auditing metrics [2].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Another major obstacle to the widespread adoption of cloud computing is the lack of quantitative information about the security status of a CSP.\n\nSentence2: the security-related quantitative information helps cloud stakeholders in performing the robust and accurate cloud auditing of a potential CSP.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To show the practicality of the proposed metric, we provide two case studies based on the available security information of two well-known CSPs.\n\nSentence2: the proposed metric computes a security index based on the security preferences define by the cloud users.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Biddle et al. [5] state that a system provides resilience against keystroke/mouse loggers when the keyboard/mouse entries for authentication vary across subsequent login sessions.\n\nSentence2: the variant response feature in CuedR offers better resilience against basic keystroke loggers compared to a password system where the same letters are entered during every login session.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: CuedR also shows that the cued-recognition class of pass­word schemes, a new design point in the field, can be effective for user authentication.\n\nSentence2: cuedR addresses each of the features needed in an effective graphical password scheme, as identified by Biddle et al. [5] from their comprehensive survey on the graphical password literature.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Ideally, there should be a clear separation between the passwords used for low-security websites and high-security websites [7].\n\nSentence2: cuedR can be used as a standard authentication mechanism for online accounts with high security requirements and where logins occur rel­atively infrequently, such as financial (e.g., online banking, brokerage services) and e-commerce accounts [23].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since there were six legitimate users, there were 30 legitimate match attempts and thus 30 finalized decisions for each feature.\n\nSentence2: the performance of the each feature measured as the percentage of legitimate user attempts were mistakenly rejected by the system, i,e, false negative.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Through this research, we assess the performance and influence of various keystroke features on keystroke dynamics authentication systems.\n\nSentence2: we investigate the performance of keystroke features on a subset of most frequently used English words.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It covers the whole range, from no relationship (0) to a perfect relationship (3, or -3), with -3 indicating a perfect negative relation; whereas, 3 indicating a perfect positive relation.\n\nSentence2: the most common effect-size measures are Pearson r or correlation coefficient and Cohen s d (also known as the standardized mean difference).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We assume that D outputs 1 (indicating RC4) with probability 1 - β when its input is from RC4, and that D outputs 0 (indicating truly random) with probability 1 - a when its input is truly random.\n\nSentence2: a is the false positive rate for D and β is the false negative rate for D.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We see that S is required to be greater, reflecting the much weaker performance of our blockwise distinguisher when the block size is small.\n\nSentence2: the total disk size is only moderately increased and the running time of the distinguisher is further reduced (because of the reduced block size).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In particular, the users can collectively re-compute some values Λi from the data available for the users.\n\nSentence2: there is no direct way to check whether Λi+1 = Λd i if the DDH Problem is hard for the group used.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Their experiments reveal that leveraging all the 2-hop neighbors of each node signi.­cantly improves the network connectivity even in the case of high unavailability.\n\nSentence2: a much larger fraction of all online honest nodes belong to the largest connected component (LCC) with the Two-Hop overlay in comparison to that of the simplistic One-Hop overlay.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Common intrusion detection systems (IDSs) generally search for single events that either show clearly malicious or at least “unusual” characteristics.\n\nSentence2: sequence attacks do not involve such characteristics and are likely to pass through unnoticed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For both scenarios, we adopt a non-naive attacker that has a sufficient amount of knowledge to inject valid-looking position messages.\n\nSentence2: we assume these ADS-B messages are well-formed and their content is reasonable and able to withstand a superficial sanity check.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If L were zero, then a would also be a single-class best response, since its only non-zero element would be ac.\n\nSentence2: when F is high, then the problem actually becomes easier, since the adversary s strategy space will be very limited (see Theorem 2).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To build a packer classifier, we need to select a model and train it with labeled data.\n\nSentence2: two main approaches for solving this problem have arisen.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, they can be difficult to use due to the complexity of the XCCDF and OVAL file formats.\n\nSentence2: our solution uses oscap to provide assurance reports based on XCCDF files (because of its standard status) but auto­mates the definition of the XCCDF files.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The assurance tests have to be consistent with the se­curity policy that has been enforced.\n\nSentence2: the processed XCCDF file should include the assurance tests that have been generated, both for the properties and for the security mechanisms.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This concept [12][13] comes from the growing complexity of systems management.\n\nSentence2: there was a need for an approach that could ease the management of systems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Enforcing security properties in a Cloud is a difficult task, which requires expertise.\n\nSentence2: it is not the only security­-related challenge met by a company migrating to a Cloud environment.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Indeed, Cloud computing can reduce the costs of the infrastructure and improve the provisioning and access to resources.\n\nSentence2: cloud computing also comes with security issues [9]: for instance, due to the higher number of users and data, the Cloud offers a more interesting target for attackers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The tests are done using the XCCDF format.\n\nSentence2: we define a secu­rity property that will be used to launch Oscap in order to process an XCCDF file generated by the S EE .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, K PARTITION is NP-Complete.\n\nSentence2: mIN PARTITION is also NP-Complete.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We refer to this as promiscuous conflict-free schedul­ing because it maximizes the mixing of vms in a given server so long as they do not conflict.\n\nSentence2: a conservative approach minimizes the co-location of vms even though their attribute values do not conflict.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our intent here is to validate the thesis that we can wrap Mohawk in a manner that we preserve its scalability for ATRBAC policies.\n\nSentence2: mohawk+T would scale signifi­cantly better than what has been shown for existing tools for ATRBAC-safety [6, 12].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that as in prior work [6], we did not try this Benchmark Class on the other existing tools.\n\nSentence2: we have the somewhat interesting situation that no single tool can be said to be good with all the input ATRBAC-safety instances we have tried.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: (4) Hagström et al. motivate the distinction between delete and negative revocations mainly through the notion of resilience as de­fined in Section 2.\n\nSentence2: in weak revocations there can be no difference between a resilient and a non-resilient revocation, since a weak revocation does not affect authorizations issued by others than the revoker.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper we studied revocation for a version of delegation that does not have any bound on the length of delegation chains.\n\nSentence2: tDL lends itself very well also to delegation with such a bound.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Carrara and Adams measured the multi-path delay spread for a common single desk closed­door office environment at upwards of 250 ms [9].\n\nSentence2: this work examines OOB-CCs from the perspective of a passive adversary and argues that a different method­ology is required in order to effectively assess OOB-CCs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The primary consequence of this bug is that a negative value injected into a histogram can result in a term that decreases rather than increases a x2 sum, creating a closer match.\n\nSentence2: there is a more sophisticated and nefarious property of this bug: a negative value of appropriately small magnitude will produce a negligible effect if compared to a zero, and a very large effect if compared to a nonzero value.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An image and its simple LBP image is displayed in figure 1.\n\nSentence2: since his­togram values are nonnegative, no fabs() call is necessary; indeed, it allows a negative value for b to be included in the sum.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The intuition behind the proof is as follows: If the adversary corrupts the party that first receives the output, then he can provoke his most preferred event E10 by aborting before sending his last message.\n\nSentence2: because this party is chosen at random, this happens only with probability 1/2; with the remaining 1/2 probability the honest party receives the output first, in which case the best choice for the adver­sary is to allow the protocol to terminate and provoke the event E11.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As discussed above, the notion of utility­balanced fairness connects the ability (or willingness) of the adversary to corrupt parties with the utility he obtains.\n\nSentence2: in the second phase, if .GMW did not abort, the pro­tocol continues in two more rounds.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If hit and dirty, fetch from untrusted storage but do not store in cache.\n\nSentence2: the ALAP update policy causes MTs incoherence: during the update, when an intermediate node is written in the cache, the update stops and the upper nodes are not hierarchical digests of their children any more.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If we consider a connection to a Red (Open) network, the user adopts the typical behaviour, they click the network name.\n\nSentence2: instead of automatically connecting and being allowed to access the network, the user is prompted upon password entry that the network may not be secure (Fig 2).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Given a running program or an OS kernel, if we can locate its function pointers, we would have been able to check their integrity and detect the control flow violations.\n\nSentence2: current practice to locate a pointer in memory requires the data structure knowledge of the corresponding program.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, the system performs key labeling of each cluster to recover each keystroke by examining the distance difference between the mean TDoA of each cluster to that of the theoretical TDoAs.\n\nSentence2: we conduct following experiments: as shown in Figure 16(a), we first use the Samsung Galaxy Note 3 to play a pre-recorded chirp sound signal via a earbud, which is to make sure the sound comes below neighboring key caps and thus has multipath effects, for 20 times at each target key’s position on a regular keyboard.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although we do not study the sound intensity level of each key, we evaluate our system with three different kinds of keyboards (i.e., an Apple wireless keyboard MC184LL/A, a Microsoft surface keyboard and a mechanical keyboard Razer Black Widow Ultimate) that produce different keystroke sound intensity levels.\n\nSentence2: the keystroke sound from the mechanical keyboard is much louder than that from the Apple keyboard.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The basic idea of our system is to perform keystroke snooping leveraging the dual-microphone on a single smartphone through studying the fine-grained acoustic signatures inherent from key typing sounds.\n\nSentence2: we consider two processing approaches, namely Single-keystroke Based Processing and Set-keystroke Based Processing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, the mean TDoA of the keystroke sounds emitted by the same key is very close to the corresponding theoretical TDoA.\n\nSentence2: we compare the mean values of the measured TDoAs of each cluster to the theoretical TDoAs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Although some mobile devices have three microphones, for example iPhone 5s and Samsung Galaxy Note 3, neither Apple nor Google provides API to record 3-channel audio with three microphones.\n\nSentence2: our system must be designed in a way that it can accurately identify keystrokes based on the stereo recording of two microphones.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The solution named Pi­ratte [8] seems to be suitable in our case.\n\nSentence2: it is able to revoke only up to a predefined numbers of users/attributes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If it comes that the same person falls in a isolated location and needs help, activation to her/his location may be vital.\n\nSentence2: access control policy must be adaptive and context-aware.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: My main objectives for attending the UbiComp 2015 Doctoral School are twofold: By presenting my PhD project to experts in the field I expect valuable feedback and input on the over­all idea of multi-device authentication as well as the chosen approach.\n\nSentence2: i would like to discuss the implica­tions on both usability and security as well as reasonable trade-offs between both properties.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This paper will not address all of the possible meanings.\n\nSentence2: we will describe the definitions that have had the greatest impact on our own efforts.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The proposed input generating technique is not only more efficient while generating valid inputs as compared to randomized inputs but also more easier to use than tools such as Monkey Runner.\n\nSentence2: the idea of the proposed input generator couples the simplicity of the random approach in the Monkey tool along with the benefit of ensuring valid inputs in the Monkey Runner tool while ensuring maximum code coverage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: AnserverBot is a malware that asks users for update and installs the malicious payload.\n\nSentence2: the services in the malware itself do not need permissions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When a receiver is defined by intent filter, it lets the other app or components of the app send implicit intents to this receiver.\n\nSentence2: an app can define the intent filter in the receiver to obtain the implicit intents that may not target the particular application, for example, getting alarms or receiving messages from the system.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: if a service is bound to a component, it has access to its public method.\n\nSentence2: it is indirectly connected to other components and will die as soon as other components unbind it.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, this approach is not immediately applicable to the evaluation of network-based MTDs because these require entire networks in order to exhibit realistic and representative behavior.\n\nSentence2: experimental processes for network-based MTDs require automated network construction and configuration, as well as automated stimuli that operate on the network.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The analytical modeling related to P(At) in the context of wireless networks with random mobility has been widely investigated in the literature (e.g., [15 17]).\n\nSentence2: a wireless network with a random node distribution can be modeled as a random geometric graph, and P(At) is a function of node density, mobility model, and network size.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Software diversity creates many different copies from an initial version of a program: each copy of the protected program is different in its binary shape, but is functionally equivalent to other copies [24].\n\nSentence2: attacks designed to work with one version might not work with other customized versions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: MiniCPS focuses on high-fidelity network emulation, which allows simulated components to exchange very similar or the same traffic as seen in the real network, from Ethernet layer up to the application layer (based on Mininet, see Section 2.2).\n\nSentence2: we aim to not only provide an abstraction of the network to per­form simulations on (similar to network simulators such as NS2 [12], OMNet++ [25]), but we target a network emula­tion that is largely identical to a real network, without the cost or overhead of running a real network or a set of vir­tual machines.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In particular, we aim to not only provide an abstraction of the network to perform simulations on (similar to network simulators such as NS2 [12], OMNet++ [25]), but we target a network emulation that is largely identical to a real network, without the cost or overhead of running a real network or a set of virtual machines.\n\nSentence2: this would allow us to develop components that are directly using industrial protocols to communicate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We identify the issue of missing generic simulation environments for applications such as cyber-physical systems.\n\nSentence2: such simulation environments should support physical interactions, parametric communication links, and specific industrial (ideally all) protocols that are used.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: MiniCPS does not aim to be a performance simulator, or tool for optimizations.\n\nSentence2: we consider that full-fledged physical process simulation is out of the scope of MiniCPS.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In [7], Dong et al propose a testbed that is similar to our MiniCPS plat­form in several ways.\n\nSentence2: they propose to use Mininet as network emulation platform, a power grid sim­ulation server, and a control center simulation server.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Ontologies that are created by people who lack either expert knowledge or ontology development skills may result in serious prob­lems and wrong results.\n\nSentence2: these questions assure the targeted value of the structure is achieved.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also, even if the resources are sufficient, project teams may not think it is necessary.\n\nSentence2: we imported classes from SUMO and Finance ontology to use the classes that are already defined in financial domain, so that we didn t need to define new classes in the finance domain.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Considering that ontologies are a web of knowledge, integrating the ontology with other ontologies will create a bigger knowledge base and extend the opportunities of integrating this ontology with the existing systems.\n\nSentence2: to increase data and information quality within a domain, we need to create an ontology that can represent that domain successfully, and creating an ontology requires expert knowledge within that specific domain as well as the skills required to create it.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The remaining data items leak no further information, as long as encrypted with probabilistic encryption.\n\nSentence2: an attacker can learn the occurrence of a data item (e.g., via the histogram attack), however he can not gain access to the actual plaintext.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The location is potentially also sensi­tive.\n\nSentence2: talos applies deterministic encryption, allowing encrypted queries correlating heart rate with location.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This data leakage could be theoretically avoided by utilizing recent advancements in theoretical cryptography (i.e., fully homomorphic cryptosystem [23]) that enable any computations over encrypted data, without revealing any information.\n\nSentence2: these approaches are still computationally very expensive, rendering them impractical [55].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is possi­ble to find secure configurations using search algorithms and machine learning approaches suited for complex spaces [1, 17, 21].\n\nSentence2: these methods were developed for tuning parameters to improve system performance (user response times), not securing applications.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Following Google's best practices for developing secure apps, the password database is saved in the app data folder, which should be accessible only to the app itself.\n\nSentence2: this defines a hierarchical relationship between domains where the bounded domain cannot have more permissions than its bounding domain (the parent).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our approach is to 1) build a System Object Dependency Graph (SODG) so that the intrusion propagation process is captured at the system ob ject level; 2) construct a Mission­Task-Asset (MTA) map to associate the missions and com­posing tasks with corresponding assets, which are namely the system objects such as processes, files, etc.\n\nSentence2: if such dependency relations among services can be discovered and represented with specific graphs, then a task can be viewed as the instantiation of a service dependency graph.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To ensure the task is benign, the related system objects should be benign and the operations should be per­formed in the right sequence.\n\nSentence2: all of the parent nodes have the AND relation for the child node to be true.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The key difficulty lies in how to extract tasks from the SODG due to its daunting size.\n\nSentence2: the extraction is ensured to be feasible by the following principles.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Due to the current limitations of intrusion detection and forensic analysis tools, reconstructing attack scenarios from evidence left behind by the attackers of an enterprise system is challenging.\n\nSentence2: reconstructing attack scenarios by using the information from IDS alerts and system logs that have a large number of false positives is a big challenge.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The system has a mechanism to develop additional rules, facts and actions, based upon changing conditions.\n\nSentence2: an application-specific classifier mechanism can be used to identify additional assets of an entity-target and, based on the type of systems detected, new rule / fact / action chains are built to represent the prospective new targets and attack vectors to target systems or systems under protection.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Previous work and existing solutions are restricted to cater single node system.\n\nSentence2: during this process AE does not use any user specific information, as the user might not have created the content or might have received an unformatted machine.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is also possible that MobileIron’s source is sufficiently obfuscated to evade our straightforward root detection checks, though we did not notice much sophistication in the obfuscation used (unen-It is likely that in the future application developers will make crypted strings and straightforward decompilation).\n\nSentence2: it relies on the which command and the PATH variable being untampered with.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, updates are too infrequent to impact the attack, and our accuracy remains the same.\n\nSentence2: we run an experiment to “stress test” our attack, with users moving at constant speeds and their location updated every 10 seconds.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our experiments, distances are calculated from the coordinates reported to the service.\n\nSentence2: we measure the exact accuracy of our attacks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our results demonstrate that, despite the defense mechanisms in place, our attacks are still very ef­fective and time-efficient, and practical for use at scale and on a continuous basis (real-time tracking).\n\nSentence2: using a single account, we pinpoint Facebook users within 5 meters of their actual location in 3 seconds, and 90% of Foursquare's Swarm users within 15m in 7 seconds.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While Facebook enforces speed constraints, attackers can exploit this discrepancy to bypass them.\n\nSentence2: the adversary can carry out the user discovery attack and query the service from multiple vantage points without any speed restriction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: seccomp and Capsicum are both available to unprivileged processes, and intended to support extremely limited access for isolated processes.\n\nSentence2: we view this as a false-positive result: it is part of a carefully-managed interaction between browser components for the purpose of enabling compartmentalization.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Such false positives could be managed by introducing more selective annotations, e.g., on content::ResourceLoader rather than the general-purpose net::URLRequest, but our goal in this work was to explore as many variations of data flowing to risky code as possible.\n\nSentence2: we had very little tolerance for false negatives, so we erred on the side of false positives and developed tools to cope with them.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this section we present our main construction, that is a methodology to convert a public-space linearly-homomorphic encryption scheme into a scheme supporting one multiplica­tion.\n\nSentence2: the resulting scheme can compactly evalu­ate arithmetic circuits in which the number of additions of degree-2 terms is bounded by some constant (yet the num­ber of additions of degree 1 is unbounded).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Actually, we do not need to assume the group structure we do it here only for ease of exposition.\n\nSentence2: the schemes are implemented with security parameters of 80 and 128 bits, and both implementations of the Paillier­based and Joye-Libert-based schemes use NTL [36] with GMP [19].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In comparison, their scheme clearly supports a wider class of functionalities.\n\nSentence2: if we consider the question of building upon a linearly-homomorphic encryption to obtain more expressive functionalities, their construction is less direct than ours: for instance, they have to change the computation model to branching programs, and it is unclear whether tools originally designed for the underlying HE are recyclable in the transformed scheme.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Recall that the wave semantics of a binary code provides the set of all instructions that can be run.\n\nSentence2: it is important to measure the approximation obtained with respect to the wave semantics in order to determine the code coverage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We disassemble recursively and we add instructions to a layer in a consistent way.\n\nSentence2: we guar­antee that layers are always a sequence of aligned instruc­tions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A layer L is a set of dynamic instructions that satisfies the following two properties: (i) two instructions in L never overlap, and (ii) the set L is connected.\n\nSentence2: pin Tracer runs one instruction and, at each step, gathers the corresponding dynamic instruction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Recall that inside a wave, there is no code self-modification.\n\nSentence2: as a result, accurate and automatic malware analysis represents a true challenge.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We consider that each in­put binary is self-modifying code.\n\nSentence2: the execution of a binary will usually deploy different waves of code.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The comparison between disassemblers is currently difficult because there is no benchmark based on obfuscated binary codes.\n\nSentence2: it makes no sense to compare CoDisasm with off-the-shelf disassemblers because none deal with self-modifying code and overlapping instructions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The median response time for a put operation was between 1.45 and 1.8 times that of its get counterpart.\n\nSentence2: the encryption key can be derived from a secret (e.g., pass­word).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The monetary cost is not always strongly correlated with the network bandwidth usage, as designs with low bandwidth overhead may actually incur higher monetary expense.\n\nSentence2: monetary expenses subsumes some traditional metrics, such as server­-storage overhead.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Based on the most optimistic parame­ter setting in the original paper [22, Table 3], we chose the number of blocks per tree-node z = 4 and stash size s = 89.\n\nSentence2: understanding the gap between the idea of ORAM and today s cloud reality is the only way to keep the research on the right track.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Recent work has shown that tree-based designs [13] can also be augmented to support grow/shrink operations.\n\nSentence2: cURIOUS performs evictions (which may lead to shuffling depending on the subORAM technology) directly after each access.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For PracticalOS, the median response we measured is quite low (e.g., 0.147 for 4KB blocks, and 0.117 for 16KB blocks) but the mean response time (Table 3) is high due to the expensive reshuffling operation (e.g., 6.084 for 16KB block size).\n\nSentence2: in terms of response time without considering reshuffling, the scheme is very competitive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The position map can be reconstructed by reading the entire storage (with block headers that store a block id and version id).\n\nSentence2: some blocks in the eviction cache may not have a copy on the cloud, so that they would be lost if a failure occurs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that revoked and expired code signing certificates were valid when they were issued.\n\nSentence2: the total number of signed samples that used a CA-issued certificate is 97%.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Furthermore, when malicious software is observed in the wild signed with a valid certificate, the CA that issued the certificate should swiftly revoke it.\n\nSentence2: it is not clear how well defenses such as identity checks and revocation work.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Overall, 343 code sign­ing certificates have been revoked.\n\nSentence2: cAs revoke less than 16% of the certificates they issue to PUP and malware authors.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In Section 5.5 we show that only 15% of the malicious certificates we observe are revoked, and the revocation practices outlined in this paragraph are likely a contributing factor for this low number.\n\nSentence2: some authors keep using their code signing certificate long after it has been revoked.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The first malware cluster at rank 22 corresponds to Zbot.\n\nSentence2: aggregating all Zbot clusters would rank Zbot as 11th largest operation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition, due to implementation limitations, DSA is known to produce even more conservative, and thus pessimistic, results on modern LLVM releases [2].\n\nSentence2: an updated version of DSA (or a more precise, but also less scalable analysis) would already likely yield substantially improved forward-edge results.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: PathArmor will forward most system calls directly to their vanilla implementation, imposing little to zero extra over­head.\n\nSentence2: we consider a total of seven system call fam­ilies as dangerous, and start verification whenever these are encountered: mprotect and the mmap family (which can be used to disable DEP/Wfix), and the exec family (which can be used to start a malicious command) are obvious choices and have been considered in prior work in the area [19].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Otherwise send (output, y) to all honest parties.\n\nSentence2: figure 1: Ideal functionality Ff for secure 3-party computation of a function f. Ff provides security with abort (i.e., unfair output) in which the adversary is allowed to learn its output from the functionality before deciding whether the uncorrupted parties should also receive their output.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As described in Section 4.4, we only rely on the accumulation value of a column in order to perform a SUM query on that column.\n\nSentence2: sUM queries over intermediate results can be handled as long as the client can obtain a verified accumulation value of the desired column of the intermediate result.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use the OpenSSL library for encryption and hashing.\n\nSentence2: we use AES-CBC-128 for encryption7 and SHA-256 for hashing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use the OpenSSL library for encryption and hashing.\n\nSentence2: the size of the min­imal covering set output by RangeCover is O(log n).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As shown in the figure, the setup time grows linearly with the total number of rows in all the tables in the database.\n\nSentence2: setup takes about 4s for two tables with 100 rows and 5 columns each, and 3,000s for two tables with 100,000 rows and 5 columns each.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Conservativeness: any type of persistent data that is not explicitly agreed or expected by the user should not be left or shared.\n\nSentence2: if storing or using of persistent data is intended, it must be mentioned as a policy.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For CLEAN files, we create files on the sandbox location; for COPY, we copy the file from the location to the sandbox path.\n\nSentence2: if storing or using of persistent data is intended, it must be mentioned as a policy.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Browser developers might not be fully aware of potential privacy leakage when instrumenting browser engine for those functionalities and hence, either causing users browsing activities in private mode be persisted to disk (violation of afore­mentioned stealthiness goal) or causing previous browsing data be carried over to private session (violation of aforementioned fresh­ness goal).\n\nSentence2: it is not uncommon to see the pattern shown in Table 3 in the browser s release cycle: a new web standard is mis­implemented by developers, causing a privacy leakage bug which takes years to be patched.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: More importantly, VCR can remain mostly generic to any hardware-specific implementations because the camera HAL must also use the generic data structures to return photographic data to the apps.\n\nSentence2: this is beneficial to VCR, which can now be designed in a more robust, generic way than tools that must recover data from individual (highly diverse) Android apps.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Increasingly, mem­ory forensics tools are employing pointer traversal to locate data structures [8, 11, 25, 37].\n\nSentence2: sigGraph [21] builds maps of structures in a memory image via brute force scanning.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our study reveals that this photographic evidence always persists in the smartphone's memory - without being erased or overwritten - until a new app uses the camera (filling the previous image buffers with new evidence).\n\nSentence2: vCR will always have some evidence to recover.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, an entity model also takes into account attributes for the entity types and constrains the type and arity of both attributes and relationships.\n\nSentence2: it enables the policy to correspond to the application domain because it can take into account any number of relationships and properties of any type for each relevant entity type.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similar to relationship-based models [4], this takes into account entity types and their corresponding relationships.\n\nSentence2: an entity model also takes into account attributes for the entity types and constrains the type and arity of both attributes and relationships.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The analysis and modeling described here are based on a single data set.\n\nSentence2: the techniques are general and could be adapted to other data sets to help improve decision making about when and how to deploy security interventions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, as the scope of cyber-insecurity has increased, no one security practitioner is able to grasp all of the relevant details associated with global problems [17].\n\nSentence2: there is a need for more explicit and rigorous methods to determine which interventions are effective and which are are not.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These regional results raise the interesting possibility that bot­nets can migrate in response to takedowns.\n\nSentence2: by reducing the number of infected hosts in one region, a takedown creates incentives for botnets to find new vulnerable hosts, thus moving the problem elsewhere.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In most common cases, an indirect jump is used to dispatch execution from a jump table, and all legitimate targets of this jump are the corresponding case branches, which can be identified by the previously discovered constants.\n\nSentence2: in this part, we constrain the rest of the indirect transfers in the binary, which correspond to the border nodes of continents.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This could be due to the fact that it introduced a new password policy of mini­mum password length of 8 while the website was live [10].\n\nSentence2: only a small amount of early users have passwords of length less than 8; (ii) the majority of passwords of most datasets are Uni and Bi.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From Table 2, we observe that (i) the majority of passwords of all datasets have length less than 12.\n\nSentence2: a significant number of passwords of most datasets have length less than 8.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Further, the sites that have many high-value accounts, e.g., Computers/Shopping/Business sites, tend to require users to choose longer passwords.\n\nSentence2: 37 sites have explicit maximum password length constraints.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This implies CSDN is more vulner­able given Tianya than given Rockyou.\n\nSentence2: according to the results of the two meters, LinkedIn is more vulnerable given Rockyou than given Tianya.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: From Fig.2 (a) and (e), we see that most cracked passwords of CSDN are labeled as weak.\n\nSentence2: there are still a considerable number of cracked passwords of CSDN that are labeled as strong.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: How is passwords' strength accurately measured?\n\nSentence2: it is important to consider which algorithms should be combined in designing HPC schemes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The number of false positives is cut by almost a factor of 5, while the proportion of lost true positives is not much more than 1/4.\n\nSentence2: dDA allows us to retain most of the benefit of being able to find OBJECT OVERRUNS, while at the same time dramatically reducing the number of false positives.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition, the DIVINE implementation would not actually create a larger object in CodeSonar/x86's IR in the presence of an overrun the ASI results would not be incorporated in that way.\n\nSentence2: there are some important differences between the two tools.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Insofar as the initial set of boundaries provides an overapproximation to the ground truth, our goal is to eliminate as many spurious object boundaries as we can; at the same time, we want to retain the true boundaries that enable the detection of actual buffer overruns.\n\nSentence2: it also has an effect (sometimes sizable) on the analysis recall.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our parameter-offset analysis was inspired by the Howard tool [24, 25].\n\nSentence2: there are some important differences between the two tools.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It provides an easy means for consumers to store and share their data, also in mobility through convenient apps.\n\nSentence2: such valuable functionality raises several security concerns.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Tavizi et al., present an architecture for the adoption of UCON in the Cloud environment, mostly focused on the obligations enforcement.\n\nSentence2: this architecture is presented at a very high level and to the best of our knowledge it does not describe in details the additional components re­quired for the enforcement of UCON policies, in particular for what concerns continuous policy enforcement.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Typical issues for such cloud services are related to data con.dentiality and ownership, uncontrolled data replication, physical storage location and so on.\n\nSentence2: through the Cloud enforcement engine, resources become available for Data Consumers through the Mobile enforcement engine.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this approach for intrusion detection, the system calls serve as the source for mining and predicting any chance of intrusion.\n\nSentence2: when compared to the artificial neural networks approach, the detection accuracy is lesser [11].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Similarly, we may choose to use the DARPA 1998 or 1999 standard dataset.\n\nSentence2: the research should first start with the studying benchmark datasets.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A Mann-Whitney U test shows that the participants' experience significantly differs based on their gender (U=262, n=42, p=0.046, r=40.4).\n\nSentence2: male participants indicated to be globally more experienced than female participants.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If the parent pro­cess is still running when the monitoring agent captures its snapshot, all further information about the parent is read­ily available.\n\nSentence2: no mat­ter the event type, this root node needs to be semantically significant.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As indicated, the margin and PNMod parameters are also ben­eficial because they expand the CRP space.\n\nSentence2: two copies of the logic expressions for GF(4) given in [23], and two copies imple­menting their inverse, synthesized to a set of 16 4-input LUTs labeled L15 down to L0.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The underly­ing entropy source consists of both individual LUT gate delays and the interconnect routing delays, which are combined in unique ways and measured as path delays by HELP.\n\nSentence2: the number of paths reflects the amount of entropy present in the functional unit.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Table 2 presents some additional information about the fragments and surprisals in our corpus, where n=1,2,3.\n\nSentence2: we observed 641 unique 1-grams in our corpus.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We noticed that compiling this code at a low level of optimization (gcc -O1) improves the stealthiness of this code by avoid­ing the setnl opcode.\n\nSentence2: the level-1 optimizations of GCC also simplify the boolean expression in the guard of the if statement, so they decrease the strength of this ob­fuscation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, the problem of the password-based authentication scheme is that a server must maintain a password table to verify the legitimacy of a login user.\n\nSentence2: the server requires additional memory space for storing the password table.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A selects any identity IDany, and generates a random number n1.\n\nSentence2: this is impossible because the secret key x does not exist in the smart card.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To overcome this drawback, new authentication schemes using biometrics and smart cards have been proposed [9, 10].\n\nSentence2: these proposed schemes are based only on a single-server environment, and are limited due to the number of network services required.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If the two conditions are not being satisfied, nobody can reconstruct the data secret by other methods.\n\nSentence2: even if a provider has more than k shares, without other providers cooperation, he cannot perform secret reconstruction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In addition, in the real use-case, one may expect that that m < l, < n .\n\nSentence2: hence in this paper, we will consider the situation where m < n.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, from the viewpoint of information security, certain risks must be mitigated such as unauthorized access, data leakage, data falsification and others.\n\nSentence2: information management has become an important topic in the last few years.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To update shares, shareholders (different groups servers) have to assemble enough shares (i.e., the number of shares must over threshold) to reconstruct data secret s or reconstruct authority secret v and redistribute new shares again.\n\nSentence2: if we do that, it will have a risk of leak data secret s or authority secret v in the process of reconstruction.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The goal of PLC1 is to ensure that there is enough water in T101 to be supplied to stage 3 for ultrafiltration.\n\nSentence2: the control algorithm in PLC1 monitors water level in T101 at predetermined intervals.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that the states of MV101 and FIT10 are locally consistent.\n\nSentence2: any anomaly in stage 1 has a very high likelihood of being reflected in stage 3; stage 2 does only has chemical sensors and hence requires chemistry-based detection method not used in the experiments reported here.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A number of experiments conducted reveal that the distributed detection based on invariants is effective in detecting SSMP attacks.\n\nSentence2: replay attacks on the first stage of a CPS, that has no previous stage, need additional state information for detection.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Further, it is as­sumed that an attacker has access to any one stage of the CPS.\n\nSentence2: the attacker could compromise one or more en­tities in any one stage to which access is available.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: According to [13], interesting points are time samples providing the most information for side channel attacks(i.e. Template Attack).\n\nSentence2: for each guessing key keyi, the attacker calculates a column of assumed power consumption Pkeyi (i.e. Hamming weight), then a correlation coefficient ρ in equation (9) will be calculated.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For the same-core channel, Smartphone has a wider dynamic range of 10 .C, while the dynamic range on Laptop is just 7 .C.\n\nSentence2: for the 1-hop channels the dynamic range is wider on Laptop, where it is at least 6 .C for both core 1 and core 3, compared to the dynamic ranges of just 4 .C and 2 .C, respectively, measured on Smartphone.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In order to verify the motion of an object, multiple successive measurements are required to estimate the velocity and heading of the target.\n\nSentence2: these approaches can only verify the average velocity and heading between the considered messages.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The en-route airspace is of special interest to us since most aircraft are usually at high altitudes.\n\nSentence2: the dataset from the OpenSky Network contained almost twice as many positions from the en-route airspace than from the lower airspace.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The goal of this section is to find a method to determine the FDOA accurately enough for secure verification real flights.\n\nSentence2: it can arbitrarily adjust its transmission frequency and we do not impose any restrictions on the attacker's knowledge.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Secondly, the center frequencies of the transponders differ significantly as indicated by the horizontal difference of the ECDFs in Figure 5a.\n\nSentence2: we must assume that the actual transmission frequency is different from 1090 MHz as the transponders are obviously not calibrated or synchro­nized (e.g.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our experimental results with air traffic control signals demonstrate that our approach can be used to significantly enhance the security of air traffic management systems.\n\nSentence2: we provide means for measuring the Doppler shift of air traffic signals using off-the-shelf software-defined ra­dios.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Each setup recorded all ADS-B messages along with their raw I/Q samples using a modi.ed version of the GNU Radio-based software receiver for transponder signals gr-air-modes2.\n\nSentence2: directly measuring the FDOA with the above method turns out to be much more robust in the presence of the highly unstable transponder signals.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Due to the low sampling rate (up to 200Hz) on most mobile devices, it's difficult to cap­ture the details of the running CPU instructions.\n\nSentence2: we show that, by extracting both time and frequency do­main features from EMRs, MagAttack can still learn the footprint with a low sampling rate and can infer which App the victim is interacting with.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The reverse channel coding aims to reduce the discrepancies between the encoded bits by the wearable and the device.\n\nSentence2: the original encoded bit sequences are considered as messages with a limited number of errors, and are converted into shorter sequences using a error correction code (FEC) decoder.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The shapebased encoding faces a similar issue, as the two curves do not coincide in non-resonant frequency ranges.\n\nSentence2: we turn to the third option that encodes the resonant and antiresonant frequencies to ensure the matching rate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that we can only accurately obtain the resonance properties when the system is in the steady state.\n\nSentence2: it is hard to identify which part of the accelerometer data is collected in the steady state, as the duration and patterns of the transient state depend on many confounding factors.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Wrist circumference and BMI are import physical attributes related to hand vibrations, as our system should be robust for users of different physical attributes.\n\nSentence2: the wrist circumferences range from 5.51 inches to 7.48 inches, and BMI ranges from 17.5 to 27.70.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To avoid repetitive extrema, we select at most one maximum and minimum in each sliding window of 10 Hz.\n\nSentence2: if there are multiple minima or maxima in one sliding window, we select a winner based on amplitude and discard the others.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In practice, the analog sinusoidal waveform is approximately generated with binary voltage levels using Pulse Width Modulation (PMW).\n\nSentence2: pMW modulates the duty cycles of the DC power to simulate a voltage between the DC power voltage and zero voltage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Like the design of bene.cial smart contracts, CSC construction requires a careful combination of cryptography with commission-fair design [34].\n\nSentence2: a party can generate arbitrarily many public keys.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: At its core, VUDEC is designed and the overall system architecture along with its centrally and decentrally operated components are specified.\n\nSentence2: the standardization of vulnerability descriptions allows system operators to make use of information and knowledge (e. g., specific measures for vulnerability mitigation), which has been acquired by other network users earlier on.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To clients, reverse proxies act as the service's endpoints for example, if a domain name is used to advertise the service, the domain will resolve to the reverse proxy's IP address.\n\nSentence2: a reverse proxy looks like the origin server and requires the collaboration of the service owner to be installed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To avoid detection, attackers may attempt to remove discrepancies introduced by the reverse proxy.\n\nSentence2: complete removal requires deep understanding of the reverse proxy code and configurable options, as well as careful configuration of the origin servers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Furthermore, they do not provide the CoKey key combination mechanism.\n\nSentence2: the physical presence of the crypto token is not sufficient to decrypt data encrypted with it.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Threshold schemes form a good protection mechanism against differential power analysis (DPA).\n\nSentence2: using it allows building cryptographic hardware that is guaranteed to be unattackable with first-order DPA, assuming certain leakage models of the cryptographic hardware at hand and for a plausible definition of “first order”.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For drivers who pay the full price the tracker can of course be turned off.\n\nSentence2: for drivers who took the discounted contract, tracking the truck position in an unrestricted way poses clear privacy problems.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Such smart industrial control systemsfall into the category of Internet of Things (IoT).\n\nSentence2: in many cases, the data transmitted by such IoT devices in­cludes sensitive information and users are faced with an all­or-nothing choice: either they adopt the proposed services and release their private data, or refrain from using services which could be bene.cial but pose significant privacy risks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Without overlaps, patterns of maximum length are uncovered in 100% of the cases when thermal attacks are performed within 30 seconds after authentication (Figure 7).\n\nSentence2: depending on the authentication scheme, the use of thermal cameras can still help the attacker to reveal the part of the input made on the touch screen.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our implementation is open source, hence allowing further experimentation with thermal attacks.\n\nSentence2: pINs and patterns remain among the most popular authentication mechanisms as of today [23, 49].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In that work, the authors address the problem of differentially private parameter tuning in machine learning applications.\n\nSentence2: a machine learning model (e.g., a classifier) is trained on a dataset T and its performance is validated on a hold-out dataset V.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Decision tree construction methods differ in the implementation of NI.\n\nSentence2: we next describe four alternate implementations of NI that result in four splitting criteria best algorithm, group regret, minimum average regret and regret variance criterion\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: There are general-purpose algorithms (e.g. the Laplace Mechanism [6] and the Exponential Mechanism [18]), which can be adapted to a wide range of settings to achieve differential privacy.\n\nSentence2: the naive application of these mechanisms nearly always results in sub-optimal error rates.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A part of homomorphic encryption has been successfully applied in some domains, such as database queries [58] and MapReduce programs [59].\n\nSentence2: its significant drawback is high performance overhead and incompatibly with existing applications [60].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: NoHype [49, 50] and TrustOSV [51] try to build an iso­lated execution on multi-tenant clouds by providing fixed cores, pre-allocated memory to a VM.\n\nSentence2: there is less interaction between the hypervisor and VMs at runtime, but the resource utilization is low.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We also try to use the HA-VMSI interfaces to send a malicious request for overwriting the page tables.\n\nSentence2: it is prohibited by checking whether the values violate the security policies of HA-VMSI.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: VA in a guest VM is translated to IPA through the Stage-1 page tables managed by guest OS just like non-virtualized systems.\n\nSentence2: having a trustworthy security monitor provides a secure foundation for the cloud server to strengthen the security of guest OS and its applications.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While the security monitor is only in charge of VM s integrity and privacy protection, such as val­idating each mapping on Stage-2 page table, tracking the us­age of physical pages, and deploying the pointer of Stage-2 page tables.\n\nSentence2: instead, HA-VMSI still main­tain the flexibility of the hypervisor, but the security protec­tion is decoupled from the hypervisor into HA-VMSI.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the meantime, these mapped pages will be unmapped from the page table of the hypervisor.\n\nSentence2: protecting VM data on the cloud is still far from trivial.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: And each core has an independent TLB and each TLB entry contains a VMID tag corresponding to the address.\n\nSentence2: hA-VMSI can run simultaneously and yet securely on different cores alongside the virtualized system.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We use 4 VM types with memory sizes from 512MB to 4GB, to evaluate the performance impact of HA-VMSI prototype for a VM creation.\n\nSentence2: section 5 presents our prototype implementation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: When the switch agent finishes its startup, HA-VMSI enables the trap events and deprives the hypervisor from executing some privileged instructions that allow the hypervisor to control critical system state, such as defining the location of its page tables and exception handlers.\n\nSentence2: the VM exit cannot be bypassed from HA-VMSI.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Compared to these sys­tems, HA-VMSI provides protection at the whole VM level, which naturally fits for the context of multi-tenant cloud.\n\nSentence2: when creating a mapping, some new page tables may be required.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Results demonstrate that their CNN outperform traditional feature-based methods in detecting JPEG steganography provided that the number of training data is huge.\n\nSentence2: not surprisingly, adding more layers causing trouble in training; this can be observed in Fig.  4 showing abnormally higher training errors achieved by a more complex 20-layer CNN, and there is no question that the validation performance also suffer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Researchers observed as children moved through the process.\n\nSentence2: they remembered what kinds of usernames and passwords they created, what picture they chose, and generally where the points were located on the image.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It would take human analysts many months to complete a single round of tests, and even then they are bound to make numerous mistakes in the course of testing\n\nSentence2: one needs an automated framework to run the tests.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: EtherNet/IP is an object oriented industrial protocol.\n\nSentence2: it is an implementation of the common industrial protocol (CIP) [22]on top of the TCP/IP protocol stack.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In order to take advantage of these features, our current implementation assumes that a house contains sensors and Wi-Fi controllable lights.\n\nSentence2: such products are already commercially available, and should become more commonplace with time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The type system definition pays particular attention to the mode of the typing judgment, in the sense of logic programming (Debray and Warren 1988).\n\nSentence2: the type context and term are in­terpreted as inputs to the typing judgment, while the term's type is viewed as an output\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Our results establish that our system ensures that well-typed programs do not leak confidential information under the security policy prescribed by the assumed security lattice.\n\nSentence2: data does not flow from a security compartment to another if they are unrelated or if it is a down-flow in the security lattice.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Duplicate keys are not allowed in our model.\n\nSentence2: if the key has changed, then the seek function restarts from the root of the tree.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If the delete operation is complex, then it advances to the discovery mode after which it will advance to the cleanup mode.\n\nSentence2: if it is simple, then it directly advances to the cleanup mode (and skips the discovery mode).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It then sets the delete-flag on the left edge of the target node using a CAS instruction.\n\nSentence2: we now describe the results of the comparative evalua­tion of different implementations of a concurrent BST using simulated workloads.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since language specifications often leave some decisions to compilers for flexiblity, a compiler is allowed to remove behaviors.\n\nSentence2: com­pilation is a refinement step.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, for stencil or sampling operations as consumer functions, the redundant computation introduced by inlining can be quite significant.\n\nSentence2: we restrict our inlining to cases where the consumer functions are point-wise functions, and rely on our schedule transformations to enhance locality for the other operations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Halide [36, 37] a recent domain-specific language and compiler for image processing pipelines focuses on both productivity and per­formance.\n\nSentence2: the Halide compiler requires a schedule specification to generate an implementation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This process is complicated by the fact that while a system call instruction is only one byte long, a jump instruction requires five bytes.\n\nSentence2: in order to rewrite the system call with a jump, we also need to relocate some of the instructions surrounding the system call i.e. perform binary detouring via trampolines [22].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While Sections 4.1 and 4.2 illustrate the worst-case synthetic and real-world scenarios for a system call monitor, in order to compare VA R A N directly with prior NVX systems, we have also run it on the same set of benchmarks used to evaluate prior systems.\n\nSentence2: we chose to compare against three state-of-the-art NVX systems: Mx [21], Orchestra [40], and Tachyon [33].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We adopt this scheduled communication [10] strategy for communication slack.\n\nSentence2: the next two classic energy saving approaches are intended in particular for slack arising from non-communication, i.e., mostly, computation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Given a set of past tickets in buckets with their resolution times, TAACT categorizes incoming new tickets in each bucket into bins within the buckets, using only the textual description field of the tickets.\n\nSentence2: based on the minimal information available for all tickets, TAACT predicts whether an incoming ticket is likely to require low, moderate, high, or very high time for resolution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Using these indicators, measures such as precision, accuracy and recall are calculated.\n\nSentence2: they consider distinct lexims in the churned source code, which are quite huge in number, as features and extract them from churned source code by bag-of-words approach (BOW) [28].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: [12] focuses on the reuse aspect of inheritance and proposes a new metric that is theoretically well grounded.\n\nSentence2: in section 5 we explain how the metric is computed and in section 6 the way the metric is visually represented for an entire system.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A task can have more or less partitions and spaces than that may be available in its target execution platform.\n\nSentence2: a task operates on logical spaces and partitions, and we call them Logical Processing Spaces (LPS) and Logical Processing Units (LPU) respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Advancements in more accessible CAD tools that allow novice designers to simply drag and drop forms into a 3D model [7], or use evolutionary modeling [19], computer vision [2] and machine learning [9] to create frameworks that map descriptive words onto a modular set of existing 3D designs have made this process slightly easier.\n\nSentence2: many thanks to the MIT Media Lab consortium for supporting this research.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, such sharing is in­effective when a single application fully utilizes the available memory by itself.\n\nSentence2: we expect GPU memory to be shared mainly among applications using relatively small amounts of GPU memory.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We consider this delay acceptable since most GPU applications tend to re-use previously allocated memory instead of allocating and freeing buffers frequently.\n\nSentence2: the delay did not always mirror the time needed for a DMA transfer of the same size.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The memory management of current GPUs lacks some features found commonly in CPUs: Current GPUs typically treat page faults as fatal errors, and their page tables do not contain reference-or dirty-bits.\n\nSentence2: well-known memory management techniques like demand paging or traditional page replacement algorithms cannot be applied to GPUs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the Monte Carlo method, true variance δ2 is usually unknown.\n\nSentence2: it can be estimated from the sample values.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The hypervisor can dynamically adjust the allo­cated memory pages of a VM by memory swapping without any permission from guest OSes.\n\nSentence2: such a benchmarkbased approach cannot account fine-grained resource allocation changes accurately.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Most of the recent studies for hypervisor security have been focused on providing confidentiality and integrity of user data.\n\nSentence2: a neglected aspect of security is the availability of resources.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If either of the transmissions in Slot 1 or 2 succeeds, following transmissions from Relay 1 to A1 will be scheduled in Slot 3 and Slot 4.\n\nSentence2: as an integral part of WCPS 2.0, we have implemented a WirelessHART protocol stack in the TOSSIM simulator.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Also, to efficiently correct emergencies, the valve, which is normally OFF , is sometimes switched to ON .\n\nSentence2: the control strategy for this system is hybrid, since whenever an emergency is activated the controller behavior is changed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This abstract model is then used to automate the generation of test cases in a systematic way.\n\nSentence2: the generated test suite is typically guaranteed to meet some coverage criteria, thus providing additional assurance about the quality of tests.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, the algorithm is guaranteed to find a unifying counterexample for every ambiguous grammar, but the search will not terminate when infinite expansions are possible on unambiguous grammars.\n\nSentence2: this semi-decision procedure for determining ambiguity is sound and complete.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The authors report average speedups of 3.8x— on 6-9 hardware threads.\n\nSentence2: the paper's evaluation is problematic in two respects, making it diffcult to interpret the results: (1) the sequential analysis they compare against used an arbitrary node ordering for the worklist, which in our experience can cause slow-downs from 2 5Ã— relative to a more optimized node ordering strategy; and (2) their evaluation reports analysis runtimes rather than speedups.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The search would be reasonably efficient if DynamoRIO were to keep a sorted data structure of spans, but it does not because that large and expensive structure would be of no value for module image VM areas, which comprise the vast majority of translated code.\n\nSentence2: this project was partly supported by the National Science Foundation under grants CCF-0846195, CCF1217854, CNS-1228995, and CCF-1319786.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: On Python, the indirect jumps are most often very well predicted for most benchmarks, even by the 6 KB ITTAGE.\n\nSentence2: in several cases the prediction of indirect jump is poor (see for example the Python chaos, django-v2, formatted-log, go).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A latency to access data within the ring varies and can be as high as accessing data from the memory.\n\nSentence2: since stencils are at the heart of many scientific com­putation and are very compute intensive, there has been a considerable body of research on how to improve the per­formance of these codes [12, 15].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: All existing works handle work-groups by scheduling them in distinct CPU threads.\n\nSentence2: cPU threads do not need to syn-chronize until kernel completion, which is convenient because synchronization across threads on a CPU is expensive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: PCFS uses a formal logic with more connectives than the Guardat policy language.\n\nSentence2: for each access pattern in each configuration, we perform five experimental runs; each run has 20,000 accesses (a total of 100,000 ac­cesses for each configuration).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, Popcorn is always faster than its competitors.\n\nSentence2: popcorn is up to 3.5 times faster on 57 cores on Class C.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This widget relayed CMAPI messages between OWF based map widgets and the uMap server.\n\nSentence2: when users using the uMap browser client would manipulate geospatial information on their maps, the OWF users could see the same information and vice versa.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Software-managed hybrid mem­ory is a promising way to incorporate NVM in main mem­ory due to its architectural simplicity.\n\nSentence2: there are significant performance issues caused by interference in the memory system due to data migration between DRAM and NVM and a lack of effective migration policies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, in healthcare, the Cochrane Collaboration provides a webpage on Other Software Resources1 , which presents a list of available tools.\n\nSentence2: the list is short and is missing many, potentially, helpful tools.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The potential for time-saving was also praised.\n\nSentence2: speeding up quality assessment by including a previously assessed study (from a past SR) might mean that “you wouldn’t have to quality assess it again.”\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The majority of tools identified by participants were reference managers.\n\nSentence2: refWorks and EndNote were mentioned most often.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Many participants felt support for multiple users (F4-F01) within a tool was really important.\n\nSentence2: allowing users to collaborate within large-scale teams was considered very useful.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As the size of J10 is odd we know that it is not empty.\n\nSentence2: all the vectors in J10 must satisfy that they have 1 in the first coordinate and 0 in the remaining coordinates.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, we believe that a better understanding of the rela­tion between Reed-Muller codes and Polar codes is needed, and perhaps more generally an understanding of which sub­spaces of polynomials generated by subsets of monomials give rise to good, efficient codes.\n\nSentence2: it would also be interesting to investigate the scaling of the blocklengh in terms of the gap to capacity for RM codes.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Curiously, the two seminal papers from the late 1940s giving birth to coding theory, by Shannon [39] and Hamming [22] differ in whether one should consider recovery for most corruptions, of from all corruptions.\n\nSentence2: our task is to show that V = U .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Consider the pattern (1, 0) in the first two rows of U .\n\nSentence2: consider all columns of U that have 1 in their first coordinate and 0 in the second.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The advantage of global recovery is that it can be done in a semi-transparent way: the application does not necessarily have to be aware of the failure.\n\nSentence2: due to the intrinsic global nature of the recovery algorithms, global recovery present scalability challenges.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, these works were often theoretical or used a (simplified) model together with a simulator, while realistic implementations in actual resource managers were not available for various reasons.\n\nSentence2: those promising results were rarely reflected in the practice.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In summary, we can apply the tile size parameterization process to any set of tiling hyperplanes (diamond or otherwise).\n\nSentence2: other stencils that require different diamond tiling hyperplanes can also benefit from this process.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 10 shows the software components which contain a large number of functions.\n\nSentence2: developing long-term stable product line architectures.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A semi-formal or formal evaluation with supporting tools is necessary to speed up the development process.\n\nSentence2: metrics are not meant to replace the scenario-based methods like ATAM [4] but to augment the analysis of architecture based on data and measurement experience.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Continued performance gains must be achieved within the context of reusing existing scientific software that represents countless hours of investment on part of expert programmers.\n\nSentence2: as the cycle of porting codes to realize the potential performance offered by the arrival of the next generation, or even the next technology of hardware continues (i.e. “performance portability”), software development productivity must be maintained by forward looking elimination of software bugs and defects.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We were specifically interested in middleware and software frameworks.\n\nSentence2: software that is very general in its purpose because of its role as part of distributed cyberinfrastructure.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Whether or not a software product was available under an open­source license per se was far less a concern for most respondents than were its capabilities, cost, and reliability.\n\nSentence2: three of the top five most important selection criteria identified relate to characteristics of open source software total cost of ownership, long-term availability, and initial purchase cost.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 5 and Table 4 shows that NMCS uses, on average, 25MB less memory than best-first search, but 40MB more memory than beam search, for the SUTs considered in these experiments.\n\nSentence2: these differences have little practical impact given that JPF in this environment always used at least 123MB.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: JPF is, in essence, a Java virtual machine (itself written in Java) and this enables JPF to control and measure many aspects of software's execution.\n\nSentence2: it can enumerate and control the choices made at non-deterministic points in the program, such as the generation of pseudo-random numbers and most importantly for the experiments here the interleaving of separate threads in concurrent program.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Figure 5 and Table 4 shows that NMCS uses, on average, 25MB less memory than best-first search, but 40MB more memory than beam search, for the SUTs considered in these experiments.\n\nSentence2: there is a much smaller effect of the simulation depth on space efficiency.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since the execution time improvements of our proposed techniques are quite significant the overall CPU energy dissipa­tion will be reduced.\n\nSentence2: the energy usage improvements presented in this paper are for the L1 DC and the DTLB only.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A typical CPU performs all data accesses in a general fashion.\n\nSentence2: the context of a data access can affect how the L1 DC can be most efficiently accessed.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This approach requires an associative check of block num­bers and word offsets of all the entries in the load queue.\n\nSentence2: fast speculative address generation and way caching for reducing L1 data cache energy.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This approach requires an associative check of block num­bers and word offsets of all the entries in the load queue.\n\nSentence2: our approach only requires checking one strided access entry.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A reference architecture on the other hand is a reference model mapped onto software elements that together implement the functionality of the reference model.\n\nSentence2: whereas reference modeling divides functionality, a reference architecture is the mapping of that functionality onto system decompositions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this section we consider the problem of detecting races in programs whose task graphs have a two-dimensional lattice structure.\n\nSentence2: we apply the algorithm for finding suprema from Section 3 in an efficient race detection algorithm.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Observe that the race detection algorithm in Figure 6 uses the result of a query only to compare it with the current vertex.\n\nSentence2: we may answer queries differently as long as any such sequence of comparisons leads to the same outcome.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As k increases, the state space to be explored by the BMC explodes exponentially.\n\nSentence2: bMCs do not scale to large k.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The fixes of the previous iteration with the development of the current one are done simultaneously.\n\nSentence2: we can see that agile methods are iterative, emergent and flexible to cater changes in requirements differently as compared to the linear approaches of traditional software development [17][18].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A sequential specifi­cation of a deterministic object determines the return value of an operation from the sequence of prior operations applied to the object.\n\nSentence2: with a replicated object, operations executing at different replicas might have inconsistent no­tions of prior operations , e.g., because updates made on one side of a partition do not propagate to the other side.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: One way, for example, is to ignore the received information (for a while).\n\nSentence2: a data store with invisible reads cannot do this (Section 5.3).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A fence event e forces the adversary to commit all the writes in p's write buffer (if any) in the order they were issued.\n\nSentence2: whenever the adversary schedules p, it commits the next write from p's write buffer, as long as the buffer is not empty.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Key idea is to take the advantages of both static and dynamic task mappings.\n\nSentence2: it refers to the pre-computed decisions on task mapping and schedule to pick best mappings for concurrent applications at run-time without heavy computation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Co-locating and cooperatively scheduling multiple enclaves within a single node complicates the already difficult problem of node-level resource allocation.\n\nSentence2: for that, they enhanced the global EDF policy by scheduling the set of threads of the same application at the same time, only when there are enough physical cores available.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Exceeding this threshold means that the formula ϕ is not true in the transition system M.\n\nSentence2: satisfiability of [M, ϕ]k, for some k means that the formula ϕ is true in the transition system M.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Note that each key x can have many independent scores drawn, one for each element of x.\n\nSentence2: the more elements a key has, the more likely it is to be sampled.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The authors believe that this paper is the very first in nature to study explicitly productivity in agile.\n\nSentence2: section 3 presents the research design of the study.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Easily programming behaviors is one major issue of a large and reconfigurable deployment in the Internet of Things.\n\nSentence2: when the event is triggered, the context is embedded into it.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As future work, we intend to generalise the singular labels on labeled values and references to become sets of labels, thereby making them more homogenous with the rest of the enforcement.\n\nSentence2: like the current label set, these labels become elements in the power set lattice of security labels.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A closer look shows that TL is dominated by the time spent on maintaining the observation table (merging abstract states/transitions).\n\nSentence2: running the test cases only takes a small portion of the time.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Besides tools for predicate abstraction, there are tools of generat­ing models for Java classes, among which we identify the TAU-TOKO tool [19, 20] to be bearing a similar goal as TLV.\n\nSentence2: in the following, we compare TLV with TAUTOKO1 in the context of answering the above research question.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, PPI greater than 1 indicates that CLOTHO generates many more constraints than those in the developer's fix.\n\nSentence2: a PPI closer to 1 is desirable.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In particular, the OpenGL ES libraries are vendor specific and not open source.\n\nSentence2: we cannot modify the source code and re-build the ROM.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, this approach cannot save power because the GPU workload required to generate those graphics buffers remains the same and thus consumes the same amount of power.\n\nSentence2: we have to seek for a new approach and next we describe how our approach works.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Like many other Android-based smartphones, the OS is not fully open source.\n\nSentence2: the OpenGL ES libraries are vendor specific and not open source.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The number of AEGIS AWS programs participating during this metrics collection period was five.\n\nSentence2: each build served five different programs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To achieve these benefits, one of the most important features of PLE is to provide an architecture that accurately captures the commonalities and variabilities of all the products in the product line family.\n\nSentence2: we narrowed down our analysis scope specifically to collect information required to build such an architecture and understand the current configuration process used at the company.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, the data transferred between such large applications is very huge and massive and hence the cost become high.\n\nSentence2: the cost needs to be minimized by minimizing scheduling length.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Queries can be executed on the source code or other relevant artifacts to, e.g., identify all classes that depend on particular other classes or packages [17].\n\nSentence2: to assess the architectural consistency, queries need to be written that identify forbidden dependencies.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Manually ensuring that the implementation of a software system is consistent with the software architecture is a laborious and error-prone task.\n\nSentence2: a variety of approaches towards automated consistency checking have been developed to counteract architecture erosion.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: NDT maximizes the sum of probabilities that the aligned points of the second point cloud can score on this density [12].\n\nSentence2: as this database grows, so would the computational cost of finding a match, possibly making the splitting operation itself become the bottleneck of our system.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Up to the first buffer flush of process 25 at about 140 seconds the communication operations (red) require little time, as can be seen by the share of red within white, which contains everything else within the application.\n\nSentence2: during the first buffer flush all other processes wait for process 25 to finish its buffer flush and engage in the communication operation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We recall that different mitigation measures can be taken depending on the outlier score.\n\nSentence2: for each iteration, we plot the outlier score of the largest outlier detected in that iteration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We reduce the false positive rate dramatically by using an elasticity parameter that learns during the first iterations of the execution and adapts dynamically.\n\nSentence2: then, we restarted the execution from the last checkpoint (i.e., time step 15,000), and we recorded the datasets of the execution at each time step for a corruption-free execution.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The main idea is to gather statistical information for the whole cluster of points.\n\nSentence2: we generate and monitor multiple distributions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We monitor the application datasets during runtime in order to find simple behavior patterns and therefore predict an interval of values that represents an acceptable value.\n\nSentence2: we de not want to keep previous data for each data point of the dataset we are monitoring, which incurs a 100%- 400% memory overhead.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In designing the method­ology and tool, the goal was to automate the tasks that require knowledge of MPI library internals and provide the user with easy-to-implement recommendations that could enhance communication performance by tuning MPI library parameters.\n\nSentence2: a study conducted at TACC shows that, as depicted in Figure 3, most of the MPI jobs do not use the entire memory available on a typical Stam­pede node (32 GB).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our model, called statically bounded region serializability (SBRS), regions are synchronization free, intraprocedural, and acyclic [31].\n\nSentence2: regions are bounded at synchronization operations, method calls, and loop back edges.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since there is no mechanism to return either a reference or a primitive type (corresponding to the any wildcard), the call to getValue() boxes the value returned.\n\nSentence2: the call introduces a silent performance regression, which is the cost of interoperability.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: our technique detected 443 clone sets (45% of our re­sults) that Bauhaus missed.\n\nSentence2: bauhaus (with threshold 50) was not able to detect our motivating example (Figure 1).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The theoretical framework of graph transformations provides a declarative, rule-based technique for modifying graphbased models such as program graphs [8].\n\nSentence2: a graph transformation rule consists of a left-hand side (LHS) and a right-hand side (RHS), both constituting typed graphs, i.e., program graphs in our case, conforming to a given type graph of the underlying modeling language.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Such an h-ordering consists of multiple h-groups, where each h-group consists of suffixes which share a common h-prefix.\n\nSentence2: we are shifting the array B by h positions to the left.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is because a work-item should be fed from two successive banks.\n\nSentence2: a bank should provide data to two work-items.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the 32-bit mode, successive 32bit (4-byte) words map to successive banks.\n\nSentence2: both OpenCL and CUDA support image processing mechanisms for GPUs to speed up processing one-, two-, or threedimensional image data.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We cannot infer any dependency on the ordering of the receives on the process.\n\nSentence2: we do not allow the sends to be reordered.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Given that, in our targeted application, each rank only communicates with a constant number of neighbors (independent of the system size), we created a layout of \"disposable communicators\" , each of which only contained two ranks.\n\nSentence2: assuming no collectives were needed, we created a communicator for every pair of ranks that had to communicate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Consequently, in this experiment we engineer the set of failures injected so that they do not allow propagation delays due to local recovery to merge, and therefore the total overhead is the sum of the recovery overhead for each failure.\n\nSentence2: this experiment evaluates the worst-case local recovery overhead.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If the quotient is greater than 1, linked weak array mourning is faster by a factor equal to the quotient.\n\nSentence2: if the quotient is 3, this means linked weak array mourning was three times faster than regular weak ar­ray mourning given a certain array size and tombstone rate.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Linked weak array referents will be referenced strongly until mourning determines it is safe to make the linked weak array slots weak again.\n\nSentence2: whether traditional weak arrays should also adopt the ephemeron finalization convention merits consideration.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Further details about MC/DC variants can be found in [11], [12].\n\nSentence2: the terms used in [12] are different.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we present a classifier that can identify po­tentially vulnerable commits with a significantly lower false­positive rate while retain high recall rates.\n\nSentence2: unlike most existing tools for vulnerability finding, we don t focus solely on code metrics, but also leverage the rich metadata contained in code repositories.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Machine-learning and data-mining approaches have been proposed by several authors for finding vulnerabilities.\n\nSentence2: unlike most existing tools for vulnerability finding, we don't focus solely on code metrics, but also leverage the rich metadata contained in code repositories.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An attractive approach to saving energy in such applications is to defer the execution of delay-tolerant operations until a time when they would consume less energy.\n\nSentence2: introducing delays to save power may have a detri­mental impact on the user experience.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: CRM applications may save significant power by changing the timing of delay-tolerant operations.\n\nSentence2: operations differ in their degree of tolerance to delays, so the developer must carefully balance energy savings against user experience.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For instance, [11] used an audio database with manually labelled adjectives belonging to one of 13 categories and trained Support Vector Machines (SVM) on timbre, rhythmic and pitch features.\n\nSentence2: categorical representations of emotions have been criticised for their numerous restrictions [13, 2].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: To adapt the notification mode to the phone position and the user's context, a first method could be to rely on the users.\n\nSentence2: our daily experiences show that users usually tend to forget it.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: These intelligent schedulers allocate resources by predicting a task's future resource usage based on its past utilization.\n\nSentence2: memory behaviors are very difficult to predict.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A large proportion of energy savings comes from devicespecific hardware power management, e.g., displays have a lower-power mode, CPUs have low-power speed governors, and on-board peripherals can transition to low-power modes.\n\nSentence2: the tasks and devices involved in handling the wakeup event are extremely limited, essentially just those represented in the figure (as well as the networking device).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This can be achieved if the re­sistances of the batteries are proportional to the square-root of their DCIR-to-SoC ratios.\n\nSentence2: the RBL-Discharge al­gorithm seeks to allocate the currents y1,...,yN in such a way that the effective resistances of batteries are as much as possible proportional to the square-root of their DCIR­to-SoC rates minimizing the total energy wasted through resistive losses.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The bendability, unfortunately, comes at the cost of other battery properties.\n\nSentence2: it is interesting to note that if the user had not gone for a run then the first policy would have given better battery life suggesting that the knowledge of an impending workload can help save energy in heterogeneous battery settings.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If it were possible to check eagerly that compile is correct, the monadic cast would produce a value of type option correct comp, and the client calling runc (? compile) would simply have to locally deal with the potential of failure.\n\nSentence2: this non-local impact of deciding to statically establish guarantees or defer them to runtime is contrary to the smooth transition path that gradual typing is meant to support.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Conventional designs of navigation systems mainly focus either indoor or outdoor navigation.\n\nSentence2: all of such mobile phones are equipped with the inertial sensors (accelerometers, compasses, and gyroscopes), light sensor, Wi-Fi module, GPS module, and GSM module.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However, directly hooking printk has one practical issue: the hook handler has to parse the input to printk, which is a non-trivial job (e.g., to interpret the semantic of format strings like \"%x\").\n\nSentence2: we look into the source code of printk and find a function cont_add that is invoked by printk and handles already formatted output.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We have proposed the idea of live migration based on-the-fly software emulation, which significantly advances the state of the art: it is analysis-flexible, supports closed source OSes, and uses modern hardware virtualization extensions.\n\nSentence2: there exist discrepancies between their handling of VM states, which cause the \"resume\" step to fail.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This confirms our previous argument that it is energy efficient to run memorybound phases on ARM and compute-bound phases on x86.\n\nSentence2: for a memory intensive application with a regular memory access pattern, it will still be energy efficient to run the application on x86.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For Korat, third-party code implementing these structures with their various operations is publicly available, making it possible to simply borrow existing implementations.\n\nSentence2: with CLP, not only is code unavailable, existing imperative code is so different semantically that it cannot be translated directly.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: They usually generalize one identifier or statement at a time and re-run the search; if the search result degrades, they undo the generalization and try a different identifier or statement.\n\nSentence2: their generalization strategy is similar to the typical greedy search.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It is challenging to directly compare CRITICS with LASE, because LASE's template generation requires multiple examples apriori and is fixed, while CRITICS is an interactive tool that a human can iteratively configure a template.\n\nSentence2: we simulate a human-driven template configuration process in CRITICS.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The original RTED algorithm finds node-level alignment by calculating the minimum edit distance, producing many false positives.\n\nSentence2: how do you like or dislike CRITICS?\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: At the same time, product quality was not sacrificed as the process ensures that all tests are ran at least once on all code changes.\n\nSentence2: we have to estimate how undetected defects would propagate through the development process and when they would be detected.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Software engineers use SPLs to increase software reusability and to rationalise software maintenance and evolution effort across a range of related products [3].\n\nSentence2: without automated support, this feature selection process is likely to be highly suboptimal: it requires the si­multaneous satisfaction of multiple objectives, such as match­ing user preferences, minimizing product cost and satisfying technical feasibility constraints in feature spaces defined by many thousands of features.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Despite the promising results, it fails to scale to large FMs mainly due to the expensive t­wise coverage computation [14].\n\nSentence2: concretely, the spread configurations suggest that there are configurations in favor of each considered objective.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: ReviewBot also added the ability to provide fixes; this fix system is completely decoupled from the analysis results themselves.\n\nSentence2: tRICORDER fixes are produced by analyzers, and our mechanism for applying fixes (from structured analysis results) is language-agnostic.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Formal software verification is the application of formal proof to guarantee the correctness of software behaviour.\n\nSentence2: the cost of formal verification is not well understood due to the relatively low number of successful large-scale applications.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Refine builds on AInvs, but we compute the proof sizes for Refine with respect to itself\n\nSentence2: proofs from AInvs do not contribute to the size of proofs from Refine.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Most lemmas make stronger assumptions than are actually necessary.\n\nSentence2: a lemma might have a concrete term where an abstract one will suffice: the statement 1 is odd is less general than the statement 2n + 1 is odd , which has an abstract term 2n + 1 in place of the concrete term 1 .\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Finally, the statistical analysis of the co-factors' influence has highlighted that only the knowledge of the used libraries has a significant effect on the completeness (p-value=0.006), although it does not interact with the main factor.\n\nSentence2: people with higher knowledge of the used libraries perform better, independently of the availability of the code examples generated by MUSE.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: MUSE's examples are meant to help developers to reuse a particular method, once they know it might help with the task at hand.\n\nSentence2: we envision MUSE to be used by developers for getting more precise information about the methods they want to use, after using one of the tools mentioned above.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: MUSE selects from each group Gi of type-2 clone examples in the ranked list the one having the highest ss value.\n\nSentence2: after the selection process, each method in Pi is associated with a list of ranked and diverse code examples.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This study aims at providing a human assessment of the ranking and selection heuristics.\n\nSentence2: we formulate the fol­lowing research questions: RQ1: Does MUSE s example ranking re.ect develop­ers judgment of code examples representing groups of clones?\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The objects (i.e., code examples) of our study are generated by MUSE for the same six systems adopted in Study I.\n\nSentence2: we randomly selected ten methods from each system for which MUSE generated at least one usage example.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: If there are only a few intervals where the counter is observed to be problematic, the severity will be close to 0.\n\nSentence2: if the counters are violated many times, severity will have a value close to 1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Today, CPU centric-benchmarks (e.g., SPEC and RPE2 [33]) are often used to map and compare CPU utilizations between different configurations.\n\nSentence2: such an approach provides no support for practitioners to understand the rationale for such regressions and cannot be used to compare other performance counters than CPU (e.g., I/O, threading and memory-related performance counters).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In our example, a user could quickly inspect and modify the schema and data in the provisioned database through a web-based UI.\n\nSentence2: the most reliable or popular posts often contain concise code snippets with implicit references to external de­pendencies and in-line placeholders for surrounding program context [3].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The greater the uncertainty inherent in a project, the more the project has to move from traditional approaches that are based on a fixed sequence of activities to approaches that allow to redefine the activities or even the structure of the project plan in midcourse.\n\nSentence2: as the project complexity and uncertainty increase, managers need to go beyond traditional risk management; adopting roles and techniques oriented less toward planning and more toward flexibility and learning.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the above process, concolic testing partly tackled the path explosion problem by using abstraction and refinement [17], [16].\n\nSentence2: concolic testing cannot automatically figure out what code blocks should be abstracted and which sideeffects of those blocks should be treated as inputs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Crowdsourcing enables a group of individuals to contribute towards solving a high-level problem.\n\nSentence2: a fundamental challenge is to break a high-level problem into a number of precise (or atomic) tasks [7].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Moreover, in that work we rely on the Speech Act Theory revisited and described in the book of Bach and Harnish [4] to guide the interpretation of text in terms of intentions.\n\nSentence2: in order to tackle the lack of data we have collected online discussions (in the domain of software development) annotated with intentions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An estimate based on this suggests that a clear separation of view and controllers, and a strict view layer will save up to 15% effort when porting code to new rendering services.\n\nSentence2: it is likely to be somewhat lower since dependencies can also be indirect and bleed through the view layer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Testing (4) on increasingly longer tracks shows that this probability slowly decreases, therefore cancelling the advantage of the starting player for longer tracks.\n\nSentence2: the probabilities are 0.5276, 0.5232, and 0.5188, for track lengths of 126, 252, and 504 spaces, respectively\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Let us now consider a strategy that changes on the basis of the duration of the game.\n\nSentence2: let us assume that a player would be conservative for some turns (in order to accumulate armies) and then become aggressive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The most commonly used approach for preventing piracy is code obfuscation, that is, making the code of an application more difficult to understand.\n\nSentence2: according to Sahin et al. [43], obfuscations techniques used on mobile applications are likely to impact their energy usage.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Nevertheless, we hope that the approach we followed helped this paper better reflect the views of researchers in the software energy consumption community.\n\nSentence2: li et al. [24] presented the first large scale study on the energy efficiency of mobile applications.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Participants were allowed to use any resource they liked.\n\nSentence2: participants spent 76% of their total time on documentation webpages or hovering over a method documentation.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Therefore, they do not necessarily reflect the effective complexity of source code.\n\nSentence2: they lead to inflated measurements of well-structured long functions that are actually reasonably simple to comprehend [11].\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The developers who had made prior changes to files in a change under review had a higher proportion of useful comments in four out of the five projects (all but Exchange which shows marginal increases), but we did not see a difference in effectiveness based on the number of times that a developer had worked on a file.\n\nSentence2: comments from developers who had changed a file ten times had the same usefulness density as from developers how had only changed a file once.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: One possible explanation for these results is that reviewers who have changed or reviewed a file before have more knowledge about the design constraints and the implementation.\n\nSentence2: they are able to provide more relevant comments.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Since these services are intended to be invoked by serverside components of applications, the keys themselves would ideally lay dormant only in server-side program memory and thus be inaccessible for users of the application.\n\nSentence2: many application developers choose to host their application source code publicly on repositories such as GitHub and BitBucket to incorporate contributions from the opensource community.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The feature model is widely used in the SPL community, but it does not support dynamic aspects.\n\nSentence2: the authors tried to add some properties, extensions or rules to model DSPLs.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: C09 -Multiple Bindings: It is possible that an DSPL have multiple bindings.\n\nSentence2: this criterion evaluates whether the technique can model multiple bindings or not.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The extended workflow provides tailoring by the possibility to distinguish between mandatory and optional activities.\n\nSentence2: projects can modify the workflow depending on project size, complexity, staffing, timeline and priorities.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The inner details are hidden from view of an HPCC System Administrator.\n\nSentence2: the type of testing that needs to be performed on an HPCC system with the typical computing node/accelerator card arrangement, is often black-box regression testing.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: After the first phase, the main results were published in [1].\n\nSentence2: doing this within the walls of the organization ignores all the opportunities for open innovation around the best practices to software engineering.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: One limitation in the modeling process was the scalability, especially when having numerical computations.\n\nSentence2: the problem has been solved by delegating a part of the functionality to the adapter.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It cannot control the behavior of the IUT via test inputs; it can only observe its behavior.\n\nSentence2: we discuss the benefits and limitations, and propose a concrete solution to address the latter.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: While such design decisions pose important limitations discussed later in Section V-D, we were able to control the same information patch for analysts to pursue the correct traceability links [10].\n\nSentence2: a unique tag, personnel , helped one analyst to make the correct decision of selecting Personnel­DAO.addEmptyPersonnel() in the final TM.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In the last decade, Information Retrieval (IR) has been widely adopted as core technology of semi-automatic tools to extract traceability links between artefacts according to their textual information.\n\nSentence2: a widely known problem of IR­based methods is that some artefacts may share more words with non-related artefacts than with related ones.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The improvement gained by the adaptive feedback is particularly evident for Easy-Clinic, while for MODIS and i-Trust there is more interleaving between the two feedback strategies.\n\nSentence2: for i-Trust the adaptive feedback is substantially better than the standard feedback only for recall lower than 50%, while for MODIS the two corresponding precision-recall curves are very similar.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: GCP provides application programming interfaces to make it easy for developers to specify how to handle these inevitable false positives.\n\nSentence2: it is possible that an incorrect value is returned for an attribute that was never inserted, i.e., the query re­turns a false positive.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Thus, the continuous time and space domains are discretized so that a set of numer­ical computations are iteratively (time discretization) ap­plied onto a mesh (space discretization).\n\nSentence2: in a mesh-based numerical simulation, the PDEs are transformed to a set of numerical computations applied at each time step on elements of the discretized space domain (the mesh).\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: A stencil kernel computes the value of one data or a sub­part of it (the kernel computation domain ) using a numerical expression which takes as input one or more data.\n\nSentence2: for ex­ample, the stencil kernel illustrated in Figure 1b computes A using B, while the one in Figure 1c computes A using C. a stencil kernel is defined by its numerical expres­sion, a set of input data (only one in the examples), and its unique output data, the result.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The Tree Decomposition: Gtsp Many works [21, 24] explains how to build a series-parallel tree decomposition from a minimal series-parallel graph.\n\nSentence2: as mentioned in the previous section, the Multi Stencil Language (MSL) is an agnostic descriptive language for multi-stencil simulations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Typically, each FW task repeatedly provides a service, with a clear start and end, in response to inputs from its interacting SW or HW layer.\n\nSentence2: in this setting, FW code is inherently associated with an infinite loop structure.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: After O2 is generated at step 2, the Nursery Region is full.\n\nSentence2: gC is triggered on this region.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Data flow 5 indicates that, at the appropriate time, the valid log records will be effectively written to the file system.\n\nSentence2: the system only needs to update the delta 2 + delta 3 based on L1.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: However a compute node may appear in one or more trees because it may be few hops away from multiple bridge nodes.\n\nSentence2: once all the trees are formed, we examine each tree rooted at a bridge node to decide the new assignment based on the distance from bridge node by invok­ing Traverse (line 7), which we outline in the next section.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Local accesses to the ob ject then make use of this data unless rget is subsequently used or the entry is evicted.\n\nSentence2: the object s origin and address are stored, along with a local pointer, in a concurrent dictionary which is consulted for future lookup and removal operations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In that view, variables must be re­named to match the mechanisms of lexical scope as macro expan­sion proceeds.\n\nSentence2: all of these benefits reflect the way that scope sets are precise enough for specification but abstract enough to allow high-level reasoning.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: At the same time, the needed cache capacity should also be exploited to alleviate the PCM endurance concerns.\n\nSentence2: we set the DRAM size as 1MB, 4MB and 16MB to represent the small, middle, and large cache configurations respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Today's machines are incorporated with specialized hardware units, such as DFP, SIMD, FPGA, and GPU, to complement diminishing singlethread performance improvements due to silicon technologies.\n\nSentence2: the original binaries are compiled with COB42 with the highest optimization-level in Fig. 9 and the lowest optimization-level in Fig. 10, respectively.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: This presentation was followed in particular in the original paper on Separation Logic [14].\n\nSentence2: we introduce the representation predicate p -> Queueof R L, to assert that, at address p in memory, there exists a mutable queue represented by the list L, when the elements stored are represented by the representation predicate R.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Furthermore, it became obvious that system features encumber the interactions.\n\nSentence2: system level features from Android (such as the main app menu) repeatedly opened during the VI users exploration of the screen with their fingers.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Norman [36] observed that a key feature that distinguishes an activity-centred approach from others (e.g. user-centred) is that it requires both a deep understanding of users, and also of the technology, the tools, and the context of the activities.\n\nSentence2: an activity-centred approach provides a better fit to understanding group activity in multi-surface spaces, as a whole.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: For example, the activity-based computing (ABC) framework [2] is conceptually close to the ACAD framework in the sense that it decomposes users activity into tasks, materials, time, and users.\n\nSentence2: the purpose of the ABC framework is to inform the implementation of 'computational activities' of a distributed system that ensures adequate synchronisation of data and methods across devices.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, some of the character array and intrinsic array types were not modified inside the called C function.\n\nSentence2: their general approach is to use a model for representing facts about different programming languages and link facts between the host and foreign languages.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Like Felix, HONE uses end hosts to implement certain measurement tasks.\n\nSentence2: hONE lacks abstractions for expressing, analyzing, and partitioning network-wide queries based on regular expressions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The tool located several explicit call locations within test classes.\n\nSentence2: it did not locate implicit calls that originated from HTML pages.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: CLOUDIATOR differentiates between cloud platforms, the API offered by a software stack, cloud provider, the organisation running a cloud platform at a dedicated endpoint/URL, and cloud, as the offering seen by the tenant.\n\nSentence2: besides the endpoint of the provider, a cloud is also linked to log-in credentials.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Layers are identified only via their name, but a reified Layer object will be created lazily once a symbol is being used as layer name.\n\nSentence2: this cannot be considered a sandbox, since a locally active layer can still cause global effects.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: We propose a method to detect invalid combinations between layers that occur at runtime using static analysis.\n\nSentence2: even though several COP languages activate layers in disciplined manners, e.g., using scoping (6) or composite layers (5; 11), such activation mechanisms do not prohibit layer activation that is mistakenly specified by the programmer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The approach presented here is novel in investigating whether Erlang has the potential to address the scalability and reliability issues with communication between ROS nodes.\n\nSentence2: we don't replace the entire Robot Operating System, only the communication layer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Traditional WSAN networks such as WirelessHART employs a uniform routing strategy across an entire net­work.\n\nSentence2: if the operator chooses graph routing, a graph routing protocol will be used for all flows in the network.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The statistical distributions are based on all bidirectional links in the topology with 500 transmissions per link.\n\nSentence2: s/G (3 Hz) is at the second best, which con.rms the impact of control frequency on system lifetime.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: The entries in the same row are the blocks that were written when the snapshot was created.\n\nSentence2: in all these cases, accessing older versions can provide superior latency and/or throughput.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In this paper, we propose a more compositional fragmentoriented approach to the problem of expressing new semantic structures.\n\nSentence2: we introduce a single extensible statically typed language, typy, that gives library providers the ability to define new semantic fragments.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Their approach automatically checks whether a configuration exists in which a referenced identifier is never declared.\n\nSentence2: in contrast to our approach, they are only concerned with the existence of declarations, but not with the resolution of a particular reference to multiple suitable declarations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Considering that the system control of each system is concentrated on its owner, the developers of the systems integration is strongly dependent of such systems owner.\n\nSentence2: there is a clear need of a change in the approach in the direction of having well-defined standards and protocols to allow developers to integrate such systems in an easy way.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As this version of Segen was developed to generate test scripts for Selenium and Selendroid only in Java, we used a library to generate code in Java called JavaPoet4.\n\nSentence2: the architecture of Segen has been prepared to add, in the future, generators of scripts in other languages.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: An advantage we observed is that Segen does not need to be the only tool used to test the user interfaces versions of an application.\n\nSentence2: any component that cannot be tested by Selendroid and Selenium can be tested separately by other tools, including those ones used by Segen, such as JUnit and other Java libraries.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "entailment"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Additionally, when a task is decomposed into (nearly) independent subtasks, community members can complete them (mostly) in in parallel.\n\nSentence2: partitioning a task also creates overhead for coordinating the execution of subtasks.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "contrasting"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, we work with seven to ten children over longer periods of time.\n\nSentence2: our priority is examining closely how the adult­child interactions support direct democratic practices, in which choices and decisions are made directly together during design sessions.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: It saves one communication phase where all replicas send mes­sages to each other; hence it spares network bandwidth and reduces end-to-end latency.\n\nSentence2: as can be seen, HybsterS is the only configuration confined by a sequential ordering protocol.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "neutral"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: Upon engaging with a community and depending on an open source software for its own products and operations, an organization has the choice to take on extra responsibilities in the community to preserve it and ensure that the valuable communal resource is maintained [23].\n\nSentence2: the organizational-communal engagement involves a selection process that can impact both the open source community and the organizations.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: In contrast, The VMXOFF instruction makes the processor run in the VMX non-root operation.\n\nSentence2: we execute the VMXOFF instruction upon devirtualization and the VMXON instruction upon revirtualization.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    },
    {
        "instruction": "Consider the following two sentences:\n\nSentence1: As BitVisor changes from the source host to the target host, the address of the virtio ring buffer also changes.\n\nSentence2: we must transfer the address of the virtio ring buffer.\n\nWhat is the semantic relation between Sentence1 and Sentence2? Choose from the following options: 1. Entailment, 2. Reasoning, 3. Contrasting, 4. Neutral.",
        "input": "",
        "output": "reasoning"
    }
]